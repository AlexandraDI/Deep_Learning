{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTyXBwyPUHsv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54fe4815-60f0-45b8-cf17-4b4b574bf08d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import h5py\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from torch.utils.data import IterableDataset, DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score\n",
        "import torchaudio\n",
        "\n",
        "sns.set_theme(context=\"poster\", style=\"darkgrid\", palette=\"colorblind\")\n",
        "\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.backends.cudnn.deterministic = True\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t2ZM9A-jR-2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4c337f-6371-4dac-83e8-4b70a8c283d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the zip file\n",
        "!unzip /content/drive/MyDrive/Data_Ass3.zip -d \".\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "732pz-mrmzMT",
        "outputId": "854e1ce7-1f3c-401c-99e9-460315ce5372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Data_Ass3.zip\n",
            "   creating: ./Data_Ass3/\n",
            "   creating: ./Data_Ass3/Cross/\n",
            "   creating: ./Data_Ass3/Cross/test1/\n",
            "  inflating: ./Data_Ass3/Cross/test1/rest_162935_1.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/rest_162935_10.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/rest_162935_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/rest_162935_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/task_motor_162935_1.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/task_motor_162935_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/task_motor_162935_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/task_motor_162935_9.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/task_story_math_162935_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/task_story_math_162935_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/task_story_math_162935_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/task_story_math_162935_6.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/task_working_memory_162935_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/task_working_memory_162935_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/task_working_memory_162935_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test1/task_working_memory_162935_7.h5  \n",
            "   creating: ./Data_Ass3/Cross/test2/\n",
            "  inflating: ./Data_Ass3/Cross/test2/rest_707749_10.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/rest_707749_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/rest_707749_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/rest_707749_7.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/task_motor_707749_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/task_motor_707749_7.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/task_motor_707749_8.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/task_motor_707749_9.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/task_story_math_707749_10.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/task_story_math_707749_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/task_story_math_707749_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/task_story_math_707749_6.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/task_working_memory_707749_10.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/task_working_memory_707749_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/task_working_memory_707749_8.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test2/task_working_memory_707749_9.h5  \n",
            "   creating: ./Data_Ass3/Cross/test3/\n",
            "  inflating: ./Data_Ass3/Cross/test3/rest_725751_10.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/rest_725751_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/rest_735148_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/rest_735148_9.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/task_motor_725751_1.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/task_motor_725751_9.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/task_motor_735148_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/task_motor_735148_7.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/task_story_math_725751_1.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/task_story_math_725751_7.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/task_story_math_735148_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/task_story_math_735148_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/task_working_memory_725751_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/task_working_memory_725751_8.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/task_working_memory_735148_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/test3/task_working_memory_735148_4.h5  \n",
            "   creating: ./Data_Ass3/Cross/train/\n",
            "  inflating: ./Data_Ass3/Cross/train/rest_113922_1.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_113922_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_113922_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_113922_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_113922_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_113922_6.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_113922_7.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_113922_8.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_164636_1.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_164636_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_164636_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_164636_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_164636_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_164636_6.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_164636_7.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/rest_164636_8.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_113922_1.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_113922_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_113922_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_113922_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_113922_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_113922_6.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_113922_7.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_113922_8.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_164636_1.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_164636_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_164636_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_164636_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_164636_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_164636_6.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_164636_7.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_motor_164636_8.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_113922_1.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_113922_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_113922_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_113922_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_113922_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_113922_6.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_113922_7.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_113922_8.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_164636_1.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_164636_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_164636_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_164636_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_164636_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_164636_6.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_164636_7.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_story_math_164636_8.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_113922_1.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_113922_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_113922_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_113922_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_113922_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_113922_6.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_113922_7.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_113922_8.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_164636_1.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_164636_2.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_164636_3.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_164636_4.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_164636_5.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_164636_6.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_164636_7.h5  \n",
            "  inflating: ./Data_Ass3/Cross/train/task_working_memory_164636_8.h5  \n",
            "   creating: ./Data_Ass3/Intra/\n",
            "   creating: ./Data_Ass3/Intra/test/\n",
            "  inflating: ./Data_Ass3/Intra/test/rest_105923_10.h5  \n",
            "  inflating: ./Data_Ass3/Intra/test/rest_105923_9.h5  \n",
            "  inflating: ./Data_Ass3/Intra/test/task_motor_105923_10.h5  \n",
            "  inflating: ./Data_Ass3/Intra/test/task_motor_105923_9.h5  \n",
            "  inflating: ./Data_Ass3/Intra/test/task_story_math_105923_10.h5  \n",
            "  inflating: ./Data_Ass3/Intra/test/task_story_math_105923_9.h5  \n",
            "  inflating: ./Data_Ass3/Intra/test/task_working_memory_105923_10.h5  \n",
            "  inflating: ./Data_Ass3/Intra/test/task_working_memory_105923_9.h5  \n",
            "   creating: ./Data_Ass3/Intra/train/\n",
            "  inflating: ./Data_Ass3/Intra/train/rest_105923_1.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/rest_105923_2.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/rest_105923_3.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/rest_105923_4.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/rest_105923_5.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/rest_105923_6.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/rest_105923_7.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/rest_105923_8.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_motor_105923_1.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_motor_105923_2.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_motor_105923_3.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_motor_105923_4.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_motor_105923_5.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_motor_105923_6.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_motor_105923_7.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_motor_105923_8.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_story_math_105923_1.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_story_math_105923_2.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_story_math_105923_3.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_story_math_105923_4.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_story_math_105923_5.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_story_math_105923_6.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_story_math_105923_7.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_story_math_105923_8.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_working_memory_105923_1.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_working_memory_105923_2.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_working_memory_105923_3.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_working_memory_105923_4.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_working_memory_105923_5.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_working_memory_105923_6.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_working_memory_105923_7.h5  \n",
            "  inflating: ./Data_Ass3/Intra/train/task_working_memory_105923_8.h5  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(file_name):\n",
        "    keys = [\"rest\", \"math\", \"memory\", \"motor\"]\n",
        "    classes = len(keys)\n",
        "    label = None\n",
        "    for cls, key in enumerate(keys):\n",
        "        if key in file_name:\n",
        "            label = F.one_hot(torch.tensor(cls), classes)\n",
        "            break\n",
        "    if label is None:\n",
        "        return torch.zeros((classes,))\n",
        "    else:\n",
        "        return label\n",
        "\n",
        "def to_index(label):\n",
        "    return label.argmax(axis=-1)\n",
        "\n",
        "class MEGDataset(IterableDataset):\n",
        "    def __init__(self, root, stride=1, window=1, transform=lambda x: x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root: folder with the files\n",
        "            transform: apply this function to each file\n",
        "            stride: when reading the data, advance of this number of time steps\n",
        "            window: return this number of timesteps\n",
        "        \"\"\"\n",
        "        # Store file locations\n",
        "        self.files = glob.glob(root + \"/*.h5\")\n",
        "        self.current = None\n",
        "        self.chunk_shape = None\n",
        "        self.transform = transform\n",
        "        self.current_label = None\n",
        "        self.stride = stride\n",
        "        self.window = window\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Open a new file if at the end of the current file\n",
        "        while len(self.files) != 0 or self.current is not None:\n",
        "            if self.current == None:\n",
        "                next_file = self.files.pop()  # Note: remove and return the last element\n",
        "                f = h5py.File(next_file,'r')\n",
        "                key = list(f.keys())[0] # Assume one dataset each file\n",
        "                tmp = torch.tensor(np.array(f[key]))\n",
        "                tmp = (tmp - tmp.min()) / (tmp.max() - tmp.min())\n",
        "                self.current = self.transform(tmp)\n",
        "                self.chunk_shape = self.current.shape\n",
        "                self.current_label = get_label(next_file)\n",
        "\n",
        "            for i in range(0, self.chunk_shape[1], self.stride):   \n",
        "                window = self.current[:, i:i+self.window].float() #.astype(np.float32)\n",
        "                window = np.concatenate((window, np.zeros((window.shape[0], self.window - window.shape[1]))), axis=-1).astype(np.float32)             \n",
        "                yield window, self.current_label.float()\n",
        "            self.current = None\n",
        "\n",
        "# If it fits in memory, we can use this\n",
        "class MEGDataset2(Dataset):\n",
        "    def __init__(self, root, stride=1, window=1, transform=lambda x: x):\n",
        "        # Store file locations\n",
        "        self.files = glob.glob(root + \"/*.h5\")\n",
        "        self.data = [None for i in self.files]\n",
        "        self.labels = [None for i in self.files]\n",
        "        self.len = 0\n",
        "        self.transform = transform\n",
        "        self.stride = stride\n",
        "        self.window = window\n",
        "        for i, file in enumerate(self.files):\n",
        "            f = h5py.File(file, 'r')\n",
        "            key = list(f.keys())[0] # Assume one dataset each file\n",
        "            tmp = torch.tensor(np.array(f[key]))\n",
        "            tmp = (tmp - tmp.min()) / (tmp.max() - tmp.min())\n",
        "            self.data[i] = self.transform(tmp)\n",
        "            self.labels[i] = get_label(file)\n",
        "            self.len += self.data[i].shape[1] // self.stride # Adjust length\n",
        "        self.chunk = self.data[-1].shape[1]        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        index = index * self.stride  # actual position\n",
        "        pos_index = index % self.chunk\n",
        "        data_index = index // self.chunk\n",
        "        window = self.data[data_index][:, pos_index:pos_index+self.window].float() #.astype(np.float32)\n",
        "        window = np.concatenate((window, np.zeros((window.shape[0], self.window - window.shape[1]))), axis=-1).astype(np.float32)\n",
        "        return window , self.labels[data_index].float()"
      ],
      "metadata": {
        "id": "TgcIOFZYVbQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data and make the datasets"
      ],
      "metadata": {
        "id": "DkEmTl6DVPIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_shape = (248, 35624) # Divisors of 35624: 2 2 2 61 73\n",
        "batch_size = 128\n",
        "original_rate = 2034\n",
        "new_rate = 500\n",
        "labels = 4\n",
        "target_names = [\"rest\", \"math\", \"memory\", \"motor\"]\n",
        "root = \"./\"  # Location of the dataset\n",
        "window = new_rate\n",
        "stride = 5 \n",
        "resampler = lambda x: torchaudio.functional.resample(x, original_rate, new_rate)\n",
        "\n",
        "def make_train_sets():\n",
        "    \n",
        "    intra_train = MEGDataset2(root + \"Data_Ass3/Intra/train\", stride, window, resampler)\n",
        "    cross_train = MEGDataset2(root + \"Data_Ass3/Cross/train\", stride, window, resampler)\n",
        "\n",
        "    # pin_memory=True should make that transfer cpu->gpu faster\n",
        "    # if it causes problems: remove it\n",
        "    intra_data_train  = DataLoader(intra_train, shuffle=True, batch_size = batch_size, pin_memory=True)    \n",
        "    cross_data_train  = DataLoader(intra_train, shuffle=True, batch_size = batch_size, pin_memory=True)\n",
        "\n",
        "    return intra_train, cross_train, intra_data_train, cross_data_train\n",
        "\n",
        "\n",
        "def make_test_sets():\n",
        "    intra_test = MEGDataset2(root + \"Data_Ass3/Intra/test\", stride, window, resampler)\n",
        "    cross_test1 = MEGDataset2(root + \"Data_Ass3/Cross/test1\", stride, window, resampler)\n",
        "    cross_test2 = MEGDataset2(root + \"Data_Ass3/Cross/test2\", stride, window, resampler)\n",
        "    cross_test3 = MEGDataset2(root + \"Data_Ass3/Cross/test3\", stride, window, resampler)\n",
        "\n",
        "    intra_data_test  = DataLoader(intra_test, batch_size = batch_size, pin_memory=True)\n",
        "    cross_data_test1  = DataLoader(cross_test1, batch_size = batch_size, pin_memory=True)\n",
        "    cross_data_test2  = DataLoader(cross_test2, batch_size = batch_size, pin_memory=True)\n",
        "    cross_data_test3  = DataLoader(cross_test3, batch_size = batch_size, pin_memory=True)\n",
        "\n",
        "    return intra_test, cross_test1, cross_test2, cross_test3, intra_data_test, cross_data_test1, cross_data_test2, cross_data_test3\n",
        "\n",
        "intra_train, cross_train, intra_data_train, cross_data_train = make_train_sets()\n",
        "intra_test, cross_test1, cross_test2, cross_test3, intra_data_test, cross_data_test1, cross_data_test2, cross_data_test3 = make_test_sets()\n",
        "\n"
      ],
      "metadata": {
        "id": "tc9eAqq0ZzaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cross - train samples:\", len(cross_train))\n",
        "print(\"Intra - train samples:\", len(intra_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXuJZyCAum50",
        "outputId": "4bbb709b-888d-4b33-9332-ac73031fc03b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross - train samples: 112064\n",
            "Intra - train samples: 56032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model"
      ],
      "metadata": {
        "id": "txbo_UBFVvhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how to use the convolutional layer\n",
        "# Input: 20 samples with 16 channels and length 30 \n",
        "# Output: 20 samples with 33 channels and length 28\n",
        "\"\"\"\n",
        "m = nn.Conv1d(16, 33, 3, stride=1) # channels in - channels out - kernel size\n",
        "input = torch.randn(20, 16, 30)  # batch - channels in - elements\n",
        "output = m(input)\n",
        "output.shape # batch - channels out - new length (from formula)\n",
        "--> torch.Size([20, 33, 28])\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tagerTENV8O4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "a11e460d-7240-4b24-bba1-4e329fb6a215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nm = nn.Conv1d(16, 33, 3, stride=1) # channels in - channels out - kernel size\\ninput = torch.randn(20, 16, 30)  # batch - channels in - elements\\noutput = m(input)\\noutput.shape # batch - channels out - new length (from formula)\\n--> torch.Size([20, 33, 28])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, num_blocks, size):\n",
        "        super().__init__()\n",
        "        self.conv1 = [nn.Conv1d(size, size, 3, padding=1, bias=True) for i in range(num_blocks)]\n",
        "        self.conv1 = nn.ModuleList(self.conv1)\n",
        "        self.bn1 = [nn.BatchNorm1d(size) for i in range(num_blocks)]  \n",
        "        self.bn1 = nn.ModuleList(self.bn1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data\n",
        "        for i in range(len(self.conv1[:-1])):\n",
        "            x = self.conv1[i](x)\n",
        "            x = self.bn1[i](x)\n",
        "            x = F.relu(x)\n",
        "        \n",
        "        x = self.conv1[-1](x)\n",
        "        x = self.bn1[-1](x)            \n",
        "        x = x + data\n",
        "        return F.relu(x)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.conv1)\n",
        "        \n",
        "class MEGClassifier(nn.Module):\n",
        "    # Input: batch size - 248 - #samples\n",
        "    def __init__(self, res_blocks, res_block_size=2, input_channels = 248, downsample=0.5, length=500):\n",
        "        super().__init__()\n",
        "        # First part: conv (e.g., ResNet layers)\n",
        "        # Output: some dimenstion x something that depends on the length of the input\n",
        "        # Second part: lstm / AdaptiveMaxPooling\n",
        "        # Third part: classifier (e.g., ReLU + Softmax)\n",
        "\n",
        "        self.res_blocks = [] # Part 1  \n",
        "        self.conv = []      \n",
        "        \n",
        "        prev_channels = input_channels\n",
        "        channels = input_channels\n",
        "        for i in range(res_blocks):            \n",
        "            self.res_blocks.append(ResidualBlock(res_block_size, prev_channels))\n",
        "            channels = int(channels * downsample)\n",
        "            self.conv.append(nn.Conv1d(prev_channels, channels, 1))            \n",
        "            prev_channels = channels\n",
        "\n",
        "        self.res_blocks = nn.ModuleList(self.res_blocks)\n",
        "        self.conv = nn.ModuleList(self.conv)\n",
        "\n",
        "        self.aggregate = nn.Sequential(nn.Conv1d(prev_channels, 1, 1, bias=True))\n",
        "        self.output = nn.Sequential(nn.Linear(length, length // 2), nn.ReLU(), nn.Linear(length // 2, 4))\n",
        "\n",
        "    def forward(self, x):        \n",
        "        for i, (res, conv) in enumerate(zip(self.res_blocks, self.conv)):\n",
        "            x = res(x)\n",
        "            x = conv(x)\n",
        "        x = self.aggregate(x)        \n",
        "        x = x.flatten(1)        \n",
        "        return self.output(x)\n",
        "        \n",
        "\n",
        "    def classify(self, x):\n",
        "        x = self(x)\n",
        "        return F.softmax(x, 1)\n",
        "\n",
        "# class MEGClassifier(nn.Module):\n",
        "#     # Input: batch size - 248 - #samples\n",
        "#     def __init__(self, res_blocks, res_block_size=2, input_channels = 248, downsample=0.5, length=500):\n",
        "#         super().__init__()\n",
        "#         self.lstm = nn.LSTM(input_channels,  10)\n",
        "#         self.lin = nn.Linear(10, 4)\n",
        "#         self.init_hidden()\n",
        "\n",
        "#     def init_hidden(self):\n",
        "#         self.hidden = torch.zeros(1, 128, 10).to(device), torch.zeros(1, 128, 10).to(device)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = torch.permute(x, (2, 0, 1))\n",
        "#         lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
        "#         return self.lin(lstm_out[-1])\n",
        "        \n",
        "#     def classify(self, x):\n",
        "#         x = self(x)\n",
        "#         return F.softmax(x, 1)"
      ],
      "metadata": {
        "id": "8xghEmq10u_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "2U_4C31fVx63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From pytorch doc\n",
        "def train_one_epoch(model, train, optimizer, loss_fn, epoch_index):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        train: train dataloader\n",
        "        test: test dataloader\n",
        "    \"\"\"    \n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    last_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(train):        \n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(inputs)        \n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:\n",
        "            last_loss = running_loss / 10 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))            \n",
        "        running_loss = 0.0                   \n",
        "    return last_loss\n",
        "\n",
        "def test_loop(model, test, loss_fn):\n",
        "    model.eval()\n",
        "    running_test_loss = 0.0\n",
        "    for j, test_data in enumerate(test):\n",
        "        test_inputs, test_labels = test_data\n",
        "        test_inputs = test_inputs.to(device)\n",
        "        test_labels = test_labels.to(device)\n",
        "        test_outputs = model(test_inputs)\n",
        "        test_loss = loss_fn(test_outputs, test_labels).item()\n",
        "        running_test_loss += test_loss\n",
        "    return running_test_loss / (j + 1)"
      ],
      "metadata": {
        "id": "PpmVBYcKkA_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_, test, epochs, lr=0.005, name=\"\"):\n",
        "    # From pytorch doc\n",
        "    history = []\n",
        "    val_history = []\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    best_loss = 1000000\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        #train = train_fn()\n",
        "        print('EPOCH {}:'.format(epoch + 1))\n",
        "        \n",
        "        avg_loss = train_one_epoch(model, train_, optimizer, loss_fn, epoch)\n",
        "        history.append(avg_loss)\n",
        "        avg_test_loss = test_loop(model, test, loss_fn)\n",
        "        val_history.append(avg_test_loss)\n",
        "\n",
        "        print('LOSS train {} valid {}'.format(avg_loss, avg_test_loss))\n",
        "                \n",
        "        if avg_test_loss < best_loss:\n",
        "            best_loss = avg_test_loss\n",
        "            model_path = f'model_{name}_{epoch}.pt'\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "    return history, val_history\n",
        "\n",
        "def test_metrics(model, test):\n",
        "    # Compute the predictions\n",
        "    model.eval()\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "    for i, test_data in enumerate(test):\n",
        "        test_inputs, test_labels = test_data\n",
        "        test_inputs = test_inputs.to(device)\n",
        "        test_labels = test_labels.to(device)\n",
        "        test_outputs = model.classify(test_inputs)        \n",
        "        \n",
        "        pred_labels.append(to_index(test_outputs))\n",
        "        true_labels.append(to_index(test_labels))\n",
        "    \n",
        "    pred_labels = torch.cat(pred_labels).cpu().detach().numpy()\n",
        "    true_labels = torch.cat(true_labels).cpu().detach().numpy()\n",
        "\n",
        "    # Compute the metrics\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    precision = precision_score(true_labels, pred_labels, average=\"macro\")\n",
        "    recall = recall_score(true_labels, pred_labels, average=\"macro\")\n",
        "    reports = classification_report(true_labels, pred_labels, target_names=target_names)\n",
        "    confusion = confusion_matrix(true_labels, pred_labels)\n",
        "    return pred_labels, true_labels, accuracy, precision, recall, reports, confusion"
      ],
      "metadata": {
        "id": "oAbj57ru5gog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(train_loss, val_loss, title):\n",
        "    fig, ax = plt.subplots(figsize=(15,8))\n",
        "    ax.set_title(title)\n",
        "    ax.plot(train_loss, label=\"Train loss\")\n",
        "    ax.plot(val_loss, label=\"Validation loss\")\n",
        "    ax.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BFiRFjqhd5e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intra case"
      ],
      "metadata": {
        "id": "TyOtVZ3jV0qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intra_classifier = MEGClassifier(3).to(device)"
      ],
      "metadata": {
        "id": "1A5WGeY8fjKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_train = lambda: make_train_sets()[2]\n",
        "history, val_history = train(intra_classifier, intra_data_train, intra_data_test, epochs=10, lr=1e-5, name=\"intra\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs2ZPztShNJQ",
        "outputId": "076a86bb-bbe1-426f-831d-b612b9119e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "  batch 10 loss: 0.1331861734390259\n",
            "  batch 20 loss: 0.12769933938980102\n",
            "  batch 30 loss: 0.12114590406417847\n",
            "  batch 40 loss: 0.1118475317955017\n",
            "  batch 50 loss: 0.10320370197296143\n",
            "  batch 60 loss: 0.0968689501285553\n",
            "  batch 70 loss: 0.0882046401500702\n",
            "  batch 80 loss: 0.07820179462432861\n",
            "  batch 90 loss: 0.07720863819122314\n",
            "  batch 100 loss: 0.06686313152313232\n",
            "  batch 110 loss: 0.06536509394645691\n",
            "  batch 120 loss: 0.0606478214263916\n",
            "  batch 130 loss: 0.05733447074890137\n",
            "  batch 140 loss: 0.060846567153930664\n",
            "  batch 150 loss: 0.05054659247398376\n",
            "  batch 160 loss: 0.04605489075183868\n",
            "  batch 170 loss: 0.047211676836013794\n",
            "  batch 180 loss: 0.048615530133247375\n",
            "  batch 190 loss: 0.040301543474197385\n",
            "  batch 200 loss: 0.03920078873634338\n",
            "  batch 210 loss: 0.03824765086174011\n",
            "  batch 220 loss: 0.030987456440925598\n",
            "  batch 230 loss: 0.029099428653717042\n",
            "  batch 240 loss: 0.028339624404907227\n",
            "  batch 250 loss: 0.027890872955322266\n",
            "  batch 260 loss: 0.026478847861289977\n",
            "  batch 270 loss: 0.025505781173706055\n",
            "  batch 280 loss: 0.021935808658599853\n",
            "  batch 290 loss: 0.023339588940143586\n",
            "  batch 300 loss: 0.017425534129142762\n",
            "  batch 310 loss: 0.01651703715324402\n",
            "  batch 320 loss: 0.01464422196149826\n",
            "  batch 330 loss: 0.013810350000858307\n",
            "  batch 340 loss: 0.013908605277538299\n",
            "  batch 350 loss: 0.011707979440689086\n",
            "  batch 360 loss: 0.014905428886413575\n",
            "  batch 370 loss: 0.010627645254135131\n",
            "  batch 380 loss: 0.01167924776673317\n",
            "  batch 390 loss: 0.010375981032848359\n",
            "  batch 400 loss: 0.010634581744670867\n",
            "  batch 410 loss: 0.010500282049179077\n",
            "  batch 420 loss: 0.008713673055171966\n",
            "  batch 430 loss: 0.008159254491329194\n",
            "LOSS train 0.008159254491329194 valid 0.09222281903705813\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.008866700530052184\n",
            "  batch 20 loss: 0.006261950731277466\n",
            "  batch 30 loss: 0.006498494744300842\n",
            "  batch 40 loss: 0.005410450696945191\n",
            "  batch 50 loss: 0.005109043419361114\n",
            "  batch 60 loss: 0.004937223345041275\n",
            "  batch 70 loss: 0.0062036018818616865\n",
            "  batch 80 loss: 0.004681010544300079\n",
            "  batch 90 loss: 0.004992208629846573\n",
            "  batch 100 loss: 0.003854059427976608\n",
            "  batch 110 loss: 0.004679221659898758\n",
            "  batch 120 loss: 0.005459311977028846\n",
            "  batch 130 loss: 0.0048217963427305225\n",
            "  batch 140 loss: 0.003724212199449539\n",
            "  batch 150 loss: 0.00401129350066185\n",
            "  batch 160 loss: 0.003659806400537491\n",
            "  batch 170 loss: 0.004647881537675857\n",
            "  batch 180 loss: 0.0037118472158908845\n",
            "  batch 190 loss: 0.004457220062613487\n",
            "  batch 200 loss: 0.0030312757939100266\n",
            "  batch 210 loss: 0.0024157483130693436\n",
            "  batch 220 loss: 0.005290297418832779\n",
            "  batch 230 loss: 0.003858178108930588\n",
            "  batch 240 loss: 0.004364877194166184\n",
            "  batch 250 loss: 0.002254158817231655\n",
            "  batch 260 loss: 0.0027662359178066255\n",
            "  batch 270 loss: 0.003510180488228798\n",
            "  batch 280 loss: 0.005143393203616143\n",
            "  batch 290 loss: 0.0023657899349927903\n",
            "  batch 300 loss: 0.0026783790439367295\n",
            "  batch 310 loss: 0.001753450371325016\n",
            "  batch 320 loss: 0.0018889699131250381\n",
            "  batch 330 loss: 0.0020865891128778458\n",
            "  batch 340 loss: 0.001566789299249649\n",
            "  batch 350 loss: 0.0019605685025453567\n",
            "  batch 360 loss: 0.00108812153339386\n",
            "  batch 370 loss: 0.0013795708306133746\n",
            "  batch 380 loss: 0.0021475713700056078\n",
            "  batch 390 loss: 0.002293798699975014\n",
            "  batch 400 loss: 0.0021162107586860657\n",
            "  batch 410 loss: 0.0017264122143387794\n",
            "  batch 420 loss: 0.0022228971123695374\n",
            "  batch 430 loss: 0.0025293681770563125\n",
            "LOSS train 0.0025293681770563125 valid 0.025574083033610473\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.0020945735275745394\n",
            "  batch 20 loss: 0.002137547917664051\n",
            "  batch 30 loss: 0.0017194563522934913\n",
            "  batch 40 loss: 0.002209334075450897\n",
            "  batch 50 loss: 0.0013081341981887817\n",
            "  batch 60 loss: 0.0033777795732021334\n",
            "  batch 70 loss: 0.0014261509291827678\n",
            "  batch 80 loss: 0.0009876280091702938\n",
            "  batch 90 loss: 0.0008669181726872921\n",
            "  batch 100 loss: 0.0009010970592498779\n",
            "  batch 110 loss: 0.0010047947987914085\n",
            "  batch 120 loss: 0.001430919300764799\n",
            "  batch 130 loss: 0.0010028960183262824\n",
            "  batch 140 loss: 0.0007250853814184666\n",
            "  batch 150 loss: 0.0012907606549561023\n",
            "  batch 160 loss: 0.0012174896895885468\n",
            "  batch 170 loss: 0.001834210753440857\n",
            "  batch 180 loss: 0.002169054001569748\n",
            "  batch 190 loss: 0.0025763645768165588\n",
            "  batch 200 loss: 0.0008635247126221657\n",
            "  batch 210 loss: 0.0011608384549617768\n",
            "  batch 220 loss: 0.002644078619778156\n",
            "  batch 230 loss: 0.0011259810999035835\n",
            "  batch 240 loss: 0.0007925160229206085\n",
            "  batch 250 loss: 0.0011769316159188747\n",
            "  batch 260 loss: 0.0015897899866104126\n",
            "  batch 270 loss: 0.0005237672012299299\n",
            "  batch 280 loss: 0.0012085443362593652\n",
            "  batch 290 loss: 0.0009213974699378014\n",
            "  batch 300 loss: 0.0005704908631742001\n",
            "  batch 310 loss: 0.00043465327471494677\n",
            "  batch 320 loss: 0.0018214177340269088\n",
            "  batch 330 loss: 0.0007030352484434843\n",
            "  batch 340 loss: 0.0005317786708474159\n",
            "  batch 350 loss: 0.0004767117090523243\n",
            "  batch 360 loss: 0.0005287447944283486\n",
            "  batch 370 loss: 0.0005362194031476974\n",
            "  batch 380 loss: 0.00044462555088102816\n",
            "  batch 390 loss: 0.0007850985042750835\n",
            "  batch 400 loss: 0.001012655347585678\n",
            "  batch 410 loss: 0.0005292857997119427\n",
            "  batch 420 loss: 0.00044917850755155087\n",
            "  batch 430 loss: 0.0005277660209685564\n",
            "LOSS train 0.0005277660209685564 valid 0.012098660834916782\n",
            "EPOCH 4:\n",
            "  batch 10 loss: 0.0006394220050424338\n",
            "  batch 20 loss: 0.00038301434833556416\n",
            "  batch 30 loss: 0.00034355386160314085\n",
            "  batch 40 loss: 0.0010536901652812957\n",
            "  batch 50 loss: 0.0005770019255578518\n",
            "  batch 60 loss: 0.0005581756588071585\n",
            "  batch 70 loss: 0.0002924325177446008\n",
            "  batch 80 loss: 0.0007886415347456932\n",
            "  batch 90 loss: 0.0010378082282841206\n",
            "  batch 100 loss: 0.0005519993137568235\n",
            "  batch 110 loss: 0.00045904675498604776\n",
            "  batch 120 loss: 0.0003093292936682701\n",
            "  batch 130 loss: 0.0006755348760634661\n",
            "  batch 140 loss: 0.0006574054248631\n",
            "  batch 150 loss: 0.00044670477509498594\n",
            "  batch 160 loss: 0.00036054924130439757\n",
            "  batch 170 loss: 0.0009018715471029281\n",
            "  batch 180 loss: 0.0003344650147482753\n",
            "  batch 190 loss: 0.00046179904602468015\n",
            "  batch 200 loss: 0.0004834587685763836\n",
            "  batch 210 loss: 0.0003651069710031152\n",
            "  batch 220 loss: 0.000276565202511847\n",
            "  batch 230 loss: 0.0002079485449939966\n",
            "  batch 240 loss: 0.00043106884695589545\n",
            "  batch 250 loss: 0.0013626842759549619\n",
            "  batch 260 loss: 0.000366866635158658\n",
            "  batch 270 loss: 0.0007075083907693624\n",
            "  batch 280 loss: 0.00030114082619547844\n",
            "  batch 290 loss: 0.0006794721819460392\n",
            "  batch 300 loss: 0.0016583032906055451\n",
            "  batch 310 loss: 0.0005227784626185894\n",
            "  batch 320 loss: 0.0006166292820125818\n",
            "  batch 330 loss: 0.001060793735086918\n",
            "  batch 340 loss: 0.0010411493480205537\n",
            "  batch 350 loss: 0.0008587820455431938\n",
            "  batch 360 loss: 0.00025204087141901256\n",
            "  batch 370 loss: 0.0008406773209571839\n",
            "  batch 380 loss: 0.00046957489103078843\n",
            "  batch 390 loss: 0.0002857577754184604\n",
            "  batch 400 loss: 0.0009479975327849388\n",
            "  batch 410 loss: 0.0004824650939553976\n",
            "  batch 420 loss: 0.00027322182431817056\n",
            "  batch 430 loss: 0.0004202025942504406\n",
            "LOSS train 0.0004202025942504406 valid 0.004552756425585936\n",
            "EPOCH 5:\n",
            "  batch 10 loss: 0.0003471452044323087\n",
            "  batch 20 loss: 0.0005558864213526249\n",
            "  batch 30 loss: 0.0002088857814669609\n",
            "  batch 40 loss: 0.0008012899197638035\n",
            "  batch 50 loss: 0.00029734952840954065\n",
            "  batch 60 loss: 0.00021289559081196784\n",
            "  batch 70 loss: 0.0005256818607449531\n",
            "  batch 80 loss: 0.00034731482155621053\n",
            "  batch 90 loss: 0.00012336361687630414\n",
            "  batch 100 loss: 0.00046083172783255576\n",
            "  batch 110 loss: 0.00035728944931179285\n",
            "  batch 120 loss: 0.00012529747327789665\n",
            "  batch 130 loss: 0.0005734247155487537\n",
            "  batch 140 loss: 0.0004997257143259049\n",
            "  batch 150 loss: 0.00020621931180357932\n",
            "  batch 160 loss: 0.0008112027309834958\n",
            "  batch 170 loss: 0.0008124666288495064\n",
            "  batch 180 loss: 0.0011003554798662662\n",
            "  batch 190 loss: 0.00017288512317463755\n",
            "  batch 200 loss: 0.0002518646419048309\n",
            "  batch 210 loss: 0.0005060294643044472\n",
            "  batch 220 loss: 0.00045693661086261274\n",
            "  batch 230 loss: 0.0001485695829614997\n",
            "  batch 240 loss: 0.00015643835067749023\n",
            "  batch 250 loss: 0.0001813484588637948\n",
            "  batch 260 loss: 0.0002290953416377306\n",
            "  batch 270 loss: 0.00028110742568969725\n",
            "  batch 280 loss: 0.00010606804862618446\n",
            "  batch 290 loss: 9.521456668153405e-05\n",
            "  batch 300 loss: 0.0004201953765004873\n",
            "  batch 310 loss: 0.00017313096905127167\n",
            "  batch 320 loss: 0.0001335212611593306\n",
            "  batch 330 loss: 0.0007189249619841576\n",
            "  batch 340 loss: 0.00020483736880123616\n",
            "  batch 350 loss: 0.0002192582003772259\n",
            "  batch 360 loss: 0.000315934419631958\n",
            "  batch 370 loss: 9.060846641659737e-05\n",
            "  batch 380 loss: 9.45492705795914e-05\n",
            "  batch 390 loss: 0.00014436382334679366\n",
            "  batch 400 loss: 0.00014550293562933802\n",
            "  batch 410 loss: 9.38098644837737e-05\n",
            "  batch 420 loss: 0.0008335921913385392\n",
            "  batch 430 loss: 7.744486210867763e-05\n",
            "LOSS train 7.744486210867763e-05 valid 0.0029450237612102434\n",
            "EPOCH 6:\n",
            "  batch 10 loss: 0.00010799246374517679\n",
            "  batch 20 loss: 9.37426695600152e-05\n",
            "  batch 30 loss: 0.0009455575607717037\n",
            "  batch 40 loss: 0.00022090598940849304\n",
            "  batch 50 loss: 0.00011664682533591985\n",
            "  batch 60 loss: 0.0001673238817602396\n",
            "  batch 70 loss: 0.00021260136272758246\n",
            "  batch 80 loss: 0.00012207928812131287\n",
            "  batch 90 loss: 0.00012964927591383458\n",
            "  batch 100 loss: 0.00015375540824607014\n",
            "  batch 110 loss: 0.00010784964542835951\n",
            "  batch 120 loss: 8.8242202764377e-05\n",
            "  batch 130 loss: 0.00032115234062075616\n",
            "  batch 140 loss: 6.464552134275437e-05\n",
            "  batch 150 loss: 0.0005573114845901728\n",
            "  batch 160 loss: 5.7474448112770915e-05\n",
            "  batch 170 loss: 0.0001388405216857791\n",
            "  batch 180 loss: 0.0001296633156016469\n",
            "  batch 190 loss: 9.61212906986475e-05\n",
            "  batch 200 loss: 0.00017806822434067725\n",
            "  batch 210 loss: 0.00014069178141653537\n",
            "  batch 220 loss: 0.0003833769354969263\n",
            "  batch 230 loss: 0.000272144959308207\n",
            "  batch 240 loss: 0.00019584216643124818\n",
            "  batch 250 loss: 5.7034182827919724e-05\n",
            "  batch 260 loss: 0.00035949251614511015\n",
            "  batch 270 loss: 0.0003686061128973961\n",
            "  batch 280 loss: 0.00010744223836809397\n",
            "  batch 290 loss: 6.455567199736834e-05\n",
            "  batch 300 loss: 0.00010473906295374035\n",
            "  batch 310 loss: 0.00013444375945255161\n",
            "  batch 320 loss: 0.000764561165124178\n",
            "  batch 330 loss: 0.0001347725512459874\n",
            "  batch 340 loss: 0.0008017161861062049\n",
            "  batch 350 loss: 0.0005552484188228845\n",
            "  batch 360 loss: 5.746984388679266e-05\n",
            "  batch 370 loss: 9.441216825507581e-05\n",
            "  batch 380 loss: 5.607586354017258e-05\n",
            "  batch 390 loss: 0.0003068022429943085\n",
            "  batch 400 loss: 0.0001889019156806171\n",
            "  batch 410 loss: 0.0008269809186458588\n",
            "  batch 420 loss: 6.671123555861414e-05\n",
            "  batch 430 loss: 0.0017225706949830055\n",
            "LOSS train 0.0017225706949830055 valid 0.002454711787603711\n",
            "EPOCH 7:\n",
            "  batch 10 loss: 6.545669166371226e-05\n",
            "  batch 20 loss: 4.6920066233724354e-05\n",
            "  batch 30 loss: 0.00044045294634997845\n",
            "  batch 40 loss: 0.0003776078810915351\n",
            "  batch 50 loss: 7.56762339733541e-05\n",
            "  batch 60 loss: 6.920485757291316e-05\n",
            "  batch 70 loss: 0.0013196060433983802\n",
            "  batch 80 loss: 6.700941012240946e-05\n",
            "  batch 90 loss: 3.357026143930852e-05\n",
            "  batch 100 loss: 0.00012918660650029778\n",
            "  batch 110 loss: 0.00014442773535847664\n",
            "  batch 120 loss: 0.00019031534902751445\n",
            "  batch 130 loss: 7.530224975198508e-05\n",
            "  batch 140 loss: 8.953126380220055e-05\n",
            "  batch 150 loss: 0.00024301905650645495\n",
            "  batch 160 loss: 8.425010018981993e-05\n",
            "  batch 170 loss: 6.041022134013474e-05\n",
            "  batch 180 loss: 0.000878510158509016\n",
            "  batch 190 loss: 3.683204122353345e-05\n",
            "  batch 200 loss: 0.0001389239332638681\n",
            "  batch 210 loss: 0.00012699299259111285\n",
            "  batch 220 loss: 0.000123170274309814\n",
            "  batch 230 loss: 0.00015204953961074353\n",
            "  batch 240 loss: 6.379373371601105e-05\n",
            "  batch 250 loss: 0.00029541929252445697\n",
            "  batch 260 loss: 3.155293234158307e-05\n",
            "  batch 270 loss: 8.75710102263838e-05\n",
            "  batch 280 loss: 0.00014774646842852235\n",
            "  batch 290 loss: 5.327644757926464e-05\n",
            "  batch 300 loss: 0.00010029395343735814\n",
            "  batch 310 loss: 5.938285030424595e-05\n",
            "  batch 320 loss: 5.851967725902796e-05\n",
            "  batch 330 loss: 5.817252676934004e-05\n",
            "  batch 340 loss: 0.00020619258284568787\n",
            "  batch 350 loss: 6.503859767690301e-05\n",
            "  batch 360 loss: 4.0963571518659594e-05\n",
            "  batch 370 loss: 0.00018587630474939943\n",
            "  batch 380 loss: 3.148846735712141e-05\n",
            "  batch 390 loss: 0.00010154469637200237\n",
            "  batch 400 loss: 3.060597518924624e-05\n",
            "  batch 410 loss: 3.8644031155854465e-05\n",
            "  batch 420 loss: 0.00022360575385391713\n",
            "  batch 430 loss: 6.0340738855302334e-05\n",
            "LOSS train 6.0340738855302334e-05 valid 0.00269449414719764\n",
            "EPOCH 8:\n",
            "  batch 10 loss: 9.166650706902147e-05\n",
            "  batch 20 loss: 2.2714919759891926e-05\n",
            "  batch 30 loss: 7.200926775112748e-05\n",
            "  batch 40 loss: 7.508528069593013e-05\n",
            "  batch 50 loss: 0.0005492499563843012\n",
            "  batch 60 loss: 3.21113649988547e-05\n",
            "  batch 70 loss: 4.5483798021450636e-05\n",
            "  batch 80 loss: 7.549034198746085e-05\n",
            "  batch 90 loss: 0.00017016577767208217\n",
            "  batch 100 loss: 4.569553420878947e-05\n",
            "  batch 110 loss: 7.34221306629479e-05\n",
            "  batch 120 loss: 6.411039503291249e-05\n",
            "  batch 130 loss: 4.230221966281533e-05\n",
            "  batch 140 loss: 6.279502413235605e-05\n",
            "  batch 150 loss: 2.933171926997602e-05\n",
            "  batch 160 loss: 3.453153476584703e-05\n",
            "  batch 170 loss: 0.0012886335141956806\n",
            "  batch 180 loss: 0.0009127997793257237\n",
            "  batch 190 loss: 0.0023567648604512215\n",
            "  batch 200 loss: 3.6621373146772386e-05\n",
            "  batch 210 loss: 0.00010062431683763862\n",
            "  batch 220 loss: 4.4619501568377015e-05\n",
            "  batch 230 loss: 8.462354307994246e-05\n",
            "  batch 240 loss: 3.9128575008362534e-05\n",
            "  batch 250 loss: 0.00012244901154190302\n",
            "  batch 260 loss: 9.265620028600096e-05\n",
            "  batch 270 loss: 9.287976427003741e-05\n",
            "  batch 280 loss: 0.0016137793660163879\n",
            "  batch 290 loss: 6.91612483933568e-05\n",
            "  batch 300 loss: 4.796117427758872e-05\n",
            "  batch 310 loss: 8.91868956387043e-05\n",
            "  batch 320 loss: 0.00011311169946566224\n",
            "  batch 330 loss: 0.00040414370596408844\n",
            "  batch 340 loss: 8.938548271544277e-05\n",
            "  batch 350 loss: 0.000133058603387326\n",
            "  batch 360 loss: 3.0788758886046706e-05\n",
            "  batch 370 loss: 4.795814747922122e-05\n",
            "  batch 380 loss: 9.203650988638401e-05\n",
            "  batch 390 loss: 2.6620220160111786e-05\n",
            "  batch 400 loss: 0.00013735948596149683\n",
            "  batch 410 loss: 7.131678867153824e-05\n",
            "  batch 420 loss: 1.914334570756182e-05\n",
            "  batch 430 loss: 4.641383420675993e-05\n",
            "LOSS train 4.641383420675993e-05 valid 0.0015500649031309876\n",
            "EPOCH 9:\n",
            "  batch 10 loss: 1.7388684500474483e-05\n",
            "  batch 20 loss: 2.4643915821798146e-05\n",
            "  batch 30 loss: 0.00033207801170647143\n",
            "  batch 40 loss: 3.335945657454431e-05\n",
            "  batch 50 loss: 4.228796460665762e-05\n",
            "  batch 60 loss: 2.1213287254795432e-05\n",
            "  batch 70 loss: 0.0001358488341793418\n",
            "  batch 80 loss: 0.0008074966259300709\n",
            "  batch 90 loss: 0.0001503372099250555\n",
            "  batch 100 loss: 5.978866247460246e-05\n",
            "  batch 110 loss: 2.475823857821524e-05\n",
            "  batch 120 loss: 4.000788321718573e-05\n",
            "  batch 130 loss: 6.468768697232008e-05\n",
            "  batch 140 loss: 4.445222730282694e-05\n",
            "  batch 150 loss: 4.288682830519974e-05\n",
            "  batch 160 loss: 7.160688983276486e-05\n",
            "  batch 170 loss: 2.132032532244921e-05\n",
            "  batch 180 loss: 4.1249676723964515e-05\n",
            "  batch 190 loss: 4.574777558445931e-05\n",
            "  batch 200 loss: 0.00018991255201399325\n",
            "  batch 210 loss: 0.00022437423467636108\n",
            "  batch 220 loss: 9.88891813904047e-05\n",
            "  batch 230 loss: 6.589457625523209e-05\n",
            "  batch 240 loss: 2.8355716494843364e-05\n",
            "  batch 250 loss: 1.5118812734726817e-05\n",
            "  batch 260 loss: 4.947806592099368e-05\n",
            "  batch 270 loss: 1.671951322350651e-05\n",
            "  batch 280 loss: 0.0013338092714548111\n",
            "  batch 290 loss: 0.0004495200701057911\n",
            "  batch 300 loss: 1.919227943290025e-05\n",
            "  batch 310 loss: 9.893603855744004e-05\n",
            "  batch 320 loss: 2.0601923461072146e-05\n",
            "  batch 330 loss: 0.0001049174927175045\n",
            "  batch 340 loss: 4.6064396155998113e-05\n",
            "  batch 350 loss: 1.3727866462431849e-05\n",
            "  batch 360 loss: 5.008622538298369e-05\n",
            "  batch 370 loss: 3.0622846679762006e-05\n",
            "  batch 380 loss: 1.2740447709802538e-05\n",
            "  batch 390 loss: 0.00011148644844070077\n",
            "  batch 400 loss: 1.7089725588448346e-05\n",
            "  batch 410 loss: 1.6119692008942366e-05\n",
            "  batch 420 loss: 2.1521441522054374e-05\n",
            "  batch 430 loss: 0.00021777176298201084\n",
            "LOSS train 0.00021777176298201084 valid 0.0035454798037542127\n",
            "EPOCH 10:\n",
            "  batch 10 loss: 0.002158166654407978\n",
            "  batch 20 loss: 4.945207037962973e-05\n",
            "  batch 30 loss: 9.220029460266232e-05\n",
            "  batch 40 loss: 7.295515388250351e-05\n",
            "  batch 50 loss: 2.5624394766055048e-05\n",
            "  batch 60 loss: 3.731512697413564e-05\n",
            "  batch 70 loss: 1.7895390919875352e-05\n",
            "  batch 80 loss: 0.0004605188500136137\n",
            "  batch 90 loss: 5.168053321540356e-05\n",
            "  batch 100 loss: 0.00012513847323134542\n",
            "  batch 110 loss: 0.0014226323924958705\n",
            "  batch 120 loss: 5.926144076511264e-05\n",
            "  batch 130 loss: 1.9632757175713778e-05\n",
            "  batch 140 loss: 3.8347599911503494e-05\n",
            "  batch 150 loss: 1.5479399007745086e-05\n",
            "  batch 160 loss: 3.353898646309972e-05\n",
            "  batch 170 loss: 0.0011820577085018158\n",
            "  batch 180 loss: 1.964700932148844e-05\n",
            "  batch 190 loss: 5.9577298816293475e-05\n",
            "  batch 200 loss: 3.2575574005022644e-05\n",
            "  batch 210 loss: 4.719675634987652e-05\n",
            "  batch 220 loss: 3.940637107007206e-05\n",
            "  batch 230 loss: 4.6688580187037584e-05\n",
            "  batch 240 loss: 1.0719557758420706e-05\n",
            "  batch 250 loss: 1.1741837079171092e-05\n",
            "  batch 260 loss: 4.243729636073112e-05\n",
            "  batch 270 loss: 1.1693880514940247e-05\n",
            "  batch 280 loss: 2.616696001496166e-05\n",
            "  batch 290 loss: 0.00013624064158648252\n",
            "  batch 300 loss: 1.2762438564095646e-05\n",
            "  batch 310 loss: 0.00032418943010270596\n",
            "  batch 320 loss: 0.00017810948193073273\n",
            "  batch 330 loss: 4.513177555054426e-05\n",
            "  batch 340 loss: 1.715480611892417e-05\n",
            "  batch 350 loss: 0.0001259146025404334\n",
            "  batch 360 loss: 1.8773539341054857e-05\n",
            "  batch 370 loss: 9.479942673351615e-06\n",
            "  batch 380 loss: 0.0003958726301789284\n",
            "  batch 390 loss: 2.082637802232057e-05\n",
            "  batch 400 loss: 0.0008398235775530338\n",
            "  batch 410 loss: 9.39238874707371e-05\n",
            "  batch 420 loss: 2.5176885537803172e-05\n",
            "  batch 430 loss: 4.8179793520830574e-05\n",
            "LOSS train 4.8179793520830574e-05 valid 0.3588757420737321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(history, val_history, \"INTRA classifier\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "Zjk4tUfkhcQ0",
        "outputId": "c4944350-1a03-41aa-ea91-02eecfff5d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model = MEGClassifier(3).to(device)\n",
        "model.load_state_dict(torch.load(\"model_intra_7.pt\"))"
      ],
      "metadata": {
        "id": "yTbFekoChmmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1b7318-24da-4819-aa1e-61224288bd12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels, true_labels, accuracy, precision, recall, reports, confusion = test_metrics(model, intra_data_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(reports)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnXrmNOVhids",
        "outputId": "6375cf3e-a1c6-434d-f44b-fe468579a703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.999785836664763\n",
            "Precision: 0.9997857958251795\n",
            "Recall: 0.9997859181692926\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        rest       1.00      1.00      1.00      3498\n",
            "        math       1.00      1.00      1.00      3504\n",
            "      memory       1.00      1.00      1.00      3503\n",
            "       motor       1.00      1.00      1.00      3503\n",
            "\n",
            "    accuracy                           1.00     14008\n",
            "   macro avg       1.00      1.00      1.00     14008\n",
            "weighted avg       1.00      1.00      1.00     14008\n",
            "\n",
            "Confusion matrix:\n",
            " [[3498    0    0    0]\n",
            " [   1 3503    0    0]\n",
            " [   1    1 3501    0]\n",
            " [   0    0    0 3503]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross case"
      ],
      "metadata": {
        "id": "AgTcVxf9V31O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_classifier = MEGClassifier(3).to(device)"
      ],
      "metadata": {
        "id": "36y20CMY0-u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_train = lambda: make_train_sets()[3]\n",
        "history, val_history = train(cross_classifier, cross_data_train, cross_data_test1, epochs=10, lr=1e-5, name=\"cross\")"
      ],
      "metadata": {
        "id": "_s-yYS671CRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082c4670-b582-4775-bdc2-4cf3a5b268c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "  batch 10 loss: 0.12807748317718506\n",
            "  batch 20 loss: 0.11980638504028321\n",
            "  batch 30 loss: 0.10599658489227295\n",
            "  batch 40 loss: 0.09648852348327637\n",
            "  batch 50 loss: 0.08954976201057434\n",
            "  batch 60 loss: 0.07914366722106933\n",
            "  batch 70 loss: 0.0752835214138031\n",
            "  batch 80 loss: 0.07331516146659851\n",
            "  batch 90 loss: 0.0702958881855011\n",
            "  batch 100 loss: 0.06352424621582031\n",
            "  batch 110 loss: 0.06572521328926087\n",
            "  batch 120 loss: 0.0555763840675354\n",
            "  batch 130 loss: 0.05724060535430908\n",
            "  batch 140 loss: 0.06175472140312195\n",
            "  batch 150 loss: 0.05558823347091675\n",
            "  batch 160 loss: 0.05200185775756836\n",
            "  batch 170 loss: 0.04113795161247254\n",
            "  batch 180 loss: 0.04205756783485413\n",
            "  batch 190 loss: 0.04151773750782013\n",
            "  batch 200 loss: 0.04132893979549408\n",
            "  batch 210 loss: 0.03932381570339203\n",
            "  batch 220 loss: 0.03525987267494202\n",
            "  batch 230 loss: 0.03238921165466309\n",
            "  batch 240 loss: 0.031010884046554565\n",
            "  batch 250 loss: 0.025905543565750123\n",
            "  batch 260 loss: 0.02766331434249878\n",
            "  batch 270 loss: 0.02998277246952057\n",
            "  batch 280 loss: 0.019153732061386108\n",
            "  batch 290 loss: 0.018919850885868072\n",
            "  batch 300 loss: 0.023731997609138487\n",
            "  batch 310 loss: 0.020101988315582277\n",
            "  batch 320 loss: 0.014592117071151734\n",
            "  batch 330 loss: 0.013888660073280334\n",
            "  batch 340 loss: 0.011822964996099472\n",
            "  batch 350 loss: 0.011590848863124847\n",
            "  batch 360 loss: 0.010777107626199722\n",
            "  batch 370 loss: 0.011633993685245514\n",
            "  batch 380 loss: 0.010069317370653152\n",
            "  batch 390 loss: 0.00899893194437027\n",
            "  batch 400 loss: 0.0072340801358222965\n",
            "  batch 410 loss: 0.007342779636383056\n",
            "  batch 420 loss: 0.006085451692342758\n",
            "  batch 430 loss: 0.009110457450151443\n",
            "LOSS train 0.009110457450151443 valid 1.4717079664742123\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.005790881440043449\n",
            "  batch 20 loss: 0.004778949543833733\n",
            "  batch 30 loss: 0.008288915455341338\n",
            "  batch 40 loss: 0.005415576323866844\n",
            "  batch 50 loss: 0.004453345388174057\n",
            "  batch 60 loss: 0.005041313171386719\n",
            "  batch 70 loss: 0.006740907579660416\n",
            "  batch 80 loss: 0.0041233494877815245\n",
            "  batch 90 loss: 0.0038822494447231294\n",
            "  batch 100 loss: 0.005119681358337402\n",
            "  batch 110 loss: 0.0045674234628677365\n",
            "  batch 120 loss: 0.0038318872451782227\n",
            "  batch 130 loss: 0.0029621612280607223\n",
            "  batch 140 loss: 0.0045887820422649385\n",
            "  batch 150 loss: 0.004836706444621086\n",
            "  batch 160 loss: 0.004241567105054855\n",
            "  batch 170 loss: 0.0025837667286396026\n",
            "  batch 180 loss: 0.0022272508591413497\n",
            "  batch 190 loss: 0.003273908793926239\n",
            "  batch 200 loss: 0.0020515631884336473\n",
            "  batch 210 loss: 0.002140184119343758\n",
            "  batch 220 loss: 0.002406958118081093\n",
            "  batch 230 loss: 0.0036502230912446977\n",
            "  batch 240 loss: 0.002447151765227318\n",
            "  batch 250 loss: 0.004335309565067291\n",
            "  batch 260 loss: 0.0023832663893699644\n",
            "  batch 270 loss: 0.002087710052728653\n",
            "  batch 280 loss: 0.0016520965844392776\n",
            "  batch 290 loss: 0.0019966674968600272\n",
            "  batch 300 loss: 0.0030002590268850327\n",
            "  batch 310 loss: 0.002914268895983696\n",
            "  batch 320 loss: 0.0012544263154268265\n",
            "  batch 330 loss: 0.002702091261744499\n",
            "  batch 340 loss: 0.001152446400374174\n",
            "  batch 350 loss: 0.001387944445014\n",
            "  batch 360 loss: 0.0010663479566574096\n",
            "  batch 370 loss: 0.002733423188328743\n",
            "  batch 380 loss: 0.0011460759676992893\n",
            "  batch 390 loss: 0.002767564915120602\n",
            "  batch 400 loss: 0.0014955177903175354\n",
            "  batch 410 loss: 0.0010671716183423996\n",
            "  batch 420 loss: 0.0010036120191216468\n",
            "  batch 430 loss: 0.0015164689160883428\n",
            "LOSS train 0.0015164689160883428 valid 2.389021939051862\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.0008660571649670601\n",
            "  batch 20 loss: 0.0008551638573408126\n",
            "  batch 30 loss: 0.0019139457494020463\n",
            "  batch 40 loss: 0.0005883784033358097\n",
            "  batch 50 loss: 0.0012264233082532883\n",
            "  batch 60 loss: 0.0008196135982871056\n",
            "  batch 70 loss: 0.0018685605376958848\n",
            "  batch 80 loss: 0.000911274366080761\n",
            "  batch 90 loss: 0.002510976418852806\n",
            "  batch 100 loss: 0.0008638449013233185\n",
            "  batch 110 loss: 0.0006912498734891415\n",
            "  batch 120 loss: 0.001183745265007019\n",
            "  batch 130 loss: 0.0005715705454349517\n",
            "  batch 140 loss: 0.0005752517841756344\n",
            "  batch 150 loss: 0.001244322769343853\n",
            "  batch 160 loss: 0.0008373815566301346\n",
            "  batch 170 loss: 0.0016437284648418427\n",
            "  batch 180 loss: 0.0008473861962556839\n",
            "  batch 190 loss: 0.0018831968307495117\n",
            "  batch 200 loss: 0.0026064299046993254\n",
            "  batch 210 loss: 0.000801285170018673\n",
            "  batch 220 loss: 0.0010750161483883858\n",
            "  batch 230 loss: 0.001754019968211651\n",
            "  batch 240 loss: 0.002084897831082344\n",
            "  batch 250 loss: 0.000823383592069149\n",
            "  batch 260 loss: 0.0017878176644444467\n",
            "  batch 270 loss: 0.0006617922335863113\n",
            "  batch 280 loss: 0.00048708291724324225\n",
            "  batch 290 loss: 0.0017621105536818505\n",
            "  batch 300 loss: 0.0003229038091376424\n",
            "  batch 310 loss: 0.0009126962162554264\n",
            "  batch 320 loss: 0.0011636678129434586\n",
            "  batch 330 loss: 0.0005175941623747349\n",
            "  batch 340 loss: 0.00031121058855205775\n",
            "  batch 350 loss: 0.0003850311506539583\n",
            "  batch 360 loss: 0.0029175132513046265\n",
            "  batch 370 loss: 0.0004774751607328653\n",
            "  batch 380 loss: 0.00025691501796245577\n",
            "  batch 390 loss: 0.00042705340310931204\n",
            "  batch 400 loss: 0.0006205425597727299\n",
            "  batch 410 loss: 0.0006088364869356155\n",
            "  batch 420 loss: 0.002115300111472607\n",
            "  batch 430 loss: 0.0004173004999756813\n",
            "LOSS train 0.0004173004999756813 valid 2.0592339134695976\n",
            "EPOCH 4:\n",
            "  batch 10 loss: 0.00024175469297915697\n",
            "  batch 20 loss: 0.0014428500086069106\n",
            "  batch 30 loss: 0.00033609543461352585\n",
            "  batch 40 loss: 0.0006231725215911865\n",
            "  batch 50 loss: 0.0014240519143640996\n",
            "  batch 60 loss: 0.0005506199784576892\n",
            "  batch 70 loss: 0.0004481128882616758\n",
            "  batch 80 loss: 0.0002514842664822936\n",
            "  batch 90 loss: 0.00027330187149345877\n",
            "  batch 100 loss: 0.0009232593700289726\n",
            "  batch 110 loss: 0.0004591977223753929\n",
            "  batch 120 loss: 0.000581506360322237\n",
            "  batch 130 loss: 0.0004451157059520483\n",
            "  batch 140 loss: 0.0007674509659409523\n",
            "  batch 150 loss: 0.0010328719392418862\n",
            "  batch 160 loss: 0.0006197301205247641\n",
            "  batch 170 loss: 0.00043324711732566356\n",
            "  batch 180 loss: 0.0005774396006017924\n",
            "  batch 190 loss: 0.0002392658032476902\n",
            "  batch 200 loss: 0.00020550964400172232\n",
            "  batch 210 loss: 0.00037849878426641224\n",
            "  batch 220 loss: 0.00019262301502749325\n",
            "  batch 230 loss: 0.0002153522800654173\n",
            "  batch 240 loss: 0.00015888292109593748\n",
            "  batch 250 loss: 0.0001302386401221156\n",
            "  batch 260 loss: 0.000167158676777035\n",
            "  batch 270 loss: 0.00018863140139728783\n",
            "  batch 280 loss: 0.0001832472044043243\n",
            "  batch 290 loss: 0.00021090651862323284\n",
            "  batch 300 loss: 0.00012487320927903056\n",
            "  batch 310 loss: 0.00021345519926398992\n",
            "  batch 320 loss: 0.00015970889944583178\n",
            "  batch 330 loss: 0.00017890953458845615\n",
            "  batch 340 loss: 0.00027292564045637845\n",
            "  batch 350 loss: 0.00031325488816946745\n",
            "  batch 360 loss: 0.00012672297889366747\n",
            "  batch 370 loss: 0.0003342212177813053\n",
            "  batch 380 loss: 0.0002461689990013838\n",
            "  batch 390 loss: 0.000180040649138391\n",
            "  batch 400 loss: 0.00032175311353057625\n",
            "  batch 410 loss: 0.0007678342051804065\n",
            "  batch 420 loss: 0.00017407406121492386\n",
            "  batch 430 loss: 0.000408171396702528\n",
            "LOSS train 0.000408171396702528 valid 2.0570530280639026\n",
            "EPOCH 5:\n",
            "  batch 10 loss: 0.00013655967777594923\n",
            "  batch 20 loss: 0.00015229624696075917\n",
            "  batch 30 loss: 0.00012748404406011106\n",
            "  batch 40 loss: 0.0002936708740890026\n",
            "  batch 50 loss: 0.000299932062625885\n",
            "  batch 60 loss: 0.00012516877613961698\n",
            "  batch 70 loss: 0.0006608917843550444\n",
            "  batch 80 loss: 0.0006954585202038288\n",
            "  batch 90 loss: 0.0002271676901727915\n",
            "  batch 100 loss: 0.0008549429476261139\n",
            "  batch 110 loss: 0.0011756060644984244\n",
            "  batch 120 loss: 0.0002740911906585097\n",
            "  batch 130 loss: 0.00036214222200214863\n",
            "  batch 140 loss: 0.0003194595919921994\n",
            "  batch 150 loss: 0.00015115723945200444\n",
            "  batch 160 loss: 0.0001076895510777831\n",
            "  batch 170 loss: 0.000130179722327739\n",
            "  batch 180 loss: 0.00018006149912253022\n",
            "  batch 190 loss: 0.0005690469406545162\n",
            "  batch 200 loss: 0.00022732554934918882\n",
            "  batch 210 loss: 0.00015751916216686369\n",
            "  batch 220 loss: 0.0003244043095037341\n",
            "  batch 230 loss: 0.00011274896096438169\n",
            "  batch 240 loss: 0.0004464808851480484\n",
            "  batch 250 loss: 0.0001606461824849248\n",
            "  batch 260 loss: 0.00012303655967116356\n",
            "  batch 270 loss: 0.00013375758426263927\n",
            "  batch 280 loss: 0.00023599981795996428\n",
            "  batch 290 loss: 9.315180941484868e-05\n",
            "  batch 300 loss: 0.00015308884903788566\n",
            "  batch 310 loss: 0.00015526963397860527\n",
            "  batch 320 loss: 0.00012378129176795482\n",
            "  batch 330 loss: 0.00012481627054512502\n",
            "  batch 340 loss: 0.00016301510622724892\n",
            "  batch 350 loss: 0.00012851846404373645\n",
            "  batch 360 loss: 0.000688008451834321\n",
            "  batch 370 loss: 7.760939188301563e-05\n",
            "  batch 380 loss: 0.000461942795664072\n",
            "  batch 390 loss: 8.438631193712354e-05\n",
            "  batch 400 loss: 9.76801966316998e-05\n",
            "  batch 410 loss: 0.00028277232777327297\n",
            "  batch 420 loss: 6.30006310530007e-05\n",
            "  batch 430 loss: 0.001553336251527071\n",
            "LOSS train 0.001553336251527071 valid 0.9257216003115318\n",
            "EPOCH 6:\n",
            "  batch 10 loss: 0.00034473198466002943\n",
            "  batch 20 loss: 0.0006722067482769489\n",
            "  batch 30 loss: 0.00010784078622236848\n",
            "  batch 40 loss: 7.761018350720406e-05\n",
            "  batch 50 loss: 0.00012428471818566323\n",
            "  batch 60 loss: 0.000883137434720993\n",
            "  batch 70 loss: 9.114082204177975e-05\n",
            "  batch 80 loss: 0.00022559361532330512\n",
            "  batch 90 loss: 5.784268141724169e-05\n",
            "  batch 100 loss: 4.912701551802456e-05\n",
            "  batch 110 loss: 0.00011077104136347771\n",
            "  batch 120 loss: 0.00030861387494951487\n",
            "  batch 130 loss: 7.4890221003443e-05\n",
            "  batch 140 loss: 0.00012416134122759104\n",
            "  batch 150 loss: 8.719650795683265e-05\n",
            "  batch 160 loss: 0.00010294390376657248\n",
            "  batch 170 loss: 0.0007701864000409841\n",
            "  batch 180 loss: 0.00014424416003748773\n",
            "  batch 190 loss: 6.540625472553074e-05\n",
            "  batch 200 loss: 0.00029627643525600436\n",
            "  batch 210 loss: 0.0003600398078560829\n",
            "  batch 220 loss: 5.4157833801582456e-05\n",
            "  batch 230 loss: 9.683227981440722e-05\n",
            "  batch 240 loss: 0.0006908842362463475\n",
            "  batch 250 loss: 0.00011060526594519615\n",
            "  batch 260 loss: 6.08323491178453e-05\n",
            "  batch 270 loss: 0.00018547035288065671\n",
            "  batch 280 loss: 0.0005053932778537274\n",
            "  batch 290 loss: 0.00013897172175347805\n",
            "  batch 300 loss: 7.807438378222286e-05\n",
            "  batch 310 loss: 0.0004903784487396478\n",
            "  batch 320 loss: 8.418692159466446e-05\n",
            "  batch 330 loss: 5.2725058048963545e-05\n",
            "  batch 340 loss: 6.226093391887843e-05\n",
            "  batch 350 loss: 0.00011121651623398065\n",
            "  batch 360 loss: 0.0005439897999167442\n",
            "  batch 370 loss: 9.232711745426059e-05\n",
            "  batch 380 loss: 6.314291385933757e-05\n",
            "  batch 390 loss: 8.049347088672221e-05\n",
            "  batch 400 loss: 0.00018121229950338603\n",
            "  batch 410 loss: 0.0005221538711339235\n",
            "  batch 420 loss: 0.0008745360188186169\n",
            "  batch 430 loss: 0.00027097377460449934\n",
            "LOSS train 0.00027097377460449934 valid 2.1806097050892723\n",
            "EPOCH 7:\n",
            "  batch 10 loss: 0.0002068155212327838\n",
            "  batch 20 loss: 9.800256229937076e-05\n",
            "  batch 30 loss: 7.6447450555861e-05\n",
            "  batch 40 loss: 0.00024471953511238096\n",
            "  batch 50 loss: 6.8773451494053e-05\n",
            "  batch 60 loss: 0.0028936421498656274\n",
            "  batch 70 loss: 0.000335286115296185\n",
            "  batch 80 loss: 9.546465589664877e-05\n",
            "  batch 90 loss: 0.00010577852372080087\n",
            "  batch 100 loss: 9.191699209623038e-05\n",
            "  batch 110 loss: 9.673972381278873e-05\n",
            "  batch 120 loss: 8.054684149101376e-05\n",
            "  batch 130 loss: 0.00017354029696434737\n",
            "  batch 140 loss: 0.000112921092659235\n",
            "  batch 150 loss: 7.880461635068059e-05\n",
            "  batch 160 loss: 0.0004776825662702322\n",
            "  batch 170 loss: 0.00011247992515563964\n",
            "  batch 180 loss: 0.00013509655836969615\n",
            "  batch 190 loss: 0.00013512453297153114\n",
            "  batch 200 loss: 3.381386632099748e-05\n",
            "  batch 210 loss: 4.006023518741131e-05\n",
            "  batch 220 loss: 0.00016359904548153282\n",
            "  batch 230 loss: 2.6937696384266018e-05\n",
            "  batch 240 loss: 0.00021874399390071632\n",
            "  batch 250 loss: 6.888614734634757e-05\n",
            "  batch 260 loss: 0.00011641962919384241\n",
            "  batch 270 loss: 9.858879493549467e-05\n",
            "  batch 280 loss: 0.00010314001701772213\n",
            "  batch 290 loss: 0.00010426758090034127\n",
            "  batch 300 loss: 7.159867091104389e-05\n",
            "  batch 310 loss: 0.00020144560839980842\n",
            "  batch 320 loss: 3.8766016950830816e-05\n",
            "  batch 330 loss: 0.00028585910331457853\n",
            "  batch 340 loss: 0.0005117232911288738\n",
            "  batch 350 loss: 0.00018432058859616519\n",
            "  batch 360 loss: 0.00010903014335781336\n",
            "  batch 370 loss: 6.492069223895669e-05\n",
            "  batch 380 loss: 6.832637591287493e-05\n",
            "  batch 390 loss: 0.000130860588978976\n",
            "  batch 400 loss: 3.909824590664357e-05\n",
            "  batch 410 loss: 0.0002486732788383961\n",
            "  batch 420 loss: 4.7057628398761155e-05\n",
            "  batch 430 loss: 5.72616292629391e-05\n",
            "LOSS train 5.72616292629391e-05 valid 2.4483326277066695\n",
            "EPOCH 8:\n",
            "  batch 10 loss: 6.269715959206223e-05\n",
            "  batch 20 loss: 3.519197052810341e-05\n",
            "  batch 30 loss: 2.6823376538231968e-05\n",
            "  batch 40 loss: 0.00018940037116408349\n",
            "  batch 50 loss: 9.298063814640045e-05\n",
            "  batch 60 loss: 8.930484764277936e-05\n",
            "  batch 70 loss: 0.00023958722595125438\n",
            "  batch 80 loss: 2.2488675313070416e-05\n",
            "  batch 90 loss: 6.151946145109833e-05\n",
            "  batch 100 loss: 3.870643849950284e-05\n",
            "  batch 110 loss: 3.6396124050952495e-05\n",
            "  batch 120 loss: 2.1682142687495798e-05\n",
            "  batch 130 loss: 3.1068903626874087e-05\n",
            "  batch 140 loss: 0.00014745083171874285\n",
            "  batch 150 loss: 5.75633137486875e-05\n",
            "  batch 160 loss: 2.121184370480478e-05\n",
            "  batch 170 loss: 3.3979094587266447e-05\n",
            "  batch 180 loss: 2.643405459821224e-05\n",
            "  batch 190 loss: 2.9801594791933894e-05\n",
            "  batch 200 loss: 0.0015023722313344478\n",
            "  batch 210 loss: 2.6047031860798596e-05\n",
            "  batch 220 loss: 6.225360557436943e-05\n",
            "  batch 230 loss: 7.820756291039288e-05\n",
            "  batch 240 loss: 2.2315679234452546e-05\n",
            "  batch 250 loss: 4.0420066216029224e-05\n",
            "  batch 260 loss: 5.339112831279635e-05\n",
            "  batch 270 loss: 0.00025119974743574857\n",
            "  batch 280 loss: 2.747180697042495e-05\n",
            "  batch 290 loss: 3.0111634987406433e-05\n",
            "  batch 300 loss: 4.343776381574571e-05\n",
            "  batch 310 loss: 3.62326594768092e-05\n",
            "  batch 320 loss: 1.9372966198716314e-05\n",
            "  batch 330 loss: 1.9094202434644104e-05\n",
            "  batch 340 loss: 3.963901835959405e-05\n",
            "  batch 350 loss: 2.890970790758729e-05\n",
            "  batch 360 loss: 6.680755759589375e-05\n",
            "  batch 370 loss: 4.12504596170038e-05\n",
            "  batch 380 loss: 0.00018739240476861596\n",
            "  batch 390 loss: 1.8058696878142654e-05\n",
            "  batch 400 loss: 9.385800221934914e-05\n",
            "  batch 410 loss: 3.094082640018314e-05\n",
            "  batch 420 loss: 1.6999855870380997e-05\n",
            "  batch 430 loss: 0.00024488780181854965\n",
            "LOSS train 0.00024488780181854965 valid 5.312863379839212\n",
            "EPOCH 9:\n",
            "  batch 10 loss: 0.000584021070972085\n",
            "  batch 20 loss: 3.770619514398277e-05\n",
            "  batch 30 loss: 5.0210201879963276e-05\n",
            "  batch 40 loss: 2.9385642847046256e-05\n",
            "  batch 50 loss: 0.0028004296123981475\n",
            "  batch 60 loss: 8.056600345298647e-05\n",
            "  batch 70 loss: 5.3836527513340116e-05\n",
            "  batch 80 loss: 0.00020216514822095631\n",
            "  batch 90 loss: 0.0010425559245049953\n",
            "  batch 100 loss: 1.1597535194596276e-05\n",
            "  batch 110 loss: 8.503238204866647e-05\n",
            "  batch 120 loss: 6.696759955957532e-05\n",
            "  batch 130 loss: 2.843453548848629e-05\n",
            "  batch 140 loss: 2.9878219356760384e-05\n",
            "  batch 150 loss: 6.967098452150821e-05\n",
            "  batch 160 loss: 6.394911906681955e-05\n",
            "  batch 170 loss: 7.209637551568449e-05\n",
            "  batch 180 loss: 1.549135777167976e-05\n",
            "  batch 190 loss: 2.0169891649857162e-05\n",
            "  batch 200 loss: 3.041691961698234e-05\n",
            "  batch 210 loss: 1.5655111928936095e-05\n",
            "  batch 220 loss: 4.041410866193473e-05\n",
            "  batch 230 loss: 2.1480648138094692e-05\n",
            "  batch 240 loss: 4.7692121006548406e-05\n",
            "  batch 250 loss: 3.4271448384970427e-05\n",
            "  batch 260 loss: 1.7714948626235127e-05\n",
            "  batch 270 loss: 1.6880955081433056e-05\n",
            "  batch 280 loss: 1.3122198288328945e-05\n",
            "  batch 290 loss: 1.5282307867892086e-05\n",
            "  batch 300 loss: 0.00044552949257195\n",
            "  batch 310 loss: 8.749192929826676e-05\n",
            "  batch 320 loss: 0.00028574331663548945\n",
            "  batch 330 loss: 0.00024410784244537352\n",
            "  batch 340 loss: 2.1745238336734472e-05\n",
            "  batch 350 loss: 0.00027243639342486856\n",
            "  batch 360 loss: 2.4483166635036467e-05\n",
            "  batch 370 loss: 2.162897144444287e-05\n",
            "  batch 380 loss: 2.541071444284171e-05\n",
            "  batch 390 loss: 9.910841472446919e-05\n",
            "  batch 400 loss: 2.3466045968234538e-05\n",
            "  batch 410 loss: 1.5982829791028052e-05\n",
            "  batch 420 loss: 3.615356399677694e-05\n",
            "  batch 430 loss: 0.000988349597901106\n",
            "LOSS train 0.000988349597901106 valid 3.2975718093491837\n",
            "EPOCH 10:\n",
            "  batch 10 loss: 8.237865986302496e-05\n",
            "  batch 20 loss: 0.00010142851388081909\n",
            "  batch 30 loss: 2.9507037834264337e-05\n",
            "  batch 40 loss: 1.3227359158918261e-05\n",
            "  batch 50 loss: 2.0240401499904693e-05\n",
            "  batch 60 loss: 1.3609361485578119e-05\n",
            "  batch 70 loss: 1.524255785625428e-05\n",
            "  batch 80 loss: 3.8592529017478226e-05\n",
            "  batch 90 loss: 1.6564634279347956e-05\n",
            "  batch 100 loss: 2.1250869031064214e-05\n",
            "  batch 110 loss: 1.8236818141303958e-05\n",
            "  batch 120 loss: 1.9142471137456597e-05\n",
            "  batch 130 loss: 3.9214815478771926e-05\n",
            "  batch 140 loss: 2.5810321676544846e-05\n",
            "  batch 150 loss: 1.8181274936068802e-05\n",
            "  batch 160 loss: 1.3329021749086678e-05\n",
            "  batch 170 loss: 0.0003506098873913288\n",
            "  batch 180 loss: 0.0010825232602655888\n",
            "  batch 190 loss: 3.1809799838811156e-05\n",
            "  batch 200 loss: 2.6079040253534913e-05\n",
            "  batch 210 loss: 3.889537183567882e-05\n",
            "  batch 220 loss: 2.042515407083556e-05\n",
            "  batch 230 loss: 8.769395208219066e-06\n",
            "  batch 240 loss: 2.022306143771857e-05\n",
            "  batch 250 loss: 1.31477223476395e-05\n",
            "  batch 260 loss: 1.895208115456626e-05\n",
            "  batch 270 loss: 0.00011132506188005209\n",
            "  batch 280 loss: 5.397595232352615e-05\n",
            "  batch 290 loss: 2.191366220358759e-05\n",
            "  batch 300 loss: 0.0001315040048211813\n",
            "  batch 310 loss: 0.0006872567813843488\n",
            "  batch 320 loss: 1.4148374611977488e-05\n",
            "  batch 330 loss: 1.4490573084913194e-05\n",
            "  batch 340 loss: 0.00013376522110775113\n",
            "  batch 350 loss: 1.2490290100686251e-05\n",
            "  batch 360 loss: 0.0004204226192086935\n",
            "  batch 370 loss: 1.640358241274953e-05\n",
            "  batch 380 loss: 4.7504197573289277e-05\n",
            "  batch 390 loss: 1.5989807434380054e-05\n",
            "  batch 400 loss: 5.656680441461504e-05\n",
            "  batch 410 loss: 9.436877735424787e-06\n",
            "  batch 420 loss: 3.296417417004704e-05\n",
            "  batch 430 loss: 1.2968946248292922e-05\n",
            "LOSS train 1.2968946248292922e-05 valid 4.166867064665807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(history, val_history, \"CROSS classifier - test set 1\")"
      ],
      "metadata": {
        "id": "U97pfv6O1PKh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "eb28ad38-f105-410c-d267-e294e9afc648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3EAAAH9CAYAAAC0mfXGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3hUZdrH8e/UFAKhhY6AlAdFEBA7KnZRsazYEeuuuuqu66u7drGufdVde0dFsa8NFVHsCEsv8ii995429f3jTIYkJCH9pPw+1zVX8px6n8kZOPc8zROPxxEREREREZH6wet2ACIiIiIiIlJ+SuJERERERETqESVxIiIiIiIi9YiSOBERERERkXpESZyIiIiIiEg9oiRORERERESkHvG7HYCIiNRtxpiJwBHAxdbaV9yNpvyMMRcBLwPfWmuHlLC+KXAncCrQGQgAS621XXe3r4iIiJuUxIlInWKMSQcuBE4E9gVaA3FgHTAV+BB4z1qbW2y/JUCXYoeLAduA34DPgH9bazeVIwYP8AfgTOBAoC0QBlYBE4FXrbWTynGcQcCVwGCgE07rh7WJ40xOHGu8tTa7hH1bJvY9CegNNAU2AWuAecC3wJfW2kW7i0NK9T5wTOL3bTjv73r3wqkdxpiuwEXAFmvtY+5G4zDGDAGGADOstR+6G83uGWOaA9cCWGtHVeE4nXC+INk/8eoPpANrrbXtqh6piDRUSuJEpM4wxgwDngMKP7xk4yRjXROvM4AHjDEXWGu/LuEw2cCOxO8BoCVwQOJ1uTHmSGutLSOGLsA7OA9UBbYDQZxkqjdwhTHmNeBP1tq8Uo5zF3Ar4EksigFbgA44yebBwF+B03ES08L7Hgh8BLQptHgb0ATol3idA/wXOK20axG2AhZYVnyFMaYPTgIXBg4vISkvdd8GoCtwB7AUqBNJHE4CdwfwKsU+D3VUc5x4AUZV4TjX4/w7ICJSIeoTJyJ1QqL52oc4CZwFLgBaW2szrLXNcB6ahuPUXnUADi/lUA9ba9slXq1warCuAvKA9sDoMmLoCvyMk8Btw3nAametbWatTcNJ4P6Nk5BdAHxujAmUcJxzgdtwEri3gUFASiKeNKAv8HdgTgn7NmdnAvc7cD7Q1Fqbaa3NSFzDOTi1SOHSrkXAWvuBtba3tXZkCav7JH7OKqlWdTf7ilSXOLAQGIvz782j7oYjIvWFauJExHXGmH2BZ3C+WPoMGF68uaS1divwHvCeMeZsnOaJu2Wt3QE8lUiO7gUOMMb0ttbOLxaDD3gTJ0laDwyx1s4rdiwL/MUY80Ni2yMSx/x7sdMWfLP+qbX27GLHiOIkb3OAh4wxqcX2PQcngcsHjrLWrii2/xqcB76xJewr5ZeW+LmjzK1Eatb11tq/FRQSX2aJiOyWkjgRqQvuAVKAlcB5xRO44qy1YxP91iriS5yEC2BvYH6x9X8ADkr8/ufiCVyx879tjDkC+DPwV2PMv6y1qwtt0jfx85PdBVVCc8yCfWcUT+DKsW+5GWNOAC7DueYsYDOwBCfmV621y8t5nIHAWTj9/vbA6T+4A5gNvA68nEhcS9r3COCaRAxtgFycvo9zgXHA89baWKHtm+L0QzoN6IVzz2zA6WP4TSLuOYW2v4hig5MYY0axsxkcwBHGmHih8pHW2onlGdjEGLMPcB1wJE7yn5eI/TXgRWttuNj2XYHFANZajzHmIJzal0MT1/9va+21JZ2ruhTrO9ql2LVDCYPXVPQ6E/u0AW4AhgLdAB/OlyPLga8S+y0t/J4kXGiMubDY4bpZa5eU8/oqdE8V2m8wcDXOfZyFcw9PB14E3rLWxgttOxHnC5yCcvH38M7y9pMr7bMhIrI7SuJExFXGmI44g3cAPJGocdutwg9V5VQ46fOVsP5POw9t3y3H8e4HrsDpK3cxcF8J23SsUIRFtTfGeCpxnWUyxgRxHkxHFFq8FcjAGcTlQJz/G0aV85BfAq0Sv+ckXi1xHnKPAE43xpxqrY0Ui+NPwLOFFuXg/F16JF6n4vSPyktsnwn8hJOAg9OkdStO0tge2A+IAjfuJt4dOIPLpAHNcJqkFh7sJlSOa8YYczXwODu7JezAeQ8PSbzONsacZK3NKWX/s3GSXH/iOmrrYX49znW3wHkPiw/kUnzAoApfZ6Jf6c84fxdwrm0bzuehE05/0FU4te9RnL9HBk6fzzyc96Owcr03Fb2nCu33AEVr07fhvD9HJ16nGGPOL5T8bcL58qB1ory2WCiq3RWRGqc+cSLitiHsTLA+qsHzHFfo9yIjOib6tR2SKP63PAdL1FRNTRSHFFv9v8TPa4wxh1UszOS+ewD3GmNSKrj/7vwLJ4GL4gyv385a2zzR325PnNqTVRU43pfAuUB7a20Ta20LnAfyC3BG0jwR+FvhHRIjkD6SKL4E7JHYNwMnIRyK01y1cI3JX3ESuPXAyTh9DFsCqTi1cjfi9C0qk7X24cSofwVNXn8q1IeynbX2p90dwxhzGk7fyGych/8sa21TnFEFT8DpyzgE570uzQs491o3a23zxL41PsiItXZ/nFpngOXFrr2dtXZswbZVuM47cBK4BTh9V4OJv1VBf9B7cO4NrLXLE3+PhxP7ji0hpt3WClfynsIY89fEta3F+SKnubU2EyehPCcR5znAPwq9h3+g0MBHJcT7MCIiNUw1cSLitr0SP/NxBjSpVsaYgoTi1sSiecC0Ypt1xXkwBZhZgcPPwnmY26vY8ruBL4BM4DtjzHycKQGmAL8Ac8uoYXsT54HRADcBVxpjvk7sNwX4pbTand1JjMh4ZaL4Z2vtc4XXW2sXs/NhulysteeVsCwbeN0YsxT4DqfZ6UOFNtkHJ9HLxhnhM1po303A54lXYQVNXR+x1n5aaPswTjLxQEXirqxE38mCZOtMa+0XhWIJAV8YY4bi3BuXGGNGFWtqW2AmcFZB7U6ipnJJjQZfAVW8zoK/1a3W2u8L7ZfPzv6g1a3C91Sin+w9ODVzx1trZxbaJxen3+ky4EfgBmPMI4lrFxFxnWriRMRtBU3xNldT08HrjTFrEq8NONMDPIVTY7MJGFHCeVoW+n1jBc61IfGzVeGF1tqvcPptLU0s6g1cjlP7MhtYbYx5yBhTZL/EvnnAUUBBotIcp+bkAeBrYIsx5iNjzMEViLPABTi1nvOLJ3A1IfEAvwXoaozpUGjVtsTPAMXeuzIU7NO+zK1q3hCcPmVzCic2hVlrFwKTcL4oHVLKcR4pqW9WHTKEyl+nG3+rytxTZ+Akfl8VTuAKs9b+jNNnrwVOk10RkTpBNXEi0tA0SbyKmwoMtdbWymTO1tqPjTHjcJpxHofT16wfTo1fW5wBLc41xhxTfKRMa+0q4GRjTG+cZHAwMBDnoTgADEus/5u19vEKhFVQQ/JZ5a9sV8aYM3GmQhiIMyhESaNmdmBnM83fE6+ewM/GmP/gDDphy0jkPwPOxhkdtBUwBvjBWru92i6kfAqa3fY0xqwpY7vMxM/Opaz/ufpCqhFVuc7PcO73B4wxPYF3gUm7G7CoiipzTxVc41G7ucaCL3k6U/f/biLSSCiJExG3FdR8taimgTySI8MZY5rhTPL9EM636I/i1EYVV3hgi/J+iw87BzbYVNLKRBO5zxKvgkFFDsfpj3UyzkAPY4wx+5V03Ynk7v6CciKpOwen31o68Kgx5ntrbfHmoaVpm/hZLRNYG2P8OPPgnV5ocT5ODWVBc7YsnFYfycTaWhs1xpyHMy/gnjh/l0eBTYmmo68BHxd+T6y1o40xh+L0WxqReMWMMbOAj4GnS2m2WN0KapdS2Pl+liW9lOWV+jLBGPM4TjJb3E+JvlrVpSrX+QDO5+0UnKa0fwYixpgpwAc4I0RuqcZYK3VPsfMa0yn971RYebYREakVSuJExG2/Jn6m4PQDKz70f6VZa7cBXxljjsFpxjjCGPOztfapYpsuwRmVLw3YF6eWpzz6JX6WOh1BsXhCOMOrf2WMeQG4FBgA9McZznx3+88HRiUeSifiJEcXsmsfv9ryR5wELgdnYJEPbLFpEYwxy3FGJCwyJYS19n+JWpo/4NRUDsZ5+B6eeI0zxgwr1rfpcmPMEzhTGhyOU9vTP/G6zhhzurV2fI1c6U4F3RD+a609rbIHqcLQ8pmUnFS1LGFZVVT6OhN9305NTKFwOs7faj+cUSkPxulfdmxpTRgrqxL3VME1Pm5reGoHEZHqpj5xIuK2b4GCb8dPqYkTWGs3snNgk3uMMS2KrQ/jDF4AzjDku2WM6czOPjLfViKsFwv93qsiO1prv8NpOlbRfQuGQu9S5lbld2bi593W2n+XkMD52FlbuQtrba619g1r7YXW2u44D9z/xLkfhuJM4VB8n7nW2justUfi9BcchpOgNwFeTYw0WpMK3sM9avg8JbLWXmSt9ZTwGlLNp6rydVprJ1lr/2GtPRinT9m5OLXAWTj9Q6tdBe8pV/+WIiJVoSRORFyVePAv6KN1TaIJ5G5VYrLv0TgPkC2A/ythfcFAH8YYM7wcx7sR59/QMM6k0BWVXej3yox4V7B/RfadlPg5tBLnK0mnxM/SahEPpeT+cSWy1i621t4MFAxzf8Rutg9Zaz9hZzLZHqdPVE0q6BPVLzHHYX1UMKBKWZ+har1Oa222tfYtds7HuJ8xpnDf1fLEVJnzlnVPFVzjEGNMWgUPXXgS+mqNWUSkPJTEiUhdcCtOX6pOOH3EynzwN8acBVxXkRMk+qcVzGd1dWLy6MLexxnCH+ApY8zelCJx/oJv9J9IDERSeP0x5XiwKzw0/4xC++5fQmzFz98Hp9lnkX3L4TWcGonexpjLK7BfaQomZe5bfEWiv9w9Je2U6BtYloIBMJJz5O1mn8IDZlT3vHrFTQCW40wi/VBZGxav8a1DCkZyLOs+q/R1lvNv5QEKb1cQU/OyzlVGDBW+p4B3cL4MaQHcvpvjF/9bbiv0e6ViFhGpCiVxIuI6a+0M4CqcBOMkYLoxZoQxJtnPxxiTaYz5gzHmG5xv1ZtW4lQvAJtxHl6vKRZDFGfQkLU4zb1+MsZcZ4xpUyiGXomBJcbg/Pv5A3BzCed5C5htjLnBGNPXGONN7O8zxuxtjHkGZ4JhcAZbWFxo37OBpcaYpxPJYPI6jTGtjDFX4vSr8+I8gJa7WZq1di7wbKL4pDFmVLHr65ZYtkszxlIU9D+7zRhzaqL5ZMEALB/jDCqTXcJ+JxpjfjbG/NEYk2zaaYxJN8b8EWekS3Dm2ivwlTHmCWPM4YVrTRIJ7SuJ4mqcppU1JtH09mqce/VcY8yHxpj+heIJGGMGGWMexBmavi76HacGOdMYc0ZJG1TxOucYY+5LfCERTGzvMcYcgDN5OMAUa+3mQvvMTfwcnOjXVlEVvqcSzaxvShRvNMY8b4zpVWjfNGPMYcaYp4Eik8AnBmYp+PLm4krEW3COgDGmdcELZ8oDAE/h5XX4CwERcYkGNhGROsFa+6IxZiNOktEbp9YIY8wOnAfJwknbUpw50yp6jh2JB7KbgWuNMY9Za3cUWr/IOPOvvYPT3+0R4BFjzDacof0LN7kaA1xWyuS/YaAP8GDiFTXGbMVJHn2FtvsRuKiEfTNxavquAEic30/R0fG2AOdYa5eX/x0A4FqcQTDOAu4A7jDGbElcX0HztjvLeayHE8fpjjMqYNgYkws0wxmd8jJgFCVP+XBQ4kVinzycGo2CGszP2NnElcQxr0m8Yon3M42dzTVzgAsSNa41ylr7kTHmUuAZnD6UpyauIZdd/8Z1jrU22xjzJjASeDfxXhaMFnm9tfbdxHaVvc42OMnRTey895vi3GPgjF56WbF9JgILce4la5w5HgsmtR9cvL9lKSp6T2Gt/Xei5vuuREyXGWOycZopZ7Lzy+4lJZzvBZwavEeMMXexc97Ix6y1j5WwfUkOBb4pYXkbio5guhToWs5jikgjoJo4EakzrLUFw4NfhfPAtQInefHjPES9i9MM0SQG96iMJ3Ae7loBV5YQw2Jgf5zk5B2cfnRBnKTkN5yHwEOsteeXMe+VwRnE4TmckSO34TwQ5gOLEtdxFnCYtbb49AQ344yqdw9Ok7aVOIlKAOeh7jvgFqBXaZMwl8Vam2+tPRvnofxjnJrHJjiTok9KHPv5ch5rE85D89M4fytwHvA/BI6w1r5Syq5f40z18CpOzVkOzkP+RpzavZHAsGIJ2WU4Sec3OH+TgoR6PvAfYB9r7YTyxF0drLUv4/ydH8OpRYriJJobcRKSOxLr66orcAb8mI/TxLBL4pVReKNKXuepiWP/iFNblYGTFM3CmTKjj7V2VrHzhIGjcb68WYnTxLEgpvJ84VyZe6rg3PfgNE9+DqeWsmBKjNU4NXd/Bw4r4Zx3Af9IXJenULxqXikiNc4Tj1d1SiYRERERERGpLaqJExERERERqUeUxImIiIiIiNQjSuJERERERETqkbo4OuV0oBuwA1jgciwiIiIiIiK1rQfOwFCLgQHFV9bFgU22UPYEpCIiIiIiIo3BVkoY9bYu1sTtADJjsTiRSNTtWJKCQeetCoVqfAoikUrRPSp1ne5Rqet0j0pdp3u08fD7fXi9HnByo13X12445bIA6BiJRNm6tbQpmGpfVpYzz3BdikmkMN2jUtfpHpW6Tveo1HW6RxuPzMy0gqS9xO5lGthERERERESkHlESJyIiIiIiUo8oiRMREREREalHlMSJiIiIiIjUI0riRERERERE6hElcSIiIiIiIvWIkjgREREREZF6pC7OE1ch8XicvLxscnJ2EA6HgHiNnGfDBh9AnZqAXKSwun2PeggEgqSnZ5Ca2gSPx+N2QCIiIiL1Vr1P4rZv30JOzrYaP08kEqvxc4hURd2+R+OEw/ls3ZpPOBymWbMWbgckIiIiUm/V6yQuPz83kcB5aNasBampTfB6a6aFqN/vHLduPyhLY1aX79FYLEZeXjbbtm0mJ2cbKSmppKSkuR2WiIiISL1Ur/vE5eXlAJCR0Yz09KY1lsCJSNV4vV7S05uSkdEM2PnZFREREZGKq9dZT35+HgApKekuRyIi5VHwWS347IqIiIhIxdXrJC4WcwZw8PsDLkciIuVR8Fkt+OyKiIiISMXV6ySuYCRKjXQnUt/UzCiyIiIiIo1BPU/iRKQ+0RcuIiIiIlWnJE5EREREql08HiO6cQbx/M1uhyLS4CiJExEREZFqF5o+itzPjyZn3JHE8ta7HY5Ig6IkTkRERESqVTxvA2H7nPN79nJCsx5wOSKRhqVeT/YtpRs8eFCF9xk69GRuuWVUtccyfPgw1qxZzTvvfET79h2q/fiF3XvvKMaN+4Sbb76DE08cVqPnEhERkZKFF4+FWDhZjix4lWivy/A17+1iVCINh5K4Bmro0JN3WbZx40YmT/6ZtLQ0hgw5epf1/fr1r43QREREpAGLx+OEF4wutjBGaNrtpB31tjtBiTQwSuIaqJJq1KZN+x+TJ/9MZmbzGqlxK83jjz9NJBIhK6tNrZ1TRERE3BFb9zPxbQucgi8dYnkQjxFdPYHIqgn4O+z6RbKIVIySOKlxHTt2cjsEERERqSXhBa8mf/fveRbEY0QSNXOhabfha3cEHq8eQUWqQp8gAYr2JevVqzevvPICs2bNYMuWzVx99bWcddZ55ORkM378F0ya9COLFi1kw4b1eL1eOnXqzJAhR3POOeeTkpK6y7FL6xN39dV/YsaMaTzxxDOkpzfh5ZefY/bsWeTl5dK5cxfOPPNsTj75tGq7xng8zhdffMYnn/yXBQt+JxTKp3XrLA488BBGjLiQtm3b7bLPsmVLGD36ZaZPn8rGjRsIBIJkZmbSs6fh+OOHFmmWGo1G+fjjD/j8889YvHgh+fn5NG3ajKysLAYMGMSIERfRokWLarseERGRuiaev5nIso+T5UCPkXjS2hFZ8h5EsolttUQWvEag18UuRilS/ymJkyJmz57Jww//k9atsxgwYCA5OTnJxOz333/noYfuo0WLluyxRxd6996LrVu3Mm/eXJ5//ml++OE7/vOf50hJSanQOX/55WfGjn2DPfbowv77H8i6dWuYPXsW999/D9u37+Dcc0dU+bri8Th33XUb48d/jt/vZ8CA/WjWrBnz5s3jgw/eYcKEL3nkkSfYa68+yX0WLlzAlVdeSk5ONl26dOXQQw/D4/Gwfv16Jk/+mfz8/CJJ3P333824cZ+QkpJCv379ycxsztatW1i5cgVjx77BkUceoyROREQatPDityGWD4C35b74Wu4LQLDPtYRm3gtAaNb9+LuegSfYzLU4Reo7JXFSxMcff8jIkZdw2WVX4PUWnYGiffv2PP740wwYsF+Rddu3b2fUqFv45ZefeOedNxkx4qIKnfONN17lxhtv4+STT00u++KLz7j77tt55ZXnOf304aSm7lrDVxEffPAu48d/TsuWrXjssafYc8/ugFN79u9/P8q7747ltttuZMyY9wgGgwCMHfsGOTnZXH75VVxwQdFvDHNycli0aEGyvGbNasaN+4Q2bdrywgujadmyVZHtf//d0rp1VpWuQUREpC6Lx+NECjWlDPQYufP33lcS/v1V4jkriOdvIDT3UVIGjHIhSpGGocEncU9PXs5DPy4hOxR1O5RyaxL0ccOhXbnygM61fu4uXbpy6aWX75LAAbRp05Y2bdrusrxp06Zce+31nHvuH5g48esKJ3FDhhxVJIEDOP74E3nttZdZsmQx8+fPo3//gRU6ZnFvvfU6AJdddkUygQPw+XxcddW1fP/9t6xZs5qJEydw3HFDAdi8eRMABx10yC7HS09PZ599+iXLBdsa03uXBA6gZ09TpfhFRETqutiGKcS2Wqfgb4K/6x+S6zz+NIIDbif/xz8BEJ7/LIGeF+PN6OJGqCL1XoOf7PvpKcvrVQIHkB2K8vSU5a6ce/DgI/D5fKWuj8fjzJw5g9GjX+Lhh+/nvvvu5N57R/Hqqy8CsHz50gqf85BDDitx+R57dAVgw4b1FT5mYevWrWXVqpV4vV6OP/7EXdYHAoFk4jZ9+tTk8oKmlQ899E+mTJlEKBQq9RxdunQlPb0JP/30A6NHv8SaNaurFLOIiEh9U2RAky6n4wkUbS7p7/IHvK32cwqxEKHpd9VmeCINSoOvibty/871sibuyv1rvxYOoF279qWu27RpI7fccgOzZ88qdZvs7OwKn7OkAUUAmjRpAlBm8lQe69c7SWCrVq1L7a/XoUPHxLbrksvOO28kM2fOYOrUyfztb1cTDAbp0aMX/fsP5PjjT6R79x7JbdPTm3DTTbfxz3/ezXPPPcVzzz1FVlYb+vTpyyGHDOboo4+rcF9BERGR+iIe2kpk6X+T5cJNKQt4PB5S9ruH3C+dL04jyz4kuu5P+NocWGtxijQUDT+JO6BztTRL9PudSstIJFblY9VlZSUa999/D7Nnz6Jv33255JI/0aNHL5o2bYrf7yccDnPkkQdX6pwej6ey4dboeVJTU3n88aeYO3cOv/zyE7Nnz2TOnNnMmzeHMWNGc+mll3PxxX9Mbn/kkccwaNCB/PDDt8yYMY3Zs2cyceIEJk6cwEsvPceTTz5fasIqIiJSn4UXvwPRXAC8zfvgbVVyNwhf1gH4u5xOZOkHAORPu5W047/A42nwjcNEqlWDT+KkeuTm5jJp0o/4fD4efPAxmjZtWmT9ihXuNP8sj6wsZ0CRDRvWEwqFkgOXFLZq1crEtrtOSN6nzz706bMPAOFwmPHjP+eBB+7hpZee4+ijj002+wSnf+DQoSczdOjJAKxcuYIHHriHadP+x9NP/5tRo+6t7ssTERFxlTOgyehk2d9jZJlfnAb7305k+WcQyye2cRqRJe8T6Da8NkIVaTD0tYeUS3b2DmKxGGlp6bskcABffjnOhajKp02btnTo0JFYLMaXX362y/pIJJKMf8CA/co8ViAQ4MQTh9GnT1/i8TgLFiwoc/uOHTsxcuQlACxY8Hslr0BERKTuim2cRmzLXKfgSyPQ7cwyt/dm7EGg9+XJcmjGXcQjuTUZokiDoyROyqVFi5Y0bdqMHTu28+WXnxdZN2nST4wdO8alyMrn7LPPB+D5559h6dIlyeXRaJSnnnqctWvX0K5d+yLzvr3//jssW7aE4lauXMHixYsAaNfOaR7522/zmTDhS/Lz83bZ/scfvy+yrYiISEMSXvBa8nd/l1PxBDN3u09wn7/hSWkNQDxnJeH5T9VYfCINkZpTSrn4fD5GjryEJ598jLvuupX333+bdu3as3LlCn79dS4XXHAxr732stthluoPfziT2bNn8tVXX3DRRecyYMB+NG3ajF9/ncuqVStp2rQZd999f5Gmlh999AGPPvoAHTp0ZM89u5OWls6mTRuZNWsG4XCYo48+jr33dppZrlmzhjvuuJnU1FR69epNmzZtiUTC/PabZdWqlaSnN+Gyy65w6/JFRERqRDy8ncjS95PlQPddBzQpiSfQjOC+N5E/+f8ACM19HH/3EXjTdp3KSER2Ve4kzhjzCnBhGZtYa23vKkckdda5546gffv2vPnm6yxevJBFixay557duf32uznuuKF1OonzeDzcccc9HHTQIXz88YfMmzeH/Px8WrfO4rTThnPBBRftMujIH/94JT/99D3z5s1hzpxZZGdn06JFS/r3H8iwYaczZMhRyW379NmHyy+/mhkzprFs2RKs/ZVAIECbNm0555wRDB9+dpkjf4qIiNRHkSXvQ8QZmdqbafBmHVDuff3dRxC2zxPbOh8i2YRm/pPUgx6rqVBFGhRPPB4v14aFkrgfgZI6Aq221t5UDTFNBI4IhSJs3Vp2++g1a5w5ydq1q/mJIhvL6JRSf9WXe7Q2P7dSt2RlOf1p16/f7nIkIiXTPVpxOeOOIrZpJgDB/e4jWKivW3lEVn9D3tcFg5p4SDtxIr4W+1RzlA2H7tHGIzMzjWDQD/AtMKT4+so0p3zBWvtK1cISERERkfosumlmMoHDm0Kg21kVPoa//ZH4OhxDdNVXQJzQ1FtJPfqDWpt+SKS+0sAmIiIiIlJh4cLTCuwxDE9Ki0odJ2XgneDxARBd+z3RlV9WS3wiDZmSOBEREeJa5aoAACAASURBVBGpkHgkm8jid5PlQI/yDWhSEm9mbwI9dg67kD/9duKxcJXiE2noKtOc8khjTD8gA1gL/ACMt9bW7Y44IiIiIlItIks/hMgOADxNu+Ntc0iVjhfs9w/CS96B8Hbi2xYQ/v1lguZP1RGqSINUmSSupK9a5hljzrHWzq5qQAWCQX+y82ZpNmzwEYnEkgM61IbaPJdIZdT9e9SD3+/d7edbGi797aWu0z26e6smvJ78vUX/P5LZplkVj9iULQfezOYfnDHyInMeot2gS/ClVq6JZkOne1Qq8rQ3A/gLsDdOLVwH4GRgZmLZV8aYjtUeoYiIiIjUGaH1s8hfM9kpeANk7HVBtRw3s//V+Jt1AyCWt4ktv9xXLccVaYjKXRNnrS0+cUc28KkxZjzO0JcHATcBV1dHYOWZYiASiSZ+1nxLzvoyfLs0XvXnHo0TiUQ1PHIjpKGxpa7TPVo++VOeSf7u73QSm7JTIbt63jP/vrcT+f5iALbNfIpIp/PxNuteLcduCHSPNh6FphgoUZXbXVlrQ8A/E8UTq3o8EREREamb4pEcp+9agr9n5Qc0KYmv8zC8WQc5hViY/Bl3VuvxRRqK6uo8Mz/xU80pRURERBqoyLKPILQVAE9GV3xtD6vW43s8HlL2uztZji7/lMjaH6r1HCINQXUlca0SP3dU0/FEREREpI4JL3gt+XugxwV4PNU/mJav1UD8Xc9MlkNTbyMer+tdBURqV3V98s5K/JxSTccTERERkToktnU+sfWTnILHj3/Pc2vsXMH+t4EvzTnv5llEFo+tsXOJ1EflSuKMMf2NMScbY3zFlvuNMf+HM2olwL+qO0ARERERcV94wc5pBXydTsCb1rbGzuVt0pHAXlcly6EZ9xCPZNfY+UTqm/KOTtkV+ADYZIyZBqzDaULZF2eqgRjwd2vtFzURpIiIiIi4Jx7NI7zorWQ50OPCGj9ncO9riCx4jXjeWuK5awjN+w8p/f5R4+cVqQ/K25xyJvA4YHHmhDsDOALIAV4GDrDWPlQjEYqIiIiIqyLLP4XQZgA8TTrjaz+kxs/pCWQQ7H9Lshye929iOatq/Lwi9UG5kjhr7WJr7bXW2kOstR2ttanW2jRrbU9r7SXW2qk1HahUzN1338bgwYO4995R5dr+0UcfYPDgQdx00/WVOt/q1asYPHgQw4cP22Xd8OHDGDx4EKtXV+wf3quv/hODBw9i2rT/VSqminrxxWcZPHgQL774bK2crzJq+z0REREBiCwYnfw90H1EjQxoUhJ/t3PwtujrFKK5hGbcUyvnFanraucTKLXupJNOBWDixAnk5OSUuW0oFGL8+C8S+51S47G5oawkU0REREoX27aAaMEw/x4v/u7n1dq5PV4fwYF3JcuRxWOJbpxea+cXqauUxDVQAwbsR4cOHcnNzeWbb74qc9vvv/+W7du30apVKw466JBqj+Xxx5/mjTfeJSurTbUfuzqdccbZvPHGu5xxxtluhyIiIlJnFBnQpONxeNM71Or5/e0Ox9fphGQ5f9rtxOPxWo1BpK5REtdAeTweTjzRqXUaN+6TMrf97LOPATj++JPw+8s71k35dezYiS5dutbIsatT8+bN6dKlK82bN3c7FBERkTohHg0RWfRmshzoPtKVOFIG3Ake5zkitu4nosvLfrYRaejq9lO1VMnQoSfz0kvPMXPmdFauXEHHjp122Wb9+nVMmeLM+XLyyU5TyjVrVjN+/OdMnjyJlStXsHnzJlJT0+jRoyfDhp3OccedsMtxyjJ8+DDWrFnNO+98RPv2Rb+927JlCy+99Czff/8tW7ZspnXrNhx99LFcdNFlpR6vovHde++oZCK7Zs1qBg8elFzXrl173n3XSWJffPFZXn75eS6++I9ceunlu5z3p59+4L333mb+/LlkZ2fTsmUrBg4cxIgRF9G1a7cyr3vFimW8/vqrzJ8/j0gkQvfuPRk58mIGDz6i/G/kbkQiYT744D0+++xTli5dQiQSoX379gwefATnnXcBmZm7Jqfz5s1hzJjXmDNnVuJ9TKV58xbstVcfhg07jf322z+5bX5+Pm+//SbffDOe5cuXEYlEaNasGe3adWC//fbnwgsvJSUlpdquR0RE3BddMY54/gYAPOkd8HU42pU4vM16EOh1KWHr9FvPn34nvo7H4fHp/x1pnJTENWBt27Zj0KADmDx5EuPGfcJll12xyzaff/4psViMvn37scceXZPLXnjhGTp27MQee3Shb99+rFu3jlmzZjB9+lTmzZvNtdfeUOX4Nm7cwJVXXsqqVStp3rwFhx56OKFQiPfeG8v06VPxeDwl7lfR+Pr1609ubg4TJ35NWloaQ4bs/A+opMSmJM888x9ef/0VvF4v/fr1p3XrLBYu/J3PP/+Ur7/+invueYBDDhlc4r6ffPJfRo9+id699+aggw5l+fKlzJs3h5tuup677vonRx55TAXetZLl5+dzww1/Zdq0/5GamsrAgYNISUll1qzpvPHGq0yY8CWPP/50kUR+ypRJ3HDDtUQiEXr1MvTtuy+RSIT169cxceIEmjRpkkziYrEYf//7tUydOoWMjAz69x9IkyYZbN68iWXLljJ69EucccZZSuJERBqYcKEBTfzdz8fjde/RMdj3BsKLx0JoC/Ediwn/9iLBvf7sWjwiblIS18CddNIpySTu0ksv3yUxKqihKjygyYEHHszhhx/Jnnt2L7Lt8uXL+Otfr+Tdd8dy7LFD6dNnnyrF9uijD7Bq1UoGDTqA++57iPT0JoBTO/iXv1zB8uXLStyvovENG3YagwYdwMSJX5OZ2ZxbbhlVoTh//vkHXn/9FdLS0njoocfp339gct2YMaN56qknuOuuW3nzzfdp0aLlLvuPGTOahx56vEh/w1deeYEXXniGZ599slqSuBdffIZp0/5Hly5deeyxp5L9D/Pz87j77tuZOPFr7rrrNp599uXkPq+99gqRSIQ77riHY48tWnu5desWVq9enSzPmjWDqVOn0KtXb5588nnS0tKS6+LxOLNnz6RJk4wqX4eIiNQdse1LiK6ZmCh5CHQf4WY4eFJaEOx7A6GpzrQDodkPEeh2Np7UVq7GJeKGBp/EhX59ktCsByCS7XYo5edvQrDfPwjudVWVD3XYYUNo1iyTtWvXMHXqFAYNOiC5bvbsmSxbtpS0tDSOOuq45PK99upT4rE6d96Diy66jAcfvJeJEydUKYlbs2YN3303EZ/Pxw033JxM4ACystpw1VXXcuON15W4b23EV9hbb70BwPDh5xRJ4ADOO28k3377DXPnzuajjz7gwgsv3WX/M844e5cBY84//0Leeut1VqxYzpo1a2jXrl2l48vPz+ODD94D4Lrr/l5kAJmUlFSuv/5mfvllEnPnzmbWrBn069cfgE2bNgFw0EGH7nLMzMzmRWopC7bdd98BRRI4cPpfFhxTREQajvDC15K/+zocjbfJrt0yalug5yWEf3uJ+PaFEN5GaPaDpOz/gNthidS6Bp/EhX99sn4lcACRbMK/PlktSVwwGOTYY4/nvffe5rPPPi6SxBUMaHLkkceQnp5eZL/8/HwmT/6ZX3+dx5YtmwmHw4DTBBJg+fKlVYpr5sxpxONx9t57nxL76g0efDgZGU3ZsWN7ifvXdHwFIpEIs2fPBEgOFFPciScOY+7c2UyfPrXEJK6kZpaBQIAOHTry22+WDRvWVymJmz9/Prm5OWRlZXHggQcRicSKrG/evDmHHnoYX331BdOnT00mXHvv3YclSxZx5523MHLkJfTp0xefz1fiOYzpjc/n45NP/kvnznswZMhRtGypbz5FRBqqeCxMZGGhAU16uDOgSXEeX5CUAaPI++4CAMK/v0yg16V4M3u5HJlI7WrwSVxgr6vqZU1coBoSuAInnXQK7733Nt999w3Z2Tto0iSDvLw8vv56fHJ9YXPmzOL2229i3bq1pR4zO7tq7+e6desA6NCh9GGK27Vrz4IFuyZxtRFfgW3bthIKhfB6vbRr177EbTp06AjAhg3rS1zftm3JCVpB7WMolF+lGDdscN7L9u07lrpNQYzr1++M8fLLr2LBgt+YNOknJk36idTUVHr33puBAwdx/PEnFkmuO3bsxDXX/I0nn3ycRx99gEcffYAOHTrSt28/Bg8+gsMPP7LUBFBEROqf6MoviOc5/896Utvi63jcbvaoPb5OQ/G1HezMXRePkj99FGlDxrgdlkitavBJXHCvq6qlRsvvd2ZjKF7LUR/06tWbnj178fvvvzFhwnhOOeV0Jk6cQHZ2Np067cG++w5IbpuXl8fNN9/Apk0bOfnkUznttOF06tSZ9PR0vF4vkydP4rrrrnZtfhY34yttoJXd8XprZyaPiobXqlVrXnjhNaZPn8qUKb8we/ZM5s2bw4wZ03j11Re54YabOfnkU5PbDx9+DkceeQzffz+RWbNmMmvWDL74YhxffDGOnj178Z//PKd+cSIiDUR4wc6mlP7u5+HxBlyMpiiPx0Nw4N3kjjsKiBNd+QWR1RPxtx/idmgitUbzxDUSBbVt48Z9nPhZMKBJ0SaCM2ZMY9OmjRizFzfeeBu9e+9FRkZGMhFZsWJ5tcSTlZUFUGTwjOLWrNl1XW3FV6BZs0yCwSCxWIzVq1eVuM2qVSsBaN06q1rPXV6tW7dJxFFyfM46J8aC972A1+tlv/3254orrubJJ5/n008ncMUVVxONRnn00QfJzt5RZPtWrVpz2mnDuf32u3n33Y95+eUxdO/eg99//43XX3+1mq9MRETcENuxnOiqCcmy2wOalMTXsh/+Pc9NlkPTbiMei7oYkUjtUhLXSBx33FCCwSCzZ89i8uRJTJv2P3w+H0OHnlxku23btgHQpk3bEo8zfvzn1RLPvvsOwOPxMHfubFauXLHL+p9++qHE/nCVjS8QcL5BjEYr9g+83++nb999AWdqg5IUJMQDBuxXoWNXl969e5OWlp6Y8++XXdZv3bqFH3/8Hth9jGlpaYwYcRFt2rQlFMpn2bKy+xb27NmLM888B4AFC36r5BWIiEhdEl70BuC0aPG1G4K3aVdX4ylNcN+bwe90TYhtmUdkkZpUSuOhJK6RaNYsMzmx9F133UY8HueAAw7apfaoS5euAEybNoWlS5ckl8diMV5++fnkIB9V1b59BwYPPpxoNMojj9xPbm5uct2GDet58snHStyvsvE1b96CQCDApk0bk4lgeZ199vkAvP32m8yaNaPIurfeep05c2aRkZHBsGGnVei41SUlJZXTTjsDgH/962E2bNiQXJefn8/DD99Pbm4Offr0LTKK5Jgxr7F27Zpdjjd//jw2btyA1+tNJstTp07h559/IBKJFNk2Go3y888/AtC2bcl9BkVEpP6IxyJEFryeLAd6XuhiNGXzprcnuPdfkuXQzPuIh0seEE2koWnwfeJkp5NOOoWvvx7Pli2bk+XijOnNIYccxk8/fc/FF5/HgAGDyMhowq+/zmPt2jWcd95IxowZvct+lfF//3cjCxb8zuTJkzjzzFPo338g4XCIadP+R7du3dlnn37MmTOrWuLz+/0cfPBgvvvuGy655Hz69t2XlJQUMjObc+WV15QZ5yGHDOb88y/kjTde5eqr/5Sc7HvRogUsWrSQYDCF226729XRGi+77Aqs/ZVp0/7Hueeenpzse+bM6WzcuIG2bdtx++13F9ln9OgXeeqpx+natRtdunQlEAiybt1a5syZRSwWY8SIi2jVqjUACxf+zhNPPEpGRga9evWmVavW5OXlMW/eHDZu3ECrVq0YMaLu/kcvIiLlE101gXiu053Bk9IaX8cTdrOHuwJ7/Znw768Qz11NPG8dobmPk9L/VrfDEqlxSuIakf33P5A2bdqybt1amjdvnqyZK+7eex9k7Ng3+OKLz5g+fSrp6Wn06dOXO+64l/z8vGpL4lq3zuK5517lxRef5YcfvuXHH7+jdessTj99OBdf/CduuOGv1RrfP/5xC82aNWPy5El8/fV4otEo7dq1320SB3DlldfQr19/3nvvbebPn8ecObNo0aIlxx9/IiNGXES3bntW6b2oqpSUFJ544knef/89xo37hGnTphKNRmjXrj3HH38i558/ssi8bwDXXfcPpkz5hfnz5zFt2lTy8/Np1ao1hx56GKeffiYHHHBQcttDDz2c7du3M3PmdFasWM6cObNIS0ujbdt2nHbaGZx22nBatGhR25ctIiLVLLxg5/+h/u7n4vEFXYxm9zz+dIL9byP/5z8DEP71KQI9LsSb0dnlyERqlsetUQbLMBE4IhSKsHVrbpkbrlnj9Ndp165LjQdVn0enlMahvtyjtfm5lbolK6spAOvXq7mT1E2N/R6N5awi58N9Ie78P5I+bDLeZt1djmr34vEYuZ8fS2yT0+XB3+UMUgc/53JUNaOx36ONSWZmGsGgH+BbYEjx9eoTJyIiIiJEFo5JJnC+toPrRQIH4PF4SdnvnmQ5svQ9ohumuBiRSM1TEiciIiLSyMVjUcILdw5o4u8x0sVoKs7X5mB8nXdOm5Q/9TbX5rQVqQ1K4kREREQaueiaicSzE3OtprTE3/kkdwOqhJQBd4DX6cMX2zCFyNIPXY5IpOYoiRMRERFp5AoPaBLodjYeX6qL0VSOt2k3AuZPyXJoxp3Eo3kuRiRSc5TEiYiIiDRisdw1RFd8niwHelzgYjRVE9znOkhxpvyJZy8nPP9ZlyMSqRlK4kREREQascjCNyEeAcCbdRDeTONyRJXnCWaS0u8fyXJozqPEcte5GJFIzVASJyK1Rp3MRUTqlng8VmRAk0A9G9CkJP4eF+Jp1sspRHYQmvWAuwGJ1IB6nsR5AIjF6va8WCLi2JnEeVyNQ0REHNE13xHfscQpBDPx73GKq/FUB4/XT8rAu5LlyMLRRLf86mJEItWvXidxfn8AgFBInVZF6oOCz2rBZ1dERNwVWfBa8vdAt7Pw+NNcjKb6+Docg6/9kU4hHiM0TVMOSMNSr5O41NR0ALZt20ReXg6xWEwfUJE6Jh6PE4vFyMvLYdu2TcDOz66IiLgnnreByIpPk2V/9/rflLKAx+MhOOAu8DiPutHV3xBdNcHlqESqj9/tAKoiPb0p+fl5hMN5bNmyvobPVtD8S0mi1FX14x4NBFJJT2/qdhgiIo1eeNFbEAsD4G09CF+LvV2OqHr5WuyNv/sFRBa8CkD+tNvwtR+Cx1uvH39FgHpeE+f1emnRIouMjOb4/UFqsp+N3+/F76/Xb5c0cHX7HvXg9wfJyGhOixZZeL11NU4RkcYhHo8XnRuuAQxoUpJgvxvBnwFAfNtvyYROpL6r919FeL1eMjIyycjIrNHzZGU5NQfr12+v0fOIVJbuURERKa/Yup+Ib1/oFAJN8Xc5zd2Aaog3rQ3Bff5GaMbdAOTPegB/1+F4gjX73ChS0/R1uIiIiEgjU7gWzt91OB5/ExejqVmB3lfgadLZKeRvJDTnX+4GJFINlMSJiIiINCLx/E1Eln2cLDfUppQFPL5Ugv3vSJbD9lli25e4F5BINVASJyIiItKIhBe/DbF8ALwt++Nr2c/liGqev8tpeFsPcgqxEPkz7nQ3IJEqUhInIiIi0kjE43EijWBAk+I8Hg8p+92TLEeXfUR03SQXIxKpGiVxIiIiIo1EbP1kYlutU/A3wd/1D+4GVIt8rffH32Xn9eZPu5V4POZiRCKVpyROREREpJEILyw0oEmXP+AJNK55O4P9bwdvCgCxjdOJLHnX5YhEKkdJnIiIiEgjEM/fQmTph8lyoGfjaEpZmDejM4G9rkyWQzPuJh7JcTEikcpREiciIiLSCISXvAPRPAC8LfbB23KAyxG5I9jnr3hSswCI56wi/OtTLkckUnFK4kREREQauJIGNPF4PC5G5B5PoBnBfW9OlkPzniCWs9rFiEQqTkmciIiISAMX2ziV2JZ5TsGXhr/rme4G5DL/nufjbb63U4hkE5r1T3cDEqkgJXEiIiIiDVx4wWvJ3/1dTsMTbOZiNO7zeH0EB96VLEcWjiG6abaLEYlUjJI4ERERkQYsHt5GZMn7yXKgx4UuRlN3+Nsfia/DcYlSnNC0W4nH467GJFJeSuJEREREGrDIkvch6ozA6M3sjbf1IJcjqjtSBo4Cjw+A6NofiK783N2ARMpJSZyIiIhIAxYuNKCJvxEPaFISb6Yh0POiZDl/2h3EoyH3AhIpJyVxIiIiIg1UdOMMYptmOgVvCoFuZ7kbUB0U7PsPCDh9BOPbFxL+/WWXIxLZPSVxIiIiIg1UkQFN9jgFT0oLF6OpmzyprQj2vT5ZDs1+kHj+ZhcjEtk9JXEiIiIiDVA8vIPIkneT5UCPkS5GU7cFel2GJ6ObUwhtITTnYXcDEtkNJXEiIiIiDVBk6YcQ2QGAp1kPvG0OdjmiusvjSyFlwB3Jcti+QGzbAhcjEimbkjgRERGRBqjwgCYBDWiyW77OJ+PNSiS68Qj500e5Go9IWZTEiYiIiDQw0c1ziW2c6hS8QQLdznE3oHrA4/GQst/dyXJ0xTgia753MSKR0imJExEREWlgIoWnFeh8Ep7UVi5GU3/4Wg3A3+3sZDk07TbisaiLEYmUTEmciIiISAMSj+QQXvx2suzXgCYVEux/C/jSAIhtnk1k8VsuRyQ1KR6PE4/H3Q6jwpTEiYiIiDQgkWX/hfA2ADxN98TXdrDLEdUv3vSOBPa+OlkOzbyXeHiHixFJTYjH44R+fYqcD/qQ981ZxGMRt0OqECVxIiIiIg1I4bnhAt1H4PHoca+igntfgyetLQDx3LWE5v3b5YikOsVDW8n7bqTTXDZ3LdHVXxPb+pvbYVWIPtUiIiIiDUR0y3xi639xCh4//j3PdTegesrjb0Jw31uT5fCvTxLLXuliRFJdoptmkzPuaKIrPksu87U7HG/z3i5GVXFK4kREREQaiMjCnbVwvk5D8aa1cTGa+s2/5zl4W/RzCtFcQjPvcTcgqZJ4PE54wevkfnE88R2Lk8sD5nJSh4ytdzXW9StaERERESlRPJpHeNHYZDnQ80IXo6n/PB4vwUJTDkQWv0104zQXI5LKikdyyJ90Nfm//BVi+c5CfxNSB79EyqD78PiC7gZYCUriRERERBqAyLJPILQZAE+TPfC1O8LliOo/f9vB+DqdmCznT72tXo5k2JjFti0g94vjiSzaOcqoN3Mv0odOwN/lVBcjqxolcSIiIiINQLjQ3HCBHhrQpLqkDBgF3gAAsfWTiC7/2N2ApNwiS/9LzrijiW2Zl1zm73Y2aSd8ibdZTxcjq7pKf7qNMfcZY+KJ1/XVGZSIiIiIlF9s2wJi6350Ch4f/j3PczegBsTbrDuBXpcly/nT7yQezXcxItmdeDRE/v9uJu+HSyCSmB7Cm0LKgY+RcvCTePzp7gZYDSqVxBlj9gf+Dqg+WURERMRlhacV8HU8Dm96exejaXiC+1wPwRYAxHcsIWyfdzkiKU0seyW5Xw0jbJ9NLvNkdCXt+M8J9LgAj8fjYnTVp8JJnDEmBXgVWAv8t9ojEhEREZFyi0fzCS96M1kO9BjpYjQNkyelOcG+NyTLoTkPE8/b4GJEUpLIqq/JGTeE2Ib/JZf5Op1I+tCv8bXs52Jk1a8yNXF3AXsBVwBbqzccEREREamIyIpxkL8RAE96B3ztj3Y5ooYp0OsSPE27O4XwdkKzH3Q3IEmKx6Lkz7qfvG/OgvxNzkKPj+DAu0g9fDSeYKa7AdaACiVxxpgDgf8Dxlhr1atTRERExGWRQgOa+LuPwOP1uRhNw+XxBkgZeFeyHP79FWJb57sYkQDE8zaQ982ZhGc/REFPL09aW9KO+S/Bva5qMM0niyt3EmeMScVpRrkJ+GuNRSQiIiIi5RLbvpjomm+dgsdLoPv57gbUwPk6Ho+v7WFOIR4lf9ooV+Np7KLrfiHnsyE7PwOAr+1hpA2diK/NwS5GVvP8Fdj2XsAA51hra7wRcDDoJyuraU2fpsLqYkwihekelbpO96jUdfXpHt3029vJ39O6nkDbrnu5GE3jkH/0I6wacyAQJ7pqPE1yJpHe5dhajaE+3aM1IR6Ps236E2z64SaIRZLLmx9wE80Pur1R1EaXqybOGHMIcC3wobV2bM2GJCIiIiK7E4+G2TH31WS56T6XuBhN45HSpj8ZfS5Kljd993fihRIJqVmx/K2s+/RsNn13QzKB86a2pO2pH9HikDsbRQIH5aiJM8akAa8A24A/13RABUKhCFu35tbW6Xar4BuP9eu3uxyJSMl0j0pdp3tU6rr6do9Gln1MNGct4PQBys44jJx6Ent9FzPXg30bItmEN85l9aRnCPS8sMbPW9/u0eoW3TSbvB8uIb59UXKZt9VAUg97mewmnchuQO9LZmYawWDpqVp5auLuA3oC11lrV1dXYCIiIiJSeYXnhvPveT4eb0V6yUhVeNPaEdz7L8lyaNY/iYe3uRhRwxde+Aa5X55QJIELmD+SduyneJt0cjEyd5Tn0346EAMuNMYU/4qhd+LnlcaYk4EF1trLEBEREZEaE9uxjOjqrxMlD4EeI1yNpzEK7HUV4QWjieesJJ63ntCcx0gZcLvbYTU48UgO+VP+QWTRmJ0L/U1IOfBxAl1Pdy8wl5X3KxsvcEQZ6/dMvJpXOSIRERERKVN44RsUDKfuaz8Eb0YXdwNqhDz+NIL9byP/pysACM9/hkDPi/Bm7OFyZA1HbNtC8r6/mNiWucll3szepB72Mt7MXi5G5r7dJnHW2q6lrTPGvAJcCNxgrX24+sISERERkZLEYxEiC19PlgM9RroYTePm73oGYfsssY3TIZZPaMbdpA5+3u2wGoTIso/I+/kaiOxILvN3O4uUAx7G42/iYmR1Q4Um+xYRERERd0VXfUU8dw0AntQsfB1PcDmixsvj8ZIy8N5kObL00vLXhgAAIABJREFUfaLrp7gYUf0Xj4bIn3oLed9fvDOB8wZJOeBRUg5+SglcgpI4ERERkXokvGB08nf/nufi8QVdjEZ8bQ7Ev8epyXL+1FuIx+MuRlR/xXJWkvvVKYTnP5Nc5snoQtrxnxPoeSEej8fF6OoWJXEiIiIi9UQsZyXRVeOT5UCPC1yMRgoE+98OXieZjm2cSmTpBy5HVP9EVn9DzmdHEtuwsybT1+kE0k/4Gl/LfV2MrG6q0li01tqLgIuqJRIRERERKVNk4RiIxwDwtT0Mb9M9XY5IALxNuxLofTnhef8GIDT9TvydhuLxp7kcWd0Xj0UJz3mE0OwHKRisB4+PYP/bCOx1tWrfSqGaOBEREZF6IB6LEl6wc0ATvwY0qVOCfa6DlFYAxHNWFGkSKCWL520gb+LZhGY/QEEC50lrS9rRHxLc+xolcGVQEiciIiJSD0RXf0M8Z4VTSGmJv/NJ7gYkRXiCzUjpd2OyHJr7L2K561yMqG6Lrp9MzmdDiK7+JrnM13YwaUO/wdf2EBcjqx+UxImIiIjUA+GFOwc0CXQ7B48vxcVopCT+HiPxZhqnEMkmNOuf7gZUB8XjcULznyF3/DDiuauTywN9/kbqUe/hTWvrYnT1h5I4ERERkToulruG6IrPk2UNaFI3ebx+ggPvTpYjC18nunluGXs0LvHwNvK+v5jQ1FsgHnEWBpuTOuRNUvrfisdbpeE6GhUlcSIiIiJ1XGThmxCPAuBtcwjezF4uRySl8Xc4Gl/7o5xCPEZo2m2acgCIbp5LzrhjiC7/OLnM22oA6UO/wd/xOBcjq5+UxImIiIjUYfF4jPDC15Jl1cLVfcGBd4PHecyOrvm2yLQQjVF44RhyvziO+PaFyWWBXpeSduyneDP2cDGy+ktJnIiIiEgdFl3zLfEdS51CsDn+zsPcDUh2y9e8d5HRQ/On3U48FnYxInfEI7nkTfoL+ZOugWies9DfhJRDnyNl/wfVr7MKlMSJiIiI1GGRBYVq4bqdpbnH6olgvxsh0BSA+LbfCf/+qssR1a7Y9kXkfnkCkYVvJJd5Mw3pJ4wn0PUMFyNrGJTEiYiIiNRRsbz1RFZ8lixrbrj6w5ua5cwdlxCa/QDx/C0uRlR7Iss+JmfcUcQ2z0ku83f9f/buO06OuvD/+Htm6116DymEkDKEFiABQoAUkkACKiBNagDxa0W/+rWgfkERBQvys4AgKoIUQUBQvwKSRkIJkd4ZA0mA9Esvt3u7OzO/P2YzubvcXe5yuze7t6/n48Fj8/nszu77dLO59075nK2qmXN2X70T7UKJAwAAKFG5ZfdL+cPwzL5HK9JzTMiJ0Baxg/5LRpf8OV91m5R566ZwAxWZ52ZV99LVSj99qZTd7k+acSWOvlGJibfJiHYJM16nQokDAAAoQZ7nKVv/UEr2wpUdI5JU/MjvBeOsfbvc7ctDTFQ8bu1qpeaeruy7vwnmjC77q+rkxxUbfZkMwwgxXedDiQMAAChBzvpnd1/NL9ZN0WGnhxsI+yS6/+ky+x7jD9ys6l65NtxARZBb85RSj02VW7MkmIsMPkXVsxYo0ueIEJN1XpQ4AACAEpRb+qfgz7EDzuFQtDJlGIYS43YvAO589A85654LMVHheJ6rzBs/U3r+2fLqNviThqn4EdcoOfkeGYme4QbsxChxAAAAJcar26RcvUWRuaBJeYv0Ha/oAWcH47qX/1ee54aYqP289EalF5ynzOs/luQvZm4k+6tq2qOKH/IVGQY1o5j4XxcAAKDEZJc9ILkZSZLZ50hFeh8WciK0V/yIq6VIUpLkbnpNueUPhpxo3zkbXlDt41PkrJkfzJn9j1fVqQsUGXB8iMkqByUOAACghPgXNKl3KCV74ToFs8sQxQ76QjDOvHqdvNzOEBO1ned5yrz7W6We/Ji82tXBfOzgr6hq2l9lVg0MMV1locQBAACUELdmibxt//EH0S6KDjsz3EAomPghX5aR7C9J8lJrlH3nN3vZonR42W2qe+bTyrz0HcnL+ZPxHkpOvk+JI6+RYUbDDVhhKHEAAAAlpP5euOgBZ8uIdQsxDQrJiHVTfOx3gnHmrV/JrV0TYqLWcTa/rdrHpyv34d+CObP3WFXPWqDokFNCTFa5KHEAAAAlwqvb0uAX5djIi0NMg2KIHniBzJ6H+AOnVpnXfhRuoL3ILvuzUv86efdyF5Kioy5T1cmPy+w6LMRklY0SBwAAUCKyKx6UnLQkyex1uMzerLHV2RhmRPGjfhCMc8vul7PptRATNc3LpZR+/r9Vt/hLkpPyJyPVSkz8rZLH3Cgjkgg3YIWjxAEAAJQAz/OUa3BBk4tlGEaIiVAs0f2mKDJ412GInjIvXyPP80LNVJ+7fblST85S7v27gzmj+yhVz5qj2PCzW9gSHYUSBwAAUALcjS/K3fK2P4hUN1hXDJ1P4qhrJcO/GIiz7hk5Kx8LOZEv99E/Vfv4SXI3vxHMRYedpeqZc2X2OCjEZKiPEgcAAFACsu/t3usRHXaGjHj3ENOg2MzuoxQbfVkwrnvl+/KcTGh5PDerupe/p/SiS6TsNn/SjCtx9M+UOP63MmJdQ8uGPVHiAAAAQuZltym34pFgHBvF2nCVIH7YN6V4D0mSt32ZskvvCCWHW7taqblnKPvOzcGc0WWoqk7+p2KjL+ew3hJEiQMAAAhZbvnDklMrSTJ7jJHZZ3zIidARjERvxQ/9RjDOvPFTeXWbOjRDbu1CpR6bKrfm+WAuMuhkVc9aoEifozo0C1qPEgcAABCyBmvDjbqEPR8VJDb60zK6HegPMluVeeNnHfK6nucq88aNSs87S17dBn/SMBUf+79KTrlXRqJXh+TAvqHEAQAAhMjZ+Krcza/7g0hSsQPODTcQOpQRiStx5PeCcfY/d8jdtrSor+nVbVL6qfOVef0GSf5VMY1kfyVP+qvih35VhkFFKHX8PwQAABCiBnvh9v+EjETPENMgDJEhp8nsf7w/8HKqe/n7RXstZ8OLqn1sipzVc4M5s/9EVZ26QNGBJxbtdVFYlDgAAICQeNkdyq14KBjHRnJBk0pkGIYS466T5B9G66x6Qrm1Cwv6Gp7nKWP/Tqk5H5NXuyqYjx38ZVVNe0Rm1cCCvh6KixIHAAAQktwHj0i5nZL8xZTNfhNCToSwRHqPVfTA84Jx5uVr5LlOQZ7by25X3TNXKPPiVZKb9SfjPZScfI8SR35PhhktyOug41DiAAAAQlL/UMrYyNlc0KTCxcd+V4pUS5LczW8qt+zP7X5OZ8s7qn1iunIfPhrMmb3HqnrmfEWHzGr38yMclDgAAIAQOJvflLvxZX9gxhUbzgVNKp1ZPUjxg68MxpnXrpeX3b7Pz5dd9oBST8yQt+29YC468lJVnfyYzG4HtCcqQkaJAwAACEGu/gVNhn5cRrJPiGlQKmIHf1FG/vw0L71Ombd/3ebn8Jy00ku+prrFX5CclD8ZqVZi4q1KHvtzGZFkISMjBJQ4AACADublapVd/mAwjo68OMQ0KCVGtIviR1wdjLPv3CJ358pWb+9uX6HUv2Yp995du5+z+0hVzZzD3t5OhBIHAADQwXIf/k3KbpMkGd0OVGTACSEnQimJDj9XZu+x/sBJK/Pqda3aLvfRY6p9fOrudQclRYd9UtUz5yrS86BiREVIKHEAAAAdLLu03gVNRlzMBU3QgGGYShz1w2CcW/GQnA0vNft4z/XXlksvujj4ckBmTPHxP1Hi+NtlxLoVOzI6GCUOAACgAzlb3pW74d/+wIwpOuL8cAOhJEUGTFRk6MeCcd3LV8vzvD0e59auUWruGcq+s/vcOaN6iKpm/FNx6wq+IOikKHEAAAAdqP4FTSJDZslM9gsxDUpZ4sjvSWZMkuTWLFHt0ocb3J9b+7RSj0+VW7M4mIsMmqHqUxco0ndch2ZFx6LEAQAAdBDPSSu7/IFgHBt5SYhpUOrMbgcqNvozwXjTM9+Vm0vL81xl3rxJ6fmflJeu8e80TMXHflfJKffJSPQOKTE6CsuzAwAAdJDch/+QMlskSUbXYYoMnBxyIpS6+GFfV3b5/VLdJuW2LdeWJT9UetUrclbPCR5jJPspcfztig6cFGJSdCT2xAEAAHSQ7Hv1L2hykQyDX8XQMiPeQ/HDvhmMt77w0wYFzuw3QVWzFlDgKgyfHAAAAB3A3bZU7vrn/IERUXTEBeEGQtmIjbpURvdRe86PuVJV0x+VWb1fCKkQJkocAABAB8i+d3fw58jgU2RWDQwxDcqJYcaUOOoHuydi3ZWc9Ccljvq+jPyFT1BZOCcOAACgyDynTtll9wfj2MjZIaZBOYoOPll9T7lDmfWvKTf0EpndDgg7EkJEiQMAACiy3EePSXUbJflreEX2mxpyIpSjbmMuksZcpJqa7WFHQcg4nBIAAKDIcu/Xu6DJyItkmJEQ0wAod5Q4AACAInK3L5ezdpE/MEwuaAKg3ShxAAAARdTggiaDZsisHhxiGgCdASUOAACgSDw3q9yyPwfj2MiLQ0wDoLOgxAEAABSJs/IJeen1kiSjaqAig2aEnAhAZ0CJAwAAKJLse7svaBIdcaEMkwuDA2g/ShwAAEARuDs+lLNmQX5kKDbiolDzAOg8KHEAAABFkH3/HkmeJCmy31SZXfcPNxCAToMSBwAAUGCem1Pu/XuDcWzkJSGmAdDZUOIAAAAKzFk9R15qrSTJSPZXZMjMkBMB6EwocQAAAAWWXVrvgiYHni/DjIWYBkBnQ4kDAAAoIHfnKjlr5gZj1oYDUGitvs6tZVlXSjpR0mGS+kvqLmmLpNck3SnpXtu2vSJkBAAAKBu59++VPFeSFBk4SWa34SEnAtDZtGVP3LcknSEpJek5SQ9Lek/SSZLulvSIZVns2QMAABXLc538VSl90ZGzQ0wDoLNqy4qTn5L0im3bO+tPWpZ1iKR5kk6XNFvSHwsXDwAAoHw4a+bLq13lDxJ9FB0yK9xAADqlVu85s237mcYFLj//lqRb8sMZhQoGAABQbrLv7b6gSezA82VEEiGmAdBZFerwx1z+tq5AzwcAAFBW3NRaOav+FYxjIy8KMQ2AzqzdJc6yrOGSPpcf/r29zwcAAFCOcu/fJ3mOJMnsf7zM7qNCTgSgs2rLOXGSJMuyLpM0WVJM0hBJE+WXwett236kUMHi8aj69etWqKcrmFLMBNTHexSljvcoSt2+vEc9z9XK5fcF4z5HfUZdea+jSPgcRZtLnKTj5V/AZJecpKsl3VSQRAAAAGUm/eF85bYtlySZiV6qHvnJkBMB6MzaXOJs275C0hWWZVVJGi7pMknfl3SuZVmn2ra9uhDBMpmctm5NFeKpCmLXNx41NdtDTgI0jfcoSh3vUZS69rxHUy/dFvw5csC52rg5KylbqGiAJD5HK0mPHlWKx5uvavuyJ06SZNt2StLbkr5hWdZaSTdKulkSXz0BAICK4abWy/nosWAcHXlJiGkAVIJCXZ3yzvztxy3LihXoOQEAAEpebtn9kudfqNvse4wiPQ8KORGAzq5QJW6z/HPjopJ6F+g5AQAASprnecq+f3cwjo1iLxyA4itUiZskv8BtkbShQM8JAABQ0px1z8jbvswfxLoruv/p4QYCUBFadU6cZVknSOop6QnbtnON7jte0h/ywz/Ytu0UNiIAAEBpyr33p+DPseHnyIhWh5gGQKVo7YVNRkr6o6QtlmW9LGmtpG6SRkg6OP+Yf8pfagAAAKDT89Iblfvo/4IxFzQB0FFaW+IWSrpO0omSRslf4NuQX+YelnSPbduPFiUhAABACcouf0ByM5Iks89RivQ6NOREACpFq0qcbdvLJV1T5CwAAABlwfM8ZesfSjlydohpAFSaQl3YBAAAoGK4Nc/L27bUH0S7KDrsjHADAagolDgAAIA2qr8XLnrAOTJiXUNMA6DSUOIAAADawKvbotyHfw/GsZEXh5gGQCWixAEAALRBdsVfJCctSTJ7j1WkzxEhJwJQaShxAAAAreR5nnJL613QZAR74QB0PEocAABAK7kbX5S79R1/EKlWdPhZ4QYCUJEocQAAAK2UXVr/giZnyoh1DzENgEpFiQMAAGgFL7NNuQ8eCcaxkZeEmAZAJaPEAQAAtEJuxUOSk5IkmT0PltlnXMiJAFQqShwAAMBeeJ7XcG24kZfIMIwQEwGoZJQ4AACAvXA3vSp38xv+IJJUbPi54QYCUNEocQAAAHvRYC/c/qfLiPcIMQ2ASkeJAwAAaIGX3a7cioeDcWzk7BDTAAAlDgAAoEW5Dx6RcjslSUb30TL7HRNyIgCVjhIHAADQgvqHUsZGzeaCJgBCR4kDAABohrPpDbkbX/EHZpwLmgAoCZQ4AACAZuQaXNDkEzISvUNMAwA+ShwAAEATvNxOZVc8GIyjIy8OMQ0A7EaJAwAAaELug79J2e2SJKPbCEX6Hx9yIgDwUeIAAACa0OCCJiMv5oImAEoGJQ4AAKARZ8s7cje84A/MmKIHfircQABQDyUOAACgkQYXNBlyqsxkvxDTAEBDlDgAAIB6vFxK2eV/CcbRkZeEmAYA9kSJAwAAqCf30T+kzBZJktH1AEUGTgo5EQA0RIkDAACoJ7u03gVNRlwkw+DXJQClhU8lAACAvMymd+XWLPYHRlTREReEGwgAmkCJAwAAyNv+5h3BnyNDTpFZNSDENADQNEocAACAJC9Xpx1v3x2MYyNnh5gGAJpHiQMAAJC08/1H5aY3SpKM6iGKDJwSbiAAaAYlDgAAQA0PpYyNvFiGGQkxDQA0Lxp2AAAAgDC4tWvkrH9ebs3zcmqWyN38hn+HYSo64vxwwwFACyhxAACg0/M8T962/8ipWSJn/WI5NUvk7figycdGBp0ss3pwBycEgNajxAEAgE7HczJyN78mZ/0SOfk9barb1PJGhqnEfhMUOerajgkJAPuIEgcAAMqel90mZ8NLctYvlluzRM6GlyQn1fJGkSpF+o6T2W+CIv0naMBBU2XGu6mmZnvHhAaAfUSJQ4fzPE+5Zfcpt/JxRXofocig6TJ7Hy7D4Do7AIDWcVNr5e7ay7b+eblb3pQ8t+WNEr0V6XesIv2PU6TfsTJ7j5VhxoK7zXi3IqcGgMKgxKFDeU5adf/+H+WW3S9JclY+Lr1+g4zkAEUGTVN08AxFBk6REe8eclIAQKnwz2db6p/Pli9t3o4Ve93O6HpAg9JmdB8lwzCKHxgAiowShw7j1q5WetFsuRtf3uM+L73O3zu37D7JiCrSf4Iig6YrMmiGzB4W/+gCQAXx3KzcTa8Hhc0/n21jyxsZpsyehyrS/1j/8Mh+x8qs3q9jAgNAB6PEoUM465co/fSl8tLrg7nosDMlM6bc6nkN/3H2cnLWPSNn3TPSK9+X0WWoIoOmKzpohiIDT5QRre74HwAAUDRedrucDS/6l/lfv7iV57MlZfYZ53/p12+CIn3HcxQHgIpBiUPRZZfepboXvyW5WX/CiCg+7oeKjf6MDMOQ5zpyN72i3Oq5clY9KXfTaw2293Z+pNzSPyq39I+SmVBkwAmKDPZLndlteAg/EQCgPdzUOv/iI+ufl1PzvNzNb0qe0/JG8V6K9D/WL2z9J8jsNVZGJN4xgQGgxFDiUDSek1HdS99WbumduycTvZU84Q5FB54YTBlmRJG+4xXpO146/Cq5qXVyVs9TbvUcOWsWSNl6Vwlz6+SsmSdnzTxl9G0Z3Uf6e+gGTVek/3EyIomO+wEBAHvleZ687e83XJ9t+7K9bmd02X/3Xrb+x8roPpoLYAFAHiUOReGm1iv99GVya54P5sxehyk56U8yu+7f4rZm1QCZIy5QbMQF/nkRNf9WbtUcOavnyN36boPHetveU3bbe8q+e6sU7aLIwMn+xVEGTZdZPagoPxsAoHmem5O7+fVgfTa3Zom8dM1etjJk9jrEP4+t33GK9D+GxbYBoAWUOBScs/EVpRddIq92dTAXHXamEhN+1ebz2QwzpsiA4xUZcLx01Pfl7vhIzuo5/qGXaxc1PGcit1POysfkrHxMkmT2OjQ4l87sO16GydsdAArNy+7YfT5bzfP++Wy5nS1vZCZk9h3nXzmy3wRF+h3D+WwA0Ab8VouCyi7/i+qWfFVy0vkZQ/Ejrlbs4C8X5AqTZtehMkdfrtjoy+U5aTnrnvVL3ao5e1xu2t38ptzNbyr71i+keE9F9zspX+qmyUj2bXcWAKhEbmq9fz7brvXZNr/RivPZeu4ubP0n+Ouzcfg7AOwzShwKwnNzyrxyrbLv/mb3ZKy7kif8XtFB04rymkYkqeigaYoOmqb4uBvkbX9PuVVz5ayeI2f9c7svpCJJmS3KffBX5T74q+pkyOxzVP6wyxksNA4AzfDPZ1vmF7b8hUi87e/vdTujy9CgtJn9j5PZg/PZAKCQKHFoN69uk9LPXCFn7cJgzug+WlWT75HZfUSHZDAMQ0b3UYp3HyWN+bx/ueq1i/JXvJwjL7WmfmK5G19SZuNL0us/lpHs7y80PmiGIvtN5ZAeABXLP5/tzd172WqWNFgapmmGzJ4H++ez5S9EYnbhfDYAKCZKHNrF2fy20osukrfjg2AuMmSWkhNvlRHrFlouI9ZN0aGnKTr0NHmeJ3fLW3JW+efSuRv+LXlu8FgvvV65ZX9WbtmfJSMqs9+xig6arsjgk1loHECn5uV2ytnwUv5y/4vlbHixFeezxWX2Oare+mxHy0j07JjAAABJlDi0Q+7Dvyu9+EsN/sGPH/YtxQ77ekkdNmMYhiK9DlWk16GKH/pVeXWblVuzIH+BlD0XGnfXP6vM+melV6+VUT0kvybdyYoMPEFGtEt4PwgAtJOX3pDfy7ZETs1iuZteb8X5bD0U6Xtsfo2242T2GSsjkuyYwACAJlHi0Gae5yrz2g3KvnXT7sloFyUn3qro0NPCC9ZKRqKXYgd8UrEDPtloofE5cje92uCxXu1K5Zbe6a91Zyb8K2UOnsFC4wBKnud58nasCNZmc2qel7ftvb1uZ1QPVqT/cfnL/U+Q2fOgkvpiDgBAiUMbeZltSj/3OTmr/hXMGd0OVHLS3Yr0PCjEZPum7QuNz5ezZr6/0Hi3EcHFUVhoHEDYPDfnHzq+/nl/fbb1S+Sl1+11O7PHGJn5vWyR/hNkdhnSAWkBAO1BiUOruduWKrXwogbf5Eb2O0nJ43/Xac6HaNNC49vfV/bd95V997b8QuOTFB18siKDprFILYCi8zxP7vrFctY/l7965AtSbkfLG5lxmX2ObLg+W6JXxwQGABQMJQ6tklv1pNLP/leDPVOxg69UfOzVMsxIiMmKp+mFxuf6e+maXGj8cTkrH5fEQuMAistzs0o/fVnwmdOsWLeG67P1OZLz2QCgE+A3S7TI8zxl37pJmddukOT5k5EqJSb8UrEDzgo1W0fzFxq/TLHRl+UXGn9OzuonWWgcQIfyPFd1i69sssAZVfsp0v84mf2O9W97HNRpv2gDgEpGiUOzvOwOpZ+/Us6Hfw/mjOohSk6+W5Heh4eYLHz+QuMnKTropPxC4+8Hh122fqHx6TJ7j+WCAQDaJPPKtcqteDAYR/c/XZEhMxXpN0FGl6EsiwIAFYAShya521covehiuVveDubM/ser6sQ72JPUiL/Q+EjFu4+st9D40/5hl21aaHyKjHiP0H4OAKUv8/bNyr5zczCOjpytxDE/p7gBQIWhxGEPuTVPKf3MFVJmczAXG32F4uN+KMOMhZisPPgLjZ+q6NBT8wuNvy1n1ZOtWGg8IrPfhPxC4zP8w6D4xQxAXnbZA8q88r1gHBl6mhJH/4zPCQCoQJQ4BDzPU/bd25R55ZrdRcOMK3HMjYqNuDDccGXKX2j8EEV6HdKKhcadZhYan6HIwBNZaByoYLnV81T3/JeDsdl/opLH3875bgBQoShxkCR5uZTq/v015Zb/JZgzqgYoOekuRfoeHWKyzqXdC40Pmq7o4JNZaByoIM6GF5VedKnk5SRJZs+DVTX5Hq4yCQAVrFUlzrKsmKRJkk6VNFnSaElJSTWSFku62bbtp4qUEUXm7lyl9KJLGpQIs+94JU+8U2b1fiEm69yaW2jcWT1XuTXzm19o/KXv1FtofLoi/Sey0DjQSbnblir11PmSUytJMroMVXLqXzh/FgAqXGv3xE2WNCf/57WSFknaKelgSWdJOsuyrOts276m8BFRTM7655V++lJ56ZpgLjriIiWO/inFoIM1udB4/uIorVlofJv1cSX6HSFnS379uuA8mUa3rZzffZpNK7cvwGuqra+ZnzeafXw7X9OIyYjEBYTBrV2t1LyzpbpN/kSit6pOepAv1wAArS5xrqSHJf3Stu2n699hWdZ5ku6VdLVlWQts215Q4IwokuzSO1X34lW7L4dvRJUYf72ioy7nRPmQNVho/Mj6C43PzS80Xrv7wfmFxjfubdFftJ2ZUGzkxYqP+xELtqNDeZmtSs8/V17tSn8iUq2qKffL7D4q3GAAgJLQqt9KbNueL2l+M/c9YFnWDEmflnSRJEpcifOcjOpevEq59+4K5oxEXyVPvMMvDSg5TS80Pie/0PjysON1Xm6dsv/5vby6jUpMvI0ihw7h5VJKPXWh3K3v+BNGVMlJdyrSd1y4wQAAJaNQv5G8kr8dUqDnQ5G4qXVKP32Z3JolwZzZ63AlJ98tswv/95WDhguNX+8vNL56riIbFslJrVcu50qel390o9vG8808ztvr9mpmft9er+XnUDPz+ax7e81WZ2mcw5WctCQp98EjkucpcfxvKXIoKs91lH72v+TWLA7mEsf9WtFB00JMBQAoNYX6bWTX8R1rWnwUQuVsfFnphZc0WHw6OuwsJSb8Qka0OsRk2Ff1Fxrv1+8bkqSamu172Qqt4XmeMi99W1n7d5Kk3IePStpV5FgvEYXneZ7qXviGnJWPBXPxo36g2PBzQ0wFAChF7S7nSH5wAAAgAElEQVRxlmUNlHRpfvhwe58PxZFddr/qlnxNcuv8CcNU/IjvKTbmi5z/BjTBMAzFx90gyVDWvl2SlPvwb/Lk5dfnosihsDJv/KTBYe6xMV9SfMwXQ0wEAChVhrfH4VGtZ1lWVNITkqZJmmfb9vQCZHpK/tUwUQCem9OmRd/Stld/HcyZiZ7qN+seVR9wcojJgPLgeZ42Lfx6g79D1SPPVP9Z98iIUORQGNte+602LrgyGHcdc6H6nvwHGYYZYioAQAlYKGlK48n2/utwm/wC95H8i5qghDipDVr7yGkNfvmM9TlYg85/jgIHtJJhGOo9+UZ1P/LLwVzte49o/eMXynOyISZDZ7Fz6cPauGD3+6tq2CnqO/12ChwAoFn7vCfOsqxfSvqy/HXjJtm2vbRAmZ6SNDmTyWnr1lSBnrL9+vXrJql8zjdyNr+p9MKL5e38MJiLDD1NyeNukRHrFmIyFEu5vUfLjed5yrx8tbLv3hrMRYaepuTxv2ctuVbiPbqn3LpnlJ5/juRmJElmn6NUNe0RGbGuISerTLxHUep4j1aOHj2qFI9HpULuibMs6+fyC1yNpGkFLHAogOwHjyr1r1kNClz88KuUPPFOChywjwzDUPyo6xQ76AvBnPPRP5V+5tPynEyIyVCunE1vKL3woqDAGd1HqmrK/RQ4AMBetbnEWZb1U0lfk7RR0nTbtt8ueCrsE891VPfqdap75tO7F4OOdlFy0t2KH/YNDs0B2skvcj9QrN7FJpyVjyn9zOUUObSJu32F0gvOk7L+t+lG1UBVTX1IRrJPyMkAAOWgTb/VW5b1Y0nfkLRZ0gzbtl8vSiq0mZfZqvTCC5V96xfBnNFthKpnPqno0FNDTAZ0LoZhKH7ktYqN+VIw56x8nCKHVnPTNUotOEdeep0/Eeuu5NQHZXYdGm4wAEDZaHWJsyzrh5K+JWmL/AL3yl42QQdxt9qqfWKGnNVzgrnIoOmqnjlHZo+DQkwGdE5+kfu+YmN2X03QL3KXUeTQIi+7XekFn5K3fZk/YSZUNfleRXodHG4wAEBZadU6cZZlfULSd/PD9yRdaVlWUw9917btHxcoG1oht/IJpZ/9rJTbEczFDv6K4mO/K8OMhJgM6Nz8Ivc9yZCyb/tXgHVWPqH005cpeeIdMiKJkBOi1HhORulFl8rd9Ko/YZhKnvA7RQZMDDcYAKDstHax7971/jw+/19TFkqixHUAz3OVffMmZV7/saT8FUYj1Uoc92vFhp0RajagUhiGofgR35NkKPv2ryRJzqpdRe6PFDkEPM9V3eIvyVn7VDCXOPpGRYeeFl4oAEDZalWJs237Tkl3FjUJWs3L7lB68RflfPR/wZzRZaiSk+9RpNehISYDKo9f5K6RX+R+KUlyVv1L6UWXKjnpToocguUpch88HMzFD/+2YqNmh5gKAFDOuFxhmXG3L1fqXzMbFLjIgBNUPXMeBQ4IiV/krlbskK8Gc87qJ5VedKk8py7EZCgF2bd/rey7twXj2KjLFTv0f0JMBAAod5S4MpJbs0C1T0yXu/WdYC5mfVbJk7gsNRA2wzAUH/tdxQ75WjDnF7nZ8px0iMkQpuz79ynz6rXBODL044qP/7EMwwgxFQCg3FHiyoDnecq8c4vSC86VMlv8STOhxIRfKzH+ehlmLNyAACTtKnLfabCXxVk9hyJXoXKrnlTdkv8OxpEBJyh5/G1cdAoA0G6UuBLn5VKqe+5zyrx8jeS5kvKLws74h2IjLgg5HYDGDMPwz3c69OvBnLN6rtILL6HIVRCn5gWln75c8hxJktnrUCUn3S0jkgw5GQCgM6DElTB350ql5pym3IqHgjmz79GqmjVPkb7jQkwGoCV+kbtKscO+Ecw5a+YpvfBiilwFcLfaSj11vuSkJElGl/2VnPqAjHj3kJMBADoLSlyJctY9p9Tj0+Ruei2Yi468RFXT/yazamCIyQC0hmEYShx+lWKHfTOYc9bMV3rhRfJyqRCToZjc2lVKzT9HymyWJBmJvqo66SE+twEABUWJKzGe5yn7nz8oNe9MeXUb/EkjqsTRP1PimJu4XDlQZhKHf6tRkVtAkeukvLrNSs8/R17tKn8i2kXJqffL7D4i3GAAgE6HEldCPKdOdUu+qroXvil5OUn5b3GnPaLY6Mu5mhlQphKHf0vxw74VjJ21T1HkOhkvV6vUwgvlbrX9CTOm5KS7FOlzZLjBAACdEiWuRLiptUrNPV259+8O5szeY1U1a74iAyaGmAxAIcQP/6bih18VjP0id6G8XG2IqVAInptT+pnPyK1ZEswljrtF0f2mhpgKANCZUeJKgLPhRf/8tw0vBHPRA85W1Yx/yuwyOMRkAAopftg3FD/828HYWbuQIlfmPM9T3b//R86qJ4K5+LgfKXbAWSGmAgB0dpS4kGXfv0+pOR+Xl1rrTxim4kddp8TE22REq8INB6Dg4od9vVGRW6T0UxdQ5MpU5rXrlXv/nmAcO/jLih/0uRATAQAqASUuJJ6bVd2L31bd81dKbsafjPdUcuqDio/5Aue/AZ1Y/LCvKz72u8HYWfc0Ra4MZezblX3rpmAcPfBTih9xTYiJAACVghIXAi+9Uen5Zytr3x7MmT3GqHrmPEX3mxJeMAAdJn7o1xQf+7/B2C9y58vL7QwxFVor+8Gjyrz4nWAcGXSyEsf+gi/gAAAdghLXwZxNb6j2iWly1j0TzEWGflxVpzwhs9sB4QUD0OHih35V8SOuDsbOumfye+QocqUst3ah6p77nCRPkmT2Ha/kiX+QYcbCDQYAqBiUuA6UXfGIUk/Okrfzo/yMofjY7yh54h9lxLqGmg1AOOKH/HeDQ/Ccdc8otYA9cqXK2fSa0gsvkdysJMnoPkpVU/4sI1odcjIAQCWhxHUAz3VU98q1qnv2CsnJrwsV7ark5HsUP/R/OPwGqHDxQ76i+BHfC8bu+meVWvApedkdIaZCY+725UovOE/K+f+/GFX7qeqkh2QkeoecDABQaShxRebVbVH6qfOVfftXwZzRbYSqZ85RdMjMEJMBKCXxQ76s+JHfD8bu+ucociXETa1Xav7Z8tI1/kS8h5InPSSzy5BwgwEAKhIlrojcre+q9l8z5KyZF8xFBs1Q9cw5MnuMDjEZgFIUP/hKxY+8Nhi7NYspciXAy25TesF58nas8CciSVVN/rMiPQ8KNRcAoHJR4ook99Fjqn3iZHnblwVzsUO+quTke2XEe4SYDEApix/8JcWP+kEw9ovcefKy20NMVbk8p07pRbPlbn7dnzBMJU/4vSL9jw03GACgolHiCszzXGVe/6nSiy6Wdl2YIFKt5Al3KHHE/8owI+EGBFDy4mO+qPhR1wVjt+Z5ilwIPM9V3XNfkLN2UTCXOOb/KTpkVoipAACgxBWUl92u9KLZyrzxk2DO6LK/qk55XNFhp4eYDEC5iY/5guJH/TAYuzVLKHIdyPM8ZV78tnIfPhrMxcd+V7GRF4WYCgAAHyWuQNzty5T61ylyVj4WzEUGnKjqWfMU6XVoiMkAlKv4mM/vWeTmnysvuy3EVJUh+9b/U/Y/vw/GMeszih3y1RATAQCwGyWuAHKr56v2ielyt9rBXOygzynJpacBtFN8zOcVH3d9MHY3/JsiV2TZ9+5R5rUfBePo/mcoPu56loMBAJQMSlw7eJ6nzNu/Vvqp86TMVn/STChx3C1KjPuRDDMabkAAnUL8oM82KnIvUOSKJLfycdX9e/cet8iAE5WY+BsZBv9cAgBKB/8q7SMvV6u6Zz+rzCvflzxXkmRUD1LVyf9U7MBPhRsOQKcTP+izio+/IRj7Re4ceRmKXKE465co/cwVwWe62etwJSf/SUYkEXIyAAAaosTtA3fHR0o9eZpyHzwczJn9jlXVzHmK9DkyxGQAOrO49V+Kj/9xMHY3vOgvQE2Razdny7tKLTxfctKSJKPrcCWnPiAj1j3kZAAA7IkS10bOumdV+8S03WsGSYqOvFRV0x6VWdU/xGQAKkHc+ozi43dfAdfd+JJS88+St+uQbrSZu3Ol0vPPDg6LN5L9VHXSg3ymAwBKFiWulTzP07bXblVq3ieluo3+pBlT4pifK3nsz2VE4uEGBFAx4tYVShz902DsbnyZIrePvLpN/mGpqTX+RLSLklMfkNlteLjBAABoASWuFTw3qw1zP6uNC74ieTlJkpHsr6ppjyo26tJwwwGoSLHRn1bi6J8FY3fjKxS5NvJyO5V66nx52/7jT5gxJSffrUjvseEGAwBgLyhxrZBdepd2vHVnMDZ7H6GqWXMV6T8hvFAAKl5s9OVKHH1jMHY3vqLUvE/Kq9sSYqry4LlZpZ++Qu6GF/MzhhITb1N04ORQcwEA0BqUuNbwnOCP0eHnqmrG/8msHhxiIADwxUZfpsQxPw/G7qZXlZpPkWuJ53mqW/LfclY/GczFx1+v2LAzQkwFAEDrsZBZK8RGf1rd+w5UpHqAdlQdy4KvAEqKf1i3obp/f02S5G56Tan5n1TVSX+VkegZarZSlHn1OuWW3R+MY4d8VXHrv0JMBABA27AnrhUMM6puYy5S9bAZFDgAJSk2arYSx9wUjHcVOa9uc4ipSk/m3duUffuXwTg64kLFx343xEQAALQdJQ4AOonYqNlKHPuLYOxuei1/jhxFTpKyKx5W5qXdhS0yeKYSx9zEl3MAgLJDiQOATiQ28mIljv2lJL+YuJtfp8hJyq1ZoLrFXwzGZr9jlTzhdzJMzioAAJQfShwAdDKxkRcpMaFxkTtTXt2mcIOFxNn4itKLZktuVpJk9rBUNfleGdHqkJMBALBvKHEA0AnFRlzYqMi9kd8jV1lFzt32vtILPiXldkqSjOrBSp70oIxEr5CTAQCw7yhxANBJ+UXuV2pY5M6Ul94YbrAO4qbWKrXgHHl1G/yJeC9VnfQgS8QAAMoeJQ4AOrHYiAuUOO7X2l3k3qyIIudltim94Dx5Oz7wJyJVqpryZ5k9rHCDAQBQAJQ4AOjkYgee37DIbXlLqXlnyEtvCDdYkXhOWulFF8vd/KY/YUSUPPEORfodHW4wAAAKhBIHABXAL3I3a3eRezu/R65zFTnPdZR+7vNy1j0TzCWO/YWig08OMRUAAIVFiQOAChE78FNKHHeLGhS5uWfITdeEG6xAPM9T5sWr5Hz492AufsQ1io24IMRUAAAUHiUOACpI7MDzlJj4G8nwP/7dre8o3UmKXPbNnyu79I5gHDvoc4od/OUQEwEAUByUOACoMLHh5ypxXP0i927ZF7ns0ruUef2GYBwddpbiR10nwzBCTAUAQHFQ4gCgAsWGn6PEcbc2KnKny02tDzlZ2+U++qfqXvh6MI4MnKLEcTfLMPgnDgDQOfEvHABUqNjws5WYeFu9ImcrPe+MsipyzrrnlH7mM5LnSpLM3mOVnHSnjEg85GQAABQPJQ4AKljsgLOUmPjbBkUuNfd0ual1ISfbO2fz20otvFBy6yRJRrcDlZz6gIxYt5CTAQBQXJQ4AKhwsQM+qcTE2yUjIknytv2n5Iucu+NDpRecLWW3SZKM5ABVnfSQzGS/kJMBAFB8lDgAgGIHnKnE8fWL3NJ8kVsbcrI9eemNSs0/W96ukhntquTUB2R2HRZuMAAAOgglDgAgSYoNO0PJ439X0kXOy+1U6qlPydv+vj9hxpWcfI8ivQ8LNxgAAB2IEgcACESHnd6oyL2n1JxPyK1dE3IyyXOzSi+6TO7Gl/MzhpLH/1bRgSeGmgsAgI5GiQMANBAddrqSJ/x+d5Hb/r6/Ry7EIud5ruoWXylnzbxgLnH0TxXd/xOhZQIAICyUOADAHqL7fyJf5KKS6he51aHkybxyrXIrHgzGsUO/rtjoy0PJAgBA2ChxAIAmNV3kzujwIpd55xZl37l5d66Rlyh++FUdmgEAgFJCiQMANCu6/8eVPPEPTeyRW9Uhr59d/hdlXr4mGEeGnKrE0T+TYRgd8voAAJQiShwAoEXRoR9rVOSWKTWn+EUut3qe6hZfGYzNfscpefztMsxoUV8XAIBSR4kDAOyVX+T+KJkxSZK3Y7lf5HYWp8g5G15UetGlkpeTJJk9D1bVlHtlRKuK8noAAJQTShwAoFWiQ0/ds8jN/UTBi5y7balST50vObWSJKN6iJJT/yIj3qOgrwMAQLlq9TEplmVZkmZKOlrSeEmjJRmSzrFt+6HixAMAlJLokFlKnnin0k9fKrlZeTtWKDX3E6qa/jeZXYa0+/nd2jVKzT9HqtvkTyR6q+qkB2VW79fu5wYAoLNoy564z0v6haQLJVnyCxwAoMJEh8xU8sQ76+2RW5HfI7eyXc/rZbYqveBceTs/8ici1aqa8meZPUa3MzEAAJ1LW0rcm5J+Juk8SSMlLSxKIgBAyfOL3F2SGZckeTs+8Ivcjo/26fm8XEqppy6Uu+Vtf8KIKjnpTkX6ji9UZAAAOo1WH05p2/bv64/9oysBAJUqOuQUJSfdpfSi2ZKbCYpc1fS/y+w6tNXP47mO0s99Vm7N4mAuMeFXig6aVozYAACUPS5sAgDYZ9HBJys56U+798jt/DC/R+7DVm3veZ7qXviGnI/+GczFj7xWsQPPK0peAAA6A0ocAKBdooNn7HORy7zxU+XeuysYx8Z8UfGDv1S0rAAAdAYlu2JqPB5Vv37dwo6xh1LMBNTHexSh6PdJ1fao1vr/O0eeUydv50eqm3+69jt7jmI9hjd8aP49uu2132rHGz8N5rscdIH6nfxzGQbfLyJcfI6i1PEeBf9SAgAKonr4TPX/+EMyIglJkrP9Q615aIayW5ft8didS/+qjQu+HIyrhp2ifjN+R4EDAKAVSnZPXCaT09atqbBjBHZ941FTsz3kJEDTeI+iJHSZqMSke5ReeJHk1snZ/qFWPTBNVdP/rgEHHiZJWvPm40rPv0SSJ0ky+xwp89jbtWFTWlI6vOyoeHyOotTxHq0cPXpUKR5vvqrxlScAoKCig05ScvI9kunvkfNqVyk19xPKbnlfdTWv5QteRpJkdBuhqin3y4h1DTMyAABlpWT3xAEAyld00ElKTrnXL2xOWl7tKq15aIbk5qSs/w2yUTVAVSc9JCPZN+S0AACUF/bEAQCKIrrfVCUn3ytFkpIkZ8dKObVr/Ttj3ZWc+qDMrvuHmBAAgPJEiQMAFE10vykNipwkyUyoavK9ivQ6JLxgAACUMUocAKCoovtNUXLKfTITvWREq5U84XeKDJgYdiwAAMpWq8+JsyzrKEm/qTd1cP72esuyvr5r0rbtCQXKBgDoJKIDJ2vAFcskz9XGrWGnAQCgvLXlwibdJR3bxPyoAmUBAHRiZqxL/k9cGhsAgPZodYmzbfspSUbxogAAAAAA9oZz4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxQ4gAAAACgjFDiAAAAAKCMUOIAAAAAoIxE27qBZVkXSPq8pMMlRSS9K+mPkm61bdstbDwAAAAAQH1t2hNnWdYtku6VNF7S05LmSBot6WZJD1mWxZ49AAAAACiiVpcuy7LOkvQFSWslHW7b9sds2z5T0ihJ70g6U9KVRUkJAAAAAJDUtj1x387ffsu27aW7Jm3bXif/8EpJuoq9cQAAAABQPK0qXJZlDZE0TlJG0oON77dte6GkVZIGSppQyIAAAAAAgN1ae2GTI/O3b9m2nWrmMS9IGpx/7HPtDVZKttXldOuT/9F/anYok8nJNCRThmTI/7NhyJB/axqSYRgyDL8hm7v+XP8+7WW7RvcZDcZNze25nZF/zqa2a/a+xtvtes7gZ91zu90/657bGY1+vqa2Mxr9b20YDWf2vF97uX8v2+/l+Ro/Yu+v1/j+jt5+jx8AReR5XsPxHvfXv89r9r69bbu37fe2beNH7PW1Gzy29T9j0/e3sP2OOknShtpMMNXSe7qlvy9t2q6Z52h8b0t/31r6u1qM7fz7+fsN7E3jz5xgvtnHNzPf7BYtbVOY12jL89dmcv5t1mkw39SnRVMfIY1/z2jqcfv6XE0/pqnn4rOtEFpb4obnbz9o4TEfNnpsp3HHy6t0w6LlYccAAEDS3opp+55vn5+jAEEKk6MQCZr/hX5fNPdLerues/BP2WwhatdzNvtabXs8KktrimRrC+LenitiGJo1uq9u+dgYmWVUMFtb4rrmb3e28Jgd+dtu+x5nt3g8qn79CvJU7TZmcM+wIwAAEGi49zS0GA2VTJD26Aw/A1D+mvqb2KqPmH36HPL017fX65qZY3T4oNLoHq3R5nXiKtFF44booP5d9e76HfLkyXUl1/Pkev6ucdeTXNffSb5r3vU8eV69x9Wbd738Y/PPs3u7XY+t9zivhedUvXm30bjZ7Rq+dsPXb/jajXPv8Zy7ttvj52ji5/XUcDvXn6+voIdtNbn9Xg4xa+vj2/t6e2zftp8HpaHlQ+naenhw6w+nbfO2bXh8sQ9z3qWl93xLf1/atp3X/H2tPPx1Xw+bbenvOH+/geJrbqdKc/taWtqb3Pw2zT2+6TvanqnZSIGmPj9aW4L29rtK88/V8u8szT1XqZp1UH8dMrB8CpzU+hK3ay9blxYes2tv3fZ9j7NbJpPT1q3NnX7X8cYP7anxQ3uqpqYgPx5QcH37+n8FeY8WFsfuF86uoyt4j7ZNW0rrPj1/AfY+FSZHAZ6jnU/Sr19XeZ60YcOOvT+4jYrxUVKMT6fi5CxUoeHzuLN9jjZ1CG9rCmFTn1vtKbOJqKlNGwv/9749evSoUjzefFVrbYlbkb8d1sJjhjZ6LIAOtOsfN/6RAzqXFveEFuSvO58ZuySikfwtqyUBHaG157DtOcnnVms/pV7J3x5iWVZVM485utFjAQAAAAAF1qoSZ9v2R5JelhSXdE7j+y3LmixpiKS1khYXMiAAAAAAYLe2HC9wQ/72J5Zljdw1aVlWf0m/yQ9/bNu2W6hwAAAAAICGWn11Stu2H7Is61ZJn5f0hmVZcyVlJU2T1F3So5JuLkpKAAAAAICktu2Jk23bX5B0ofxDKydLOkXSe5K+JOks27adFjYHAAAAALRTm9eJs237Pkn3FSELAAAAAGAvuIYuAAAAAJQRShwAAAAAlBFKHAAAAACUEUocAAAAAJQRShwAAAAAlBFKHAAAAACUEUocAAAAAJQRShwAAAAAlBFKHAAAAACUEcPzvLAzNLZS0mDX9ZTLOWFnCcTjUUlSJpMLOQnQNN6jKHW8R1HqeI+i1PEerRzRaESmaUjSKklDGt9fiiVui6QeYYcAAAAAgJBtldSz8WQ0hCB7s1zScEk7JL0XchYAAAAA6GgjJXWV3432UIp74gAAAAAAzeDCJgAAAABQRihxAAAAAFBGKHEAAAAAUEYocQAAAABQRihxAAAAAFBGKHEAAAAAUEYocQAAAABQRihxAAAAAFBGKHEAAAAAUEYocQAAAABQRihxAAAAAFBGKHEAAAAAUEYocQAAAABQRihxAAAAAFBGomEHKHWWZV0g6fOSDpcUkfSupD9KutW2bTfMbKhslmXFJE2SdKqkyZJGS0pKqpG0WNLNtm0/FVpAoAmWZV0v6dv54Tds274xzDzALpZlVUm6UtI5kkZJiktaJ+lFSb+wbfvZEOOhwlmWNUTStySdLGl/SYakjyTNk/RT27aXhRgPIWBPXAssy7pF0r2Sxkt6WtIc+b8o3yzpIcuy+N8PYZosaa6kr0kaLGmRpEckbZJ0lqQFlmX9ILx4QEOWZR0t6ZuSvLCzAPVZljVc0uuSfiL/83SBpH/K/1LsDElTw0uHSmdZ1pGS3pD0JUnVkv4l6QlJVZI+K+k1y7ImhpcQYWBPXDMsyzpL0hckrZU0ybbtpfn5AfI/3M+U/43dL0MLiUrnSnpY0i9t2366/h2WZZ0n/wuIqy3LWmDb9oIwAgK7WJaVkHSX/D0b/5b/izEQOsuyusj/kvZASVdJutG2bafe/X0k9QkpHiBJt0jqKel3kr5o23ZWCo7IuU3S5ZJulTQ2tITocIbn8YVoUyzLelHSOEmzbdv+U6P7Jkt6Sn7BG8xhlShFlmX9XtKnJd1h2/anw86DymZZ1k/k74X7hPw9xbPF4ZQoAZZl3SC/vN1s2/aVYecB6rMsKykplR8Osm17TaP795O0Oj/sYtt2bUfmQ3g4HLAJ+eOOx0nKSHqw8f22bS+UtErSQEkTOjYd0Gqv5G+HhJoCFc+yrGMl/Y+k+2zb/kfYeYBdLMuKS/pMfnhTmFmAZjiScq143E7tLnuoABxO2bQj87dv2bbd3F+IF+QfN3+kpOc6JBXQNqPyt2tafBRQRPlvke+Sf67mV0KOAzQ2Tv6hkqts215uWdZR8k+X6C//0N8nbdt+JsyAqGy2bWcty5on6RRJ11qW1fhwyuvyD/2DbdscXldBKHFNGx7OPwQAAARZSURBVJ6//aCFx3zY6LFAybAsa6CkS/PDh0OMAvxIkiXpU7Ztbwg7DNDIYfnbVZZl3Sh/j3F9V1uW9aiki2zb3tmx0YDAF+RfyOQzkmblT/mRpKMl9ZL0C/mHq6OCcDhl07rmb1v6wN6Rv+1W5CxAm1iWFZV0j6QekuZx+BrCkr9a2n9LetS27QfCzgM0oXf+9kj5Be4XkkbK/8X4dPmnTpwh6TehpAMk5ZcPmCjpcfmnSJyR/2+wpLclPb1r7xwqByUO6HxukzRN/voxF4WcBRUqv+bWnZK2yf8WGShFu34Pikm6x7btr9q2/b5t21ts2/67/F+UPUkXW5Y1IrSUqGj5L8TelP8Fw+mS+uX/O0P+Fw4PW5Z1TXgJEQZKXNN27WXr0sJjdu2t217kLECrWZb1S/lXpFwraZpt22tDjoTKdb388zK/1vhqakAJqf9v+O8a32nb9ouSXpK/sPLkjgoF7GJZVk9Jj8o/8mumbdt/t217Q/6/v0maKf+CJldbljWqpedC58I5cU1bkb8d1sJjhjZ6LBAqy7J+LunL8hennbZrbUMgJGfKX8twtmVZsxvdd1D+9vOWZX1M0nu2/f/bu59Xm6IogOPflyIG8itFkqTWwETeY6aIN1JGjE1M/BiJlDF5JMqAEfEfMJKSUvKjMCCySikyklASyY/B3jfvXfc+Gbin434/ddrnnL0Ha3ZaZ/9YuWug0UnFiz733WPGKCdSS4O2lTLrdqMuq5wiM59HxD1gY7389g8Jk7jeOkezr46I2X1OqFzXNVZqTEScAPYDb4Etmfm04ZAkKKs9ppu9WFmveYMJR/rN5G/4Qsoy9G6LavuxR5/0ry2v7Ydpxryv7YJpxug/43LKHjLzFfAQmAns6O6vxb6XUZas3RlsdNJUETEBHATeAeOZ+ajhkCQyc0VmjvS6KCUHoBT7HsnMNU3GquGVma+Be/Vxc3d/RMwH1tbH+9390gB0CnmP1pICU9R3o/Wx32yy/kMmcf0dq+3xiFjVeRkRi/l1StVEZn4feGRSFRFHgEOUv3DjmenMsCT9naO1PRwRY52XtcbhOcpJvw/wp62acRX4RJmROx0Rszod9f4MZYvPO+BaIxGqESM/flgXsJ+IOAvsBj4D14GvlD91cymbTLdn5rfmItQwi4htwJX6eB940mfos8ycGExU0p9FxEVgJ2Um7mTD4UhMqhH3FbhLWZq+HlhKKTOwyX3GakrdV3wemEGZmXtYu0aBJcAXSi3Oy81EqCa4J24ambknIm4Beyn7OmYAz4ALwDln4dSwyWvfx+rVy03AJE6S+sjMAxFxG9hHqRk3B3gJnKKsunnTZHwabpl5KSIeU+pubgDGa9drSnJ3yr3ww8eZOEmSJElqEffESZIkSVKLmMRJkiRJUouYxEmSJElSi5jESZIkSVKLmMRJkiRJUouYxEmSJElSi5jESZIkSVKLmMRJkiRJUouYxEmSJElSi/wEN+qq/iYEKfYAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model = MEGClassifier(3).to(device)\n",
        "model.load_state_dict(torch.load(\"model_cross_0.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxYSdzBJmH8j",
        "outputId": "8a86941e-988a-45c5-efb0-566292b3ccae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"############# Test Set 1 #############\")\n",
        "pred_labels1, true_labels1, accuracy1, precision1, recall1, reports1, confusion1 = test_metrics(model, cross_data_test1)\n",
        "print(\"Accuracy:\", accuracy1)\n",
        "print(\"Precision:\", precision1)\n",
        "print(\"Recall:\", recall1)\n",
        "print(reports1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "print(\"############# Test Set 2 #############\")\n",
        "pred_labels2, true_labels2, accuracy2, precision2, recall2, reports2, confusion2 = test_metrics(model, cross_data_test2)\n",
        "print(\"Accuracy:\", accuracy2)\n",
        "print(\"Precision:\", precision2)\n",
        "print(\"Recall:\", recall2)\n",
        "print(reports2)\n",
        "print(\"Confusion matrix:\\n\", confusion2)\n",
        "\n",
        "print(\"############# Test Set 2 #############\")\n",
        "pred_labels3, true_labels3, accuracy3, precision3, recall3, reports3, confusion3 = test_metrics(model, cross_data_test3)\n",
        "print(\"Accuracy:\", accuracy3)\n",
        "print(\"Precision:\", precision3)\n",
        "print(\"Recall:\", recall3)\n",
        "print(reports3)\n",
        "print(\"Confusion matrix:\\n\", confusion3)"
      ],
      "metadata": {
        "id": "zKX1vUi22RLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5662d1d5-b38b-428b-e39d-d5c6159bac7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############# Test Set 1 #############\n",
            "Accuracy: 0.47458595088520844\n",
            "Precision: 0.5873462657444807\n",
            "Recall: 0.47451891282029235\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        rest       1.00      0.27      0.43      6997\n",
            "        math       0.77      0.53      0.63      7007\n",
            "      memory       0.18      0.10      0.13      7006\n",
            "       motor       0.40      0.99      0.57      7006\n",
            "\n",
            "    accuracy                           0.47     28016\n",
            "   macro avg       0.59      0.47      0.44     28016\n",
            "weighted avg       0.59      0.47      0.44     28016\n",
            "\n",
            "Confusion matrix:\n",
            " [[1906    1    0 5090]\n",
            " [   7 3739 3261    0]\n",
            " [   1 1108  733 5164]\n",
            " [   0    1   87 6918]]\n",
            "############# Test Set 2 #############\n",
            "Accuracy: 0.4634494574528841\n",
            "Precision: 0.37507179598025275\n",
            "Recall: 0.46331960430438224\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        rest       0.46      0.85      0.60      7006\n",
            "        math       0.08      0.01      0.02      6998\n",
            "      memory       0.00      0.00      0.00      7006\n",
            "       motor       0.96      0.99      0.97      7006\n",
            "\n",
            "    accuracy                           0.46     28016\n",
            "   macro avg       0.38      0.46      0.40     28016\n",
            "weighted avg       0.38      0.46      0.40     28016\n",
            "\n",
            "Confusion matrix:\n",
            " [[5989  693    0  324]\n",
            " [   2   60 6936    0]\n",
            " [6978    1   27    0]\n",
            " [   0    1   97 6908]]\n",
            "############# Test Set 2 #############\n",
            "Accuracy: 0.24757281553398058\n",
            "Precision: 0.30834797222467675\n",
            "Recall: 0.24746681889539032\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        rest       1.00      0.48      0.65      7007\n",
            "        math       0.00      0.00      0.00      6995\n",
            "      memory       0.03      0.02      0.02      7007\n",
            "       motor       0.20      0.49      0.28      7007\n",
            "\n",
            "    accuracy                           0.25     28016\n",
            "   macro avg       0.31      0.25      0.24     28016\n",
            "weighted avg       0.31      0.25      0.24     28016\n",
            "\n",
            "Confusion matrix:\n",
            " [[3394 1906    0 1707]\n",
            " [   1    0 1885 5109]\n",
            " [   0    1  110 6896]\n",
            " [   1 2314 1260 3432]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "ODKZ5KNw6l2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "cross_classifier = MEGClassifier(3).to(device)\n",
        "cross_classifier.load_state_dict(torch.load(\"model_cross_0.pt\"))\n",
        "# Load the best model\n",
        "intra_classifier = MEGClassifier(3).to(device)\n",
        "intra_classifier.load_state_dict(torch.load(\"model_intra_7.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4poSiBItyXd2",
        "outputId": "e46833b3-3f4f-4115-aa70-739cd1916bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels, true_labels, accuracy, precision, recall, reports, confusion = test_metrics(cross_classifier, intra_data_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(reports)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ],
      "metadata": {
        "id": "vSoE_nNZ3uCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c0868ae-d8d2-458e-df52-a75519862f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9937178754997145\n",
            "Precision: 0.9938675199798368\n",
            "Recall: 0.9937206261234662\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        rest       1.00      1.00      1.00      3498\n",
            "        math       1.00      0.99      0.99      3504\n",
            "      memory       0.98      1.00      0.99      3503\n",
            "       motor       1.00      0.99      0.99      3503\n",
            "\n",
            "    accuracy                           0.99     14008\n",
            "   macro avg       0.99      0.99      0.99     14008\n",
            "weighted avg       0.99      0.99      0.99     14008\n",
            "\n",
            "Confusion matrix:\n",
            " [[3498    0    0    0]\n",
            " [   0 3457   47    0]\n",
            " [   0    1 3502    0]\n",
            " [   0    0   40 3463]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"############# Test Set 1 #############\")\n",
        "pred_labels1, true_labels1, accuracy1, precision1, recall1, reports1, confusion1 = test_metrics(intra_classifier, cross_data_test1)\n",
        "print(\"Accuracy:\", accuracy1)\n",
        "print(\"Precision:\", precision1)\n",
        "print(\"Recall:\", recall1)\n",
        "print(reports1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "print(\"############# Test Set 2 #############\")\n",
        "pred_labels2, true_labels2, accuracy2, precision2, recall2, reports2, confusion2 = test_metrics(intra_classifier, cross_data_test2)\n",
        "print(\"Accuracy:\", accuracy2)\n",
        "print(\"Precision:\", precision2)\n",
        "print(\"Recall:\", recall2)\n",
        "print(reports2)\n",
        "print(\"Confusion matrix:\\n\", confusion2)\n",
        "\n",
        "print(\"############# Test Set 3 #############\")\n",
        "pred_labels3, true_labels3, accuracy3, precision3, recall3, reports3, confusion3 = test_metrics(intra_classifier, cross_data_test3)\n",
        "print(\"Accuracy:\", accuracy3)\n",
        "print(\"Precision:\", precision3)\n",
        "print(\"Recall:\", recall3)\n",
        "print(reports3)\n",
        "print(\"Confusion matrix:\\n\", confusion3)"
      ],
      "metadata": {
        "id": "BvbqRJ8s3zW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20bd27e-5987-4a70-bb46-6c292dde2f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############# Test Set 1 #############\n",
            "Accuracy: 0.3999143346659052\n",
            "Precision: 0.41695500276041\n",
            "Recall: 0.3997835950032526\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        rest       0.25      0.00      0.00      6997\n",
            "        math       0.80      0.47      0.59      7007\n",
            "      memory       0.09      0.13      0.11      7006\n",
            "       motor       0.53      1.00      0.69      7006\n",
            "\n",
            "    accuracy                           0.40     28016\n",
            "   macro avg       0.42      0.40      0.35     28016\n",
            "weighted avg       0.42      0.40      0.35     28016\n",
            "\n",
            "Confusion matrix:\n",
            " [[   1    0 6066  930]\n",
            " [   1 3264 3742    0]\n",
            " [   1  816  936 5253]\n",
            " [   1    2    0 7003]]\n",
            "############# Test Set 2 #############\n",
            "Accuracy: 0.7884066247858367\n",
            "Precision: 0.7881226823312776\n",
            "Recall: 0.788342038429152\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        rest       1.00      1.00      1.00      7006\n",
            "        math       0.58      0.56      0.57      6998\n",
            "      memory       0.57      0.59      0.58      7006\n",
            "       motor       1.00      1.00      1.00      7006\n",
            "\n",
            "    accuracy                           0.79     28016\n",
            "   macro avg       0.79      0.79      0.79     28016\n",
            "weighted avg       0.79      0.79      0.79     28016\n",
            "\n",
            "Confusion matrix:\n",
            " [[7005    0    1    0]\n",
            " [   0 3934 3064    0]\n",
            " [  19 2841 4145    1]\n",
            " [   1    0    1 7004]]\n",
            "############# Test Set 3 #############\n",
            "Accuracy: 0.37896202170188464\n",
            "Precision: 0.5805638158424381\n",
            "Recall: 0.3788066268460427\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        rest       1.00      0.50      0.67      7007\n",
            "        math       0.98      0.02      0.03      6995\n",
            "      memory       0.00      0.00      0.00      7007\n",
            "       motor       0.34      1.00      0.51      7007\n",
            "\n",
            "    accuracy                           0.38     28016\n",
            "   macro avg       0.58      0.38      0.30     28016\n",
            "weighted avg       0.58      0.38      0.30     28016\n",
            "\n",
            "Confusion matrix:\n",
            " [[3503    0 3504    0]\n",
            " [   0  112  300 6583]\n",
            " [   1    2    0 7004]\n",
            " [   0    0    5 7002]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter search"
      ],
      "metadata": {
        "id": "DYnhcQrz5cjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_search(model, samples, param_range, epochs=4, is_cross=True):\n",
        "    # param_range: dictionary with generators for the parameters\n",
        "    best_model = None\n",
        "    best_param = None\n",
        "    best_accuracy = 0\n",
        "    for i in range(samples):\n",
        "\n",
        "        if is_cross:\n",
        "            train_ = cross_data_train\n",
        "            test_ = cross_data_test1\n",
        "        else:\n",
        "            train_ = intra_data_train\n",
        "            test_ = intra_data_test\n",
        "\n",
        "        sample_param = {}\n",
        "        for k, v in param_range.items():\n",
        "            sample_param[k] = v()\n",
        "        classifier = MEGClassifier(**sample_param).to(device)\n",
        "        history, val_history = train(classifier, train_, test_, epochs=epochs, lr=2e-5, name=f\"random_search_{is_cross}_{i}\")\n",
        "        pred_labels, true_labels, accuracy, precision, recall, reports, confusion = test_metrics(classifier, test_)\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_model = classifier\n",
        "            best_param = sample_param\n",
        "            best_accuracy = accuracy\n",
        "    return best_model, best_param, best_accuracy"
      ],
      "metadata": {
        "id": "zmeyg0Uc5kgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_range = {\n",
        "    \"res_blocks\": lambda: np.random.choice([1, 3, 5], 1).item(),\n",
        "    \"res_block_size\": lambda: np.random.choice([2, 3], 1).item(), \n",
        "    \"input_channels\": lambda: 248, \n",
        "    \"downsample\": lambda: 0.5\n",
        "}"
      ],
      "metadata": {
        "id": "ao__XA0I8QNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, best_cross_param, best_cross_accuracy = random_search(MEGClassifier, 10, param_range, epochs=3, is_cross=True)\n",
        "print(best_cross_param)\n",
        "print(\"Accuracy:\", best_cross_accuracy)\n",
        "torch.save(best_model.state_dict(), \"best_cross_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_gfE9Zf9KgX",
        "outputId": "d030004d-cef4-4b3a-8c80-083d11925729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "  batch 10 loss: 0.13013877868652343\n",
            "  batch 20 loss: 0.11290889978408813\n",
            "  batch 30 loss: 0.09670919179916382\n",
            "  batch 40 loss: 0.08013597130775452\n",
            "  batch 50 loss: 0.06852532625198364\n",
            "  batch 60 loss: 0.057302439212799074\n",
            "  batch 70 loss: 0.050299590826034545\n",
            "  batch 80 loss: 0.04734691083431244\n",
            "  batch 90 loss: 0.045319277048110965\n",
            "  batch 100 loss: 0.034149950742721556\n",
            "  batch 110 loss: 0.032622674107551576\n",
            "  batch 120 loss: 0.024947550892829896\n",
            "  batch 130 loss: 0.02263665497303009\n",
            "  batch 140 loss: 0.017024631798267364\n",
            "  batch 150 loss: 0.01217714548110962\n",
            "  batch 160 loss: 0.01379900872707367\n",
            "  batch 170 loss: 0.01298072338104248\n",
            "  batch 180 loss: 0.008580560982227325\n",
            "  batch 190 loss: 0.006281262636184693\n",
            "  batch 200 loss: 0.005419099330902099\n",
            "  batch 210 loss: 0.007340378314256668\n",
            "  batch 220 loss: 0.004097068309783935\n",
            "  batch 230 loss: 0.004906826838850975\n",
            "  batch 240 loss: 0.005060631036758423\n",
            "  batch 250 loss: 0.004401919990777969\n",
            "  batch 260 loss: 0.0022729463875293733\n",
            "  batch 270 loss: 0.003197597712278366\n",
            "  batch 280 loss: 0.002252291701734066\n",
            "  batch 290 loss: 0.0027196701616048814\n",
            "  batch 300 loss: 0.003436320275068283\n",
            "  batch 310 loss: 0.0034000229090452192\n",
            "  batch 320 loss: 0.001898612082004547\n",
            "  batch 330 loss: 0.0021655740216374396\n",
            "  batch 340 loss: 0.0018695423379540444\n",
            "  batch 350 loss: 0.00199365746229887\n",
            "  batch 360 loss: 0.001907530426979065\n",
            "  batch 370 loss: 0.002526342310011387\n",
            "  batch 380 loss: 0.002711915969848633\n",
            "  batch 390 loss: 0.0022485777735710144\n",
            "  batch 400 loss: 0.003685012087225914\n",
            "  batch 410 loss: 0.0019709788262844085\n",
            "  batch 420 loss: 0.0008425884880125523\n",
            "  batch 430 loss: 0.0015639105811715126\n",
            "LOSS train 0.0015639105811715126 valid 1.8062756916159364\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.0015369150787591934\n",
            "  batch 20 loss: 0.0019281458109617234\n",
            "  batch 30 loss: 0.0005236455239355564\n",
            "  batch 40 loss: 0.0008672194555401802\n",
            "  batch 50 loss: 0.00035754265263676643\n",
            "  batch 60 loss: 0.00047387070953845977\n",
            "  batch 70 loss: 0.0014242907986044885\n",
            "  batch 80 loss: 0.00040710354223847387\n",
            "  batch 90 loss: 0.0007461375091224909\n",
            "  batch 100 loss: 0.00033944076858460903\n",
            "  batch 110 loss: 0.0007620820309966802\n",
            "  batch 120 loss: 0.0009749911725521088\n",
            "  batch 130 loss: 0.0006648728623986245\n",
            "  batch 140 loss: 0.0006015065591782331\n",
            "  batch 150 loss: 0.00029544001445174216\n",
            "  batch 160 loss: 0.0005718793720006943\n",
            "  batch 170 loss: 0.0009480316191911697\n",
            "  batch 180 loss: 0.0014100010506808759\n",
            "  batch 190 loss: 0.0007146202027797699\n",
            "  batch 200 loss: 0.0008337389677762985\n",
            "  batch 210 loss: 0.0015248041599988937\n",
            "  batch 220 loss: 0.0002443818375468254\n",
            "  batch 230 loss: 0.0023664701730012894\n",
            "  batch 240 loss: 0.0003682065522298217\n",
            "  batch 250 loss: 0.0007712898775935173\n",
            "  batch 260 loss: 0.00021925531327724457\n",
            "  batch 270 loss: 0.0004885877482593059\n",
            "  batch 280 loss: 0.000946098193526268\n",
            "  batch 290 loss: 0.00023361677303910256\n",
            "  batch 300 loss: 0.0003446128452196717\n",
            "  batch 310 loss: 0.0005369645543396473\n",
            "  batch 320 loss: 0.000256101181730628\n",
            "  batch 330 loss: 0.0009395897388458252\n",
            "  batch 340 loss: 0.00026332233101129533\n",
            "  batch 350 loss: 0.0005322686396539211\n",
            "  batch 360 loss: 0.00016297438414767385\n",
            "  batch 370 loss: 0.00034641495440155266\n",
            "  batch 380 loss: 0.00017193728126585484\n",
            "  batch 390 loss: 0.00015171917621046305\n",
            "  batch 400 loss: 0.001237890962511301\n",
            "  batch 410 loss: 0.00010096305049955845\n",
            "  batch 420 loss: 0.0007762880995869637\n",
            "  batch 430 loss: 0.0006101456936448812\n",
            "LOSS train 0.0006101456936448812 valid 2.6744547269613266\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.00016988877905532718\n",
            "  batch 20 loss: 0.0001675959676504135\n",
            "  batch 30 loss: 0.0024642849341034887\n",
            "  batch 40 loss: 0.00020459124352782964\n",
            "  batch 50 loss: 0.00011762874200940132\n",
            "  batch 60 loss: 0.00010834471322596073\n",
            "  batch 70 loss: 0.00015887008048594\n",
            "  batch 80 loss: 0.00013444763608276843\n",
            "  batch 90 loss: 7.598950760439039e-05\n",
            "  batch 100 loss: 0.0005936650559306145\n",
            "  batch 110 loss: 8.551893988624215e-05\n",
            "  batch 120 loss: 0.0009116647765040397\n",
            "  batch 130 loss: 0.00012696610065177083\n",
            "  batch 140 loss: 0.00114095788449049\n",
            "  batch 150 loss: 0.000836117472499609\n",
            "  batch 160 loss: 0.0009282045066356659\n",
            "  batch 170 loss: 0.0001431833254173398\n",
            "  batch 180 loss: 0.00019130357541143894\n",
            "  batch 190 loss: 0.001903662458062172\n",
            "  batch 200 loss: 0.0001377287320792675\n",
            "  batch 210 loss: 8.327167015522718e-05\n",
            "  batch 220 loss: 0.00011641584569588304\n",
            "  batch 230 loss: 0.00015443674055859447\n",
            "  batch 240 loss: 0.00026119924150407313\n",
            "  batch 250 loss: 7.17543181963265e-05\n",
            "  batch 260 loss: 6.610658019781112e-05\n",
            "  batch 270 loss: 0.001420166715979576\n",
            "  batch 280 loss: 4.1056604823097585e-05\n",
            "  batch 290 loss: 0.00013895407319068908\n",
            "  batch 300 loss: 0.00042081428691744807\n",
            "  batch 310 loss: 0.0001432774355635047\n",
            "  batch 320 loss: 8.74497345648706e-05\n",
            "  batch 330 loss: 0.0001737206126563251\n",
            "  batch 340 loss: 6.661539664492011e-05\n",
            "  batch 350 loss: 0.00015952554531395435\n",
            "  batch 360 loss: 2.875386562664062e-05\n",
            "  batch 370 loss: 4.247991601005197e-05\n",
            "  batch 380 loss: 3.031519299838692e-05\n",
            "  batch 390 loss: 0.00012404171284288167\n",
            "  batch 400 loss: 0.0001111718942411244\n",
            "  batch 410 loss: 0.0001719894236885011\n",
            "  batch 420 loss: 0.001075209304690361\n",
            "  batch 430 loss: 9.329665917903184e-05\n",
            "LOSS train 9.329665917903184e-05 valid 2.432504924697287\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.12160375118255615\n",
            "  batch 20 loss: 0.10944757461547852\n",
            "  batch 30 loss: 0.08213952779769898\n",
            "  batch 40 loss: 0.06823182106018066\n",
            "  batch 50 loss: 0.06870315670967102\n",
            "  batch 60 loss: 0.060255002975463864\n",
            "  batch 70 loss: 0.06134752631187439\n",
            "  batch 80 loss: 0.0471002459526062\n",
            "  batch 90 loss: 0.03487585783004761\n",
            "  batch 100 loss: 0.03422029018402099\n",
            "  batch 110 loss: 0.033970695734024045\n",
            "  batch 120 loss: 0.03370987176895142\n",
            "  batch 130 loss: 0.021604906022548675\n",
            "  batch 140 loss: 0.01946777105331421\n",
            "  batch 150 loss: 0.01624334752559662\n",
            "  batch 160 loss: 0.015231579542160034\n",
            "  batch 170 loss: 0.01155785471200943\n",
            "  batch 180 loss: 0.00970822274684906\n",
            "  batch 190 loss: 0.009388569742441177\n",
            "  batch 200 loss: 0.014506766200065612\n",
            "  batch 210 loss: 0.007922296226024628\n",
            "  batch 220 loss: 0.00911465883255005\n",
            "  batch 230 loss: 0.008715067058801651\n",
            "  batch 240 loss: 0.00997634083032608\n",
            "  batch 250 loss: 0.005022182315587998\n",
            "  batch 260 loss: 0.005641040951013565\n",
            "  batch 270 loss: 0.004671517759561539\n",
            "  batch 280 loss: 0.015536049008369445\n",
            "  batch 290 loss: 0.006362259387969971\n",
            "  batch 300 loss: 0.008409111946821212\n",
            "  batch 310 loss: 0.0041649904102087024\n",
            "  batch 320 loss: 0.004299446940422058\n",
            "  batch 330 loss: 0.004194967448711395\n",
            "  batch 340 loss: 0.0038216494023799895\n",
            "  batch 350 loss: 0.0040400601923465725\n",
            "  batch 360 loss: 0.004217585176229477\n",
            "  batch 370 loss: 0.005579252541065216\n",
            "  batch 380 loss: 0.006258993595838547\n",
            "  batch 390 loss: 0.0063774891197681425\n",
            "  batch 400 loss: 0.004624602198600769\n",
            "  batch 410 loss: 0.005408914014697075\n",
            "  batch 420 loss: 0.0034585636109113692\n",
            "  batch 430 loss: 0.002301749587059021\n",
            "LOSS train 0.002301749587059021 valid 3.0171398606032267\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.003256020322442055\n",
            "  batch 20 loss: 0.0023322977125644685\n",
            "  batch 30 loss: 0.00451456643640995\n",
            "  batch 40 loss: 0.0029272070154547692\n",
            "  batch 50 loss: 0.004604699835181236\n",
            "  batch 60 loss: 0.0050248377025127414\n",
            "  batch 70 loss: 0.0018175331875681876\n",
            "  batch 80 loss: 0.0038988042622804643\n",
            "  batch 90 loss: 0.001863805577158928\n",
            "  batch 100 loss: 0.0027237690985202788\n",
            "  batch 110 loss: 0.0021778956055641175\n",
            "  batch 120 loss: 0.0017892021685838699\n",
            "  batch 130 loss: 0.002583080902695656\n",
            "  batch 140 loss: 0.007717753946781159\n",
            "  batch 150 loss: 0.001672334223985672\n",
            "  batch 160 loss: 0.0059413667768239975\n",
            "  batch 170 loss: 0.000903162732720375\n",
            "  batch 180 loss: 0.001126826833933592\n",
            "  batch 190 loss: 0.0034259848296642305\n",
            "  batch 200 loss: 0.0013616444543004037\n",
            "  batch 210 loss: 0.003316105902194977\n",
            "  batch 220 loss: 0.0044815272092819216\n",
            "  batch 230 loss: 0.0034648921340703965\n",
            "  batch 240 loss: 0.002487727627158165\n",
            "  batch 250 loss: 0.005332282930612564\n",
            "  batch 260 loss: 0.0009015440940856933\n",
            "  batch 270 loss: 0.0011232701130211354\n",
            "  batch 280 loss: 0.0036610081791877747\n",
            "  batch 290 loss: 0.0012106269598007203\n",
            "  batch 300 loss: 0.0016270786523818969\n",
            "  batch 310 loss: 0.0026492560282349586\n",
            "  batch 320 loss: 0.0030872341245412826\n",
            "  batch 330 loss: 0.0008670703507959842\n",
            "  batch 340 loss: 0.001333494484424591\n",
            "  batch 350 loss: 0.001755690574645996\n",
            "  batch 360 loss: 0.0006969398818910122\n",
            "  batch 370 loss: 0.0002991563640534878\n",
            "  batch 380 loss: 0.0014049571007490158\n",
            "  batch 390 loss: 0.0020991820842027665\n",
            "  batch 400 loss: 0.001827242225408554\n",
            "  batch 410 loss: 0.0013507336378097534\n",
            "  batch 420 loss: 0.0017318546772003174\n",
            "  batch 430 loss: 0.002137518487870693\n",
            "LOSS train 0.002137518487870693 valid 4.49333856462131\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.001729094609618187\n",
            "  batch 20 loss: 0.004340703412890434\n",
            "  batch 30 loss: 0.0005703110247850419\n",
            "  batch 40 loss: 0.0005947154480963945\n",
            "  batch 50 loss: 0.00036254166625440123\n",
            "  batch 60 loss: 0.0010703882202506066\n",
            "  batch 70 loss: 0.0003162195906043053\n",
            "  batch 80 loss: 0.0012048245407640934\n",
            "  batch 90 loss: 0.0005686982069164515\n",
            "  batch 100 loss: 0.0027534957975149156\n",
            "  batch 110 loss: 0.0016883892938494682\n",
            "  batch 120 loss: 0.0008380118757486344\n",
            "  batch 130 loss: 0.0011539870873093605\n",
            "  batch 140 loss: 0.003255469724535942\n",
            "  batch 150 loss: 0.0008138215169310569\n",
            "  batch 160 loss: 0.0012940894812345505\n",
            "  batch 170 loss: 0.0005481925792992115\n",
            "  batch 180 loss: 0.0011156881228089333\n",
            "  batch 190 loss: 0.0007985355332493782\n",
            "  batch 200 loss: 0.0024902336299419403\n",
            "  batch 210 loss: 0.0010972077026963234\n",
            "  batch 220 loss: 0.0010339880362153053\n",
            "  batch 230 loss: 0.0026964390650391578\n",
            "  batch 240 loss: 0.0008551178500056267\n",
            "  batch 250 loss: 0.0008745542727410793\n",
            "  batch 260 loss: 0.00030881520360708236\n",
            "  batch 270 loss: 0.0004169702529907227\n",
            "  batch 280 loss: 0.00015173180727288127\n",
            "  batch 290 loss: 0.0022999519482254983\n",
            "  batch 300 loss: 0.0020098067820072175\n",
            "  batch 310 loss: 0.0001603901735506952\n",
            "  batch 320 loss: 0.001440194621682167\n",
            "  batch 330 loss: 0.0028891466557979585\n",
            "  batch 340 loss: 0.002215770073235035\n",
            "  batch 350 loss: 0.00053434818983078\n",
            "  batch 360 loss: 0.0009247038513422012\n",
            "  batch 370 loss: 0.003948568925261497\n",
            "  batch 380 loss: 0.0003045241581276059\n",
            "  batch 390 loss: 0.0006862782873213291\n",
            "  batch 400 loss: 0.0008314918726682663\n",
            "  batch 410 loss: 0.0029008865356445314\n",
            "  batch 420 loss: 0.003295079246163368\n",
            "  batch 430 loss: 0.002876598760485649\n",
            "LOSS train 0.002876598760485649 valid 9.180152379204138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "  batch 10 loss: 0.1257521390914917\n",
            "  batch 20 loss: 0.10909409523010254\n",
            "  batch 30 loss: 0.0936348557472229\n",
            "  batch 40 loss: 0.07828993201255799\n",
            "  batch 50 loss: 0.06846886873245239\n",
            "  batch 60 loss: 0.06605583429336548\n",
            "  batch 70 loss: 0.0625554621219635\n",
            "  batch 80 loss: 0.0629896104335785\n",
            "  batch 90 loss: 0.0493851900100708\n",
            "  batch 100 loss: 0.042828336358070374\n",
            "  batch 110 loss: 0.04213797450065613\n",
            "  batch 120 loss: 0.032576605677604675\n",
            "  batch 130 loss: 0.02596684992313385\n",
            "  batch 140 loss: 0.020522518455982207\n",
            "  batch 150 loss: 0.01935880780220032\n",
            "  batch 160 loss: 0.016745400428771973\n",
            "  batch 170 loss: 0.010215936601161957\n",
            "  batch 180 loss: 0.011330153048038482\n",
            "  batch 190 loss: 0.00999646782875061\n",
            "  batch 200 loss: 0.006212945282459259\n",
            "  batch 210 loss: 0.006443243473768234\n",
            "  batch 220 loss: 0.0058249164372682575\n",
            "  batch 230 loss: 0.005382799357175827\n",
            "  batch 240 loss: 0.004973930865526199\n",
            "  batch 250 loss: 0.0033547937870025634\n",
            "  batch 260 loss: 0.003273835778236389\n",
            "  batch 270 loss: 0.004780545085668564\n",
            "  batch 280 loss: 0.0028401551768183707\n",
            "  batch 290 loss: 0.0025317341089248655\n",
            "  batch 300 loss: 0.0026724416762590407\n",
            "  batch 310 loss: 0.0035700298845767974\n",
            "  batch 320 loss: 0.0033849813044071198\n",
            "  batch 330 loss: 0.0027582528069615363\n",
            "  batch 340 loss: 0.0053203865885734555\n",
            "  batch 350 loss: 0.002122986689209938\n",
            "  batch 360 loss: 0.0030108030885457994\n",
            "  batch 370 loss: 0.004006236419081688\n",
            "  batch 380 loss: 0.00132283978164196\n",
            "  batch 390 loss: 0.0022151410579681397\n",
            "  batch 400 loss: 0.0022606929764151574\n",
            "  batch 410 loss: 0.0016276659443974495\n",
            "  batch 420 loss: 0.0010086831636726857\n",
            "  batch 430 loss: 0.0009233023971319199\n",
            "LOSS train 0.0009233023971319199 valid 2.799705871340466\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.0017521781846880913\n",
            "  batch 20 loss: 0.0007323257625102997\n",
            "  batch 30 loss: 0.0014977958053350449\n",
            "  batch 40 loss: 0.0021466732025146483\n",
            "  batch 50 loss: 0.0012674960307776928\n",
            "  batch 60 loss: 0.002559000067412853\n",
            "  batch 70 loss: 0.0013590319082140922\n",
            "  batch 80 loss: 0.001212596707046032\n",
            "  batch 90 loss: 0.0008457768708467484\n",
            "  batch 100 loss: 0.0006822222843766212\n",
            "  batch 110 loss: 0.0007495139725506306\n",
            "  batch 120 loss: 0.0005987342912703753\n",
            "  batch 130 loss: 0.0007226951420307159\n",
            "  batch 140 loss: 0.0005286458414047957\n",
            "  batch 150 loss: 0.0007214432582259179\n",
            "  batch 160 loss: 0.0008554655127227306\n",
            "  batch 170 loss: 0.0007878197357058525\n",
            "  batch 180 loss: 0.001171083189547062\n",
            "  batch 190 loss: 0.0003841670695692301\n",
            "  batch 200 loss: 0.00042319162748754023\n",
            "  batch 210 loss: 0.0002659246325492859\n",
            "  batch 220 loss: 0.0002997774165123701\n",
            "  batch 230 loss: 0.0003148975316435099\n",
            "  batch 240 loss: 0.00026820769999176265\n",
            "  batch 250 loss: 0.001130480319261551\n",
            "  batch 260 loss: 0.00017814001766964794\n",
            "  batch 270 loss: 0.00016606799326837062\n",
            "  batch 280 loss: 0.000467248959466815\n",
            "  batch 290 loss: 0.0002951173577457666\n",
            "  batch 300 loss: 0.00033941762521862986\n",
            "  batch 310 loss: 0.0002607236383482814\n",
            "  batch 320 loss: 0.0002592347795143723\n",
            "  batch 330 loss: 0.002740577608346939\n",
            "  batch 340 loss: 0.00013404889032244682\n",
            "  batch 350 loss: 0.0001251955982297659\n",
            "  batch 360 loss: 0.00017153233056887984\n",
            "  batch 370 loss: 0.0006210310384631157\n",
            "  batch 380 loss: 0.00021118137519806623\n",
            "  batch 390 loss: 0.00048209987580776214\n",
            "  batch 400 loss: 0.00023762336932122707\n",
            "  batch 410 loss: 0.00012370307231321931\n",
            "  batch 420 loss: 0.00018368547316640615\n",
            "  batch 430 loss: 0.0001831346075050533\n",
            "LOSS train 0.0001831346075050533 valid 2.9479920213747572\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.0002583550289273262\n",
            "  batch 20 loss: 0.00047947484999895097\n",
            "  batch 30 loss: 0.00011900903191417456\n",
            "  batch 40 loss: 0.0004162805154919624\n",
            "  batch 50 loss: 0.00044363574124872684\n",
            "  batch 60 loss: 0.00027579381130635736\n",
            "  batch 70 loss: 0.0006109606008976697\n",
            "  batch 80 loss: 0.00014069614699110388\n",
            "  batch 90 loss: 0.00012828628532588482\n",
            "  batch 100 loss: 0.00011520991101861\n",
            "  batch 110 loss: 0.0005974047817289829\n",
            "  batch 120 loss: 0.00019789091311395168\n",
            "  batch 130 loss: 0.00047774468548595906\n",
            "  batch 140 loss: 0.0005809927359223365\n",
            "  batch 150 loss: 0.00041006682440638544\n",
            "  batch 160 loss: 0.00010295000392943621\n",
            "  batch 170 loss: 7.994510233402252e-05\n",
            "  batch 180 loss: 9.771320037543774e-05\n",
            "  batch 190 loss: 0.00011254576966166497\n",
            "  batch 200 loss: 0.00022583045065402985\n",
            "  batch 210 loss: 0.0014522390440106392\n",
            "  batch 220 loss: 0.001894720457494259\n",
            "  batch 230 loss: 0.00017757988534867765\n",
            "  batch 240 loss: 0.0001006348873488605\n",
            "  batch 250 loss: 8.052012417465448e-05\n",
            "  batch 260 loss: 7.00124423019588e-05\n",
            "  batch 270 loss: 0.001371001172810793\n",
            "  batch 280 loss: 5.9485528618097304e-05\n",
            "  batch 290 loss: 5.2079849410802126e-05\n",
            "  batch 300 loss: 0.00027270142454653977\n",
            "  batch 310 loss: 6.21768762357533e-05\n",
            "  batch 320 loss: 0.00013930030399933457\n",
            "  batch 330 loss: 0.00023960899561643602\n",
            "  batch 340 loss: 7.420594338327646e-05\n",
            "  batch 350 loss: 8.314232691191137e-05\n",
            "  batch 360 loss: 0.00010877989698201417\n",
            "  batch 370 loss: 0.00021161357872188092\n",
            "  batch 380 loss: 0.00016226675361394883\n",
            "  batch 390 loss: 0.0001570962369441986\n",
            "  batch 400 loss: 0.0001193367876112461\n",
            "  batch 410 loss: 0.0002494927030056715\n",
            "  batch 420 loss: 0.0025508185848593713\n",
            "  batch 430 loss: 0.00015704678371548654\n",
            "LOSS train 0.00015704678371548654 valid 2.7571615155391878\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.1283828854560852\n",
            "  batch 20 loss: 0.11853125095367431\n",
            "  batch 30 loss: 0.1073045015335083\n",
            "  batch 40 loss: 0.09416324496269227\n",
            "  batch 50 loss: 0.08042986392974853\n",
            "  batch 60 loss: 0.06855651140213012\n",
            "  batch 70 loss: 0.05393266677856445\n",
            "  batch 80 loss: 0.04495167136192322\n",
            "  batch 90 loss: 0.032749950885772705\n",
            "  batch 100 loss: 0.02861236333847046\n",
            "  batch 110 loss: 0.021134567260742188\n",
            "  batch 120 loss: 0.01839245855808258\n",
            "  batch 130 loss: 0.013610824942588806\n",
            "  batch 140 loss: 0.01301107108592987\n",
            "  batch 150 loss: 0.008812399208545684\n",
            "  batch 160 loss: 0.010776101052761078\n",
            "  batch 170 loss: 0.008309917151927948\n",
            "  batch 180 loss: 0.008567560464143753\n",
            "  batch 190 loss: 0.0075240589678287504\n",
            "  batch 200 loss: 0.00460839681327343\n",
            "  batch 210 loss: 0.005293767899274826\n",
            "  batch 220 loss: 0.004174632579088211\n",
            "  batch 230 loss: 0.003359825909137726\n",
            "  batch 240 loss: 0.00697522759437561\n",
            "  batch 250 loss: 0.003617152199149132\n",
            "  batch 260 loss: 0.0029554123058915137\n",
            "  batch 270 loss: 0.002100015804171562\n",
            "  batch 280 loss: 0.003112513944506645\n",
            "  batch 290 loss: 0.002008255571126938\n",
            "  batch 300 loss: 0.001890021376311779\n",
            "  batch 310 loss: 0.0021524414420127868\n",
            "  batch 320 loss: 0.0019796730950474737\n",
            "  batch 330 loss: 0.003605522960424423\n",
            "  batch 340 loss: 0.00286489836871624\n",
            "  batch 350 loss: 0.0021527230739593504\n",
            "  batch 360 loss: 0.00118605587631464\n",
            "  batch 370 loss: 0.002905033901333809\n",
            "  batch 380 loss: 0.0010594421066343785\n",
            "  batch 390 loss: 0.0017757272347807885\n",
            "  batch 400 loss: 0.000995069555938244\n",
            "  batch 410 loss: 0.0014073389582335949\n",
            "  batch 420 loss: 0.0013408893719315529\n",
            "  batch 430 loss: 0.00167598444968462\n",
            "LOSS train 0.00167598444968462 valid 1.8440608326345682\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.0009979978203773499\n",
            "  batch 20 loss: 0.0009936146438121797\n",
            "  batch 30 loss: 0.0008034856989979744\n",
            "  batch 40 loss: 0.0007256399840116501\n",
            "  batch 50 loss: 0.0009305844083428383\n",
            "  batch 60 loss: 0.001157183852046728\n",
            "  batch 70 loss: 0.0006296172272413969\n",
            "  batch 80 loss: 0.0010295536369085312\n",
            "  batch 90 loss: 0.0016455812379717826\n",
            "  batch 100 loss: 0.0005792819429188967\n",
            "  batch 110 loss: 0.00046921526081860064\n",
            "  batch 120 loss: 0.000528409332036972\n",
            "  batch 130 loss: 0.0009593304246664047\n",
            "  batch 140 loss: 0.0005374782718718052\n",
            "  batch 150 loss: 0.0005755760241299868\n",
            "  batch 160 loss: 0.0005742418114095926\n",
            "  batch 170 loss: 0.0005036123096942901\n",
            "  batch 180 loss: 0.000570653099566698\n",
            "  batch 190 loss: 0.001243414916098118\n",
            "  batch 200 loss: 0.0003773624543100595\n",
            "  batch 210 loss: 0.0011562916450202465\n",
            "  batch 220 loss: 0.0004148161970078945\n",
            "  batch 230 loss: 0.0007178832311183214\n",
            "  batch 240 loss: 0.000513833062723279\n",
            "  batch 250 loss: 0.0002752720145508647\n",
            "  batch 260 loss: 0.0004102625884115696\n",
            "  batch 270 loss: 0.00028895181603729727\n",
            "  batch 280 loss: 0.0003634660504758358\n",
            "  batch 290 loss: 0.0003183780238032341\n",
            "  batch 300 loss: 0.0005194495432078838\n",
            "  batch 310 loss: 0.00023616161197423935\n",
            "  batch 320 loss: 0.00022179908119142057\n",
            "  batch 330 loss: 0.00021792505867779255\n",
            "  batch 340 loss: 0.00026457265485078094\n",
            "  batch 350 loss: 0.000461298692971468\n",
            "  batch 360 loss: 0.0006634579971432686\n",
            "  batch 370 loss: 0.0002954262774437666\n",
            "  batch 380 loss: 0.00044578956440091135\n",
            "  batch 390 loss: 0.0006423314101994037\n",
            "  batch 400 loss: 0.00026098284870386125\n",
            "  batch 410 loss: 0.002717612311244011\n",
            "  batch 420 loss: 0.0003474485594779253\n",
            "  batch 430 loss: 0.00016343067400157452\n",
            "LOSS train 0.00016343067400157452 valid 1.293363647312181\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.000289320619776845\n",
            "  batch 20 loss: 0.0009996633045375348\n",
            "  batch 30 loss: 0.00017322072526440024\n",
            "  batch 40 loss: 0.0003554415190592408\n",
            "  batch 50 loss: 0.0001787142944522202\n",
            "  batch 60 loss: 0.0008428346365690232\n",
            "  batch 70 loss: 0.00012389806797727943\n",
            "  batch 80 loss: 0.0001848264946602285\n",
            "  batch 90 loss: 0.001116699166595936\n",
            "  batch 100 loss: 0.0001962868496775627\n",
            "  batch 110 loss: 0.0001837277552112937\n",
            "  batch 120 loss: 0.0006364147644490004\n",
            "  batch 130 loss: 0.0005381792318075896\n",
            "  batch 140 loss: 0.00020509730093181132\n",
            "  batch 150 loss: 0.001388371642678976\n",
            "  batch 160 loss: 0.00030166513752192257\n",
            "  batch 170 loss: 0.0008348019793629647\n",
            "  batch 180 loss: 0.00012115475255995989\n",
            "  batch 190 loss: 0.00012058310676366091\n",
            "  batch 200 loss: 0.00014154331292957068\n",
            "  batch 210 loss: 0.0002355191158130765\n",
            "  batch 220 loss: 9.481085580773652e-05\n",
            "  batch 230 loss: 0.0001779046142473817\n",
            "  batch 240 loss: 0.0003524743719026446\n",
            "  batch 250 loss: 0.00013499979395419358\n",
            "  batch 260 loss: 0.00015072997193783523\n",
            "  batch 270 loss: 8.378751808777452e-05\n",
            "  batch 280 loss: 0.0008187328465282917\n",
            "  batch 290 loss: 0.0002350427908822894\n",
            "  batch 300 loss: 0.0006194435525685548\n",
            "  batch 310 loss: 0.0001450655166991055\n",
            "  batch 320 loss: 5.444026319310069e-05\n",
            "  batch 330 loss: 6.202494259923696e-05\n",
            "  batch 340 loss: 6.921868771314621e-05\n",
            "  batch 350 loss: 0.000199280958622694\n",
            "  batch 360 loss: 0.0004324892535805702\n",
            "  batch 370 loss: 0.00014328227844089269\n",
            "  batch 380 loss: 0.0001949951401911676\n",
            "  batch 390 loss: 5.1361072110012174e-05\n",
            "  batch 400 loss: 7.341650198213756e-05\n",
            "  batch 410 loss: 7.418893510475754e-05\n",
            "  batch 420 loss: 7.540460210293531e-05\n",
            "  batch 430 loss: 6.575386505573988e-05\n",
            "LOSS train 6.575386505573988e-05 valid 1.9076562862146644\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.11365606784820556\n",
            "  batch 20 loss: 0.0895238995552063\n",
            "  batch 30 loss: 0.07944611310958863\n",
            "  batch 40 loss: 0.0660172939300537\n",
            "  batch 50 loss: 0.06898636817932129\n",
            "  batch 60 loss: 0.05336029529571533\n",
            "  batch 70 loss: 0.05613868832588196\n",
            "  batch 80 loss: 0.04639952480792999\n",
            "  batch 90 loss: 0.045973238348960874\n",
            "  batch 100 loss: 0.03615245223045349\n",
            "  batch 110 loss: 0.03729807734489441\n",
            "  batch 120 loss: 0.02631537616252899\n",
            "  batch 130 loss: 0.024965800344944\n",
            "  batch 140 loss: 0.026054227352142335\n",
            "  batch 150 loss: 0.02180351912975311\n",
            "  batch 160 loss: 0.018276476860046388\n",
            "  batch 170 loss: 0.020712651312351227\n",
            "  batch 180 loss: 0.012502457201480865\n",
            "  batch 190 loss: 0.00941728577017784\n",
            "  batch 200 loss: 0.013645705580711365\n",
            "  batch 210 loss: 0.014048543572425843\n",
            "  batch 220 loss: 0.014513295888900758\n",
            "  batch 230 loss: 0.011469722539186478\n",
            "  batch 240 loss: 0.010440120100975036\n",
            "  batch 250 loss: 0.00957195907831192\n",
            "  batch 260 loss: 0.005532220005989075\n",
            "  batch 270 loss: 0.006061895936727524\n",
            "  batch 280 loss: 0.009781169891357421\n",
            "  batch 290 loss: 0.006382346898317337\n",
            "  batch 300 loss: 0.004937799274921417\n",
            "  batch 310 loss: 0.005666428431868553\n",
            "  batch 320 loss: 0.0030490633100271223\n",
            "  batch 330 loss: 0.002747378870844841\n",
            "  batch 340 loss: 0.006106532737612724\n",
            "  batch 350 loss: 0.004296818375587463\n",
            "  batch 360 loss: 0.006365486979484558\n",
            "  batch 370 loss: 0.00462508611381054\n",
            "  batch 380 loss: 0.0066279366612434385\n",
            "  batch 390 loss: 0.005805889517068863\n",
            "  batch 400 loss: 0.007368067651987076\n",
            "  batch 410 loss: 0.0023217622190713884\n",
            "  batch 420 loss: 0.0032434992492198942\n",
            "  batch 430 loss: 0.0027817703783512117\n",
            "LOSS train 0.0027817703783512117 valid 2.1014181632021245\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.0028721306473016737\n",
            "  batch 20 loss: 0.004355527460575104\n",
            "  batch 30 loss: 0.002495476230978966\n",
            "  batch 40 loss: 0.0028160445392131804\n",
            "  batch 50 loss: 0.0055326797068119046\n",
            "  batch 60 loss: 0.0073608458042144775\n",
            "  batch 70 loss: 0.004153410717844963\n",
            "  batch 80 loss: 0.004263981431722641\n",
            "  batch 90 loss: 0.002627304382622242\n",
            "  batch 100 loss: 0.0031246449798345566\n",
            "  batch 110 loss: 0.0027327828109264376\n",
            "  batch 120 loss: 0.004685140773653984\n",
            "  batch 130 loss: 0.0026735525578260423\n",
            "  batch 140 loss: 0.005846988037228584\n",
            "  batch 150 loss: 0.0038599155843257902\n",
            "  batch 160 loss: 0.0008672206662595272\n",
            "  batch 170 loss: 0.003099295496940613\n",
            "  batch 180 loss: 0.0023996502161026\n",
            "  batch 190 loss: 0.0017589051276445389\n",
            "  batch 200 loss: 0.0030715618282556533\n",
            "  batch 210 loss: 0.0015794932842254639\n",
            "  batch 220 loss: 0.0039407283067703245\n",
            "  batch 230 loss: 0.012056255340576172\n",
            "  batch 240 loss: 0.0012462221086025238\n",
            "  batch 250 loss: 0.0027610618621110917\n",
            "  batch 260 loss: 0.002065565064549446\n",
            "  batch 270 loss: 0.002906407415866852\n",
            "  batch 280 loss: 0.002413809671998024\n",
            "  batch 290 loss: 0.0023355992510914803\n",
            "  batch 300 loss: 0.0020389417186379433\n",
            "  batch 310 loss: 0.0011369992047548295\n",
            "  batch 320 loss: 0.0010354532860219479\n",
            "  batch 330 loss: 0.0014713766053318978\n",
            "  batch 340 loss: 0.0009065329097211361\n",
            "  batch 350 loss: 0.002234102040529251\n",
            "  batch 360 loss: 0.0012546814978122711\n",
            "  batch 370 loss: 0.0006434411276131869\n",
            "  batch 380 loss: 0.002946102060377598\n",
            "  batch 390 loss: 0.0007469337899237872\n",
            "  batch 400 loss: 0.0008291778154671192\n",
            "  batch 410 loss: 0.0012658914551138879\n",
            "  batch 420 loss: 0.0009101156145334243\n",
            "  batch 430 loss: 0.001320747658610344\n",
            "LOSS train 0.001320747658610344 valid 1.9112208518688674\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.002111649513244629\n",
            "  batch 20 loss: 0.0033726684749126436\n",
            "  batch 30 loss: 0.0026420414447784423\n",
            "  batch 40 loss: 0.001399686001241207\n",
            "  batch 50 loss: 0.0017106814309954643\n",
            "  batch 60 loss: 0.0038550421595573425\n",
            "  batch 70 loss: 0.00227670855820179\n",
            "  batch 80 loss: 0.0017415843904018402\n",
            "  batch 90 loss: 0.003412170708179474\n",
            "  batch 100 loss: 0.0036030847579240797\n",
            "  batch 110 loss: 0.0037786815315485\n",
            "  batch 120 loss: 0.0017663618549704553\n",
            "  batch 130 loss: 0.0066879414021968845\n",
            "  batch 140 loss: 0.0011308230459690094\n",
            "  batch 150 loss: 0.0007378676906228066\n",
            "  batch 160 loss: 0.0008414650335907936\n",
            "  batch 170 loss: 0.0030215242877602577\n",
            "  batch 180 loss: 0.0006932027637958527\n",
            "  batch 190 loss: 0.001461078319698572\n",
            "  batch 200 loss: 0.0009387293830513954\n",
            "  batch 210 loss: 0.0014505902305245399\n",
            "  batch 220 loss: 0.0010368496179580689\n",
            "  batch 230 loss: 0.0005808509886264801\n",
            "  batch 240 loss: 0.002753317356109619\n",
            "  batch 250 loss: 0.002711632288992405\n",
            "  batch 260 loss: 0.00043078437447547914\n",
            "  batch 270 loss: 0.0008548079058527946\n",
            "  batch 280 loss: 0.0007156999781727791\n",
            "  batch 290 loss: 0.0007141978479921818\n",
            "  batch 300 loss: 0.006663341075181961\n",
            "  batch 310 loss: 0.003146284818649292\n",
            "  batch 320 loss: 0.0001895901747047901\n",
            "  batch 330 loss: 0.0008018174208700657\n",
            "  batch 340 loss: 0.0008449623361229897\n",
            "  batch 350 loss: 0.0014898212626576423\n",
            "  batch 360 loss: 0.0004057088866829872\n",
            "  batch 370 loss: 0.0013298256322741508\n",
            "  batch 380 loss: 0.0016412127763032914\n",
            "  batch 390 loss: 0.0012731196358799935\n",
            "  batch 400 loss: 0.0011376753449440003\n",
            "  batch 410 loss: 0.0004631318151950836\n",
            "  batch 420 loss: 0.00163533017039299\n",
            "  batch 430 loss: 0.005891228094696999\n",
            "LOSS train 0.005891228094696999 valid 6.683825914025235\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.12792714834213256\n",
            "  batch 20 loss: 0.11029183864593506\n",
            "  batch 30 loss: 0.09500405788421631\n",
            "  batch 40 loss: 0.09159148335456849\n",
            "  batch 50 loss: 0.06867250204086303\n",
            "  batch 60 loss: 0.07253841161727906\n",
            "  batch 70 loss: 0.05819445848464966\n",
            "  batch 80 loss: 0.05950334668159485\n",
            "  batch 90 loss: 0.05680071115493775\n",
            "  batch 100 loss: 0.04044478535652161\n",
            "  batch 110 loss: 0.048712968826293945\n",
            "  batch 120 loss: 0.041579249501228335\n",
            "  batch 130 loss: 0.029059553146362306\n",
            "  batch 140 loss: 0.024915584921836854\n",
            "  batch 150 loss: 0.022034741938114166\n",
            "  batch 160 loss: 0.023782229423522948\n",
            "  batch 170 loss: 0.016961663961410522\n",
            "  batch 180 loss: 0.01576421707868576\n",
            "  batch 190 loss: 0.011462730169296265\n",
            "  batch 200 loss: 0.010347844660282135\n",
            "  batch 210 loss: 0.008734038472175598\n",
            "  batch 220 loss: 0.008572467416524888\n",
            "  batch 230 loss: 0.00973069816827774\n",
            "  batch 240 loss: 0.006489358842372894\n",
            "  batch 250 loss: 0.006117472797632218\n",
            "  batch 260 loss: 0.0038869842886924745\n",
            "  batch 270 loss: 0.006324165314435959\n",
            "  batch 280 loss: 0.0034613817930221557\n",
            "  batch 290 loss: 0.003796074539422989\n",
            "  batch 300 loss: 0.0032948806881904604\n",
            "  batch 310 loss: 0.003204576298594475\n",
            "  batch 320 loss: 0.0030513664707541464\n",
            "  batch 330 loss: 0.0023797275498509407\n",
            "  batch 340 loss: 0.0022623013705015183\n",
            "  batch 350 loss: 0.0021586952731013297\n",
            "  batch 360 loss: 0.0035322636365890505\n",
            "  batch 370 loss: 0.00449770875275135\n",
            "  batch 380 loss: 0.0022619448602199553\n",
            "  batch 390 loss: 0.002801225706934929\n",
            "  batch 400 loss: 0.003892718255519867\n",
            "  batch 410 loss: 0.002549459971487522\n",
            "  batch 420 loss: 0.0032920517027378083\n",
            "  batch 430 loss: 0.0015365594998002052\n",
            "LOSS train 0.0015365594998002052 valid 1.654312522473775\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.0020400134846568106\n",
            "  batch 20 loss: 0.002061713859438896\n",
            "  batch 30 loss: 0.003246342018246651\n",
            "  batch 40 loss: 0.004386930540204048\n",
            "  batch 50 loss: 0.002409270778298378\n",
            "  batch 60 loss: 0.0011948484927415848\n",
            "  batch 70 loss: 0.0008841576054692268\n",
            "  batch 80 loss: 0.002942407317459583\n",
            "  batch 90 loss: 0.0005573452450335026\n",
            "  batch 100 loss: 0.000848971214145422\n",
            "  batch 110 loss: 0.0004939563572406769\n",
            "  batch 120 loss: 0.0009072439745068551\n",
            "  batch 130 loss: 0.0007018386386334896\n",
            "  batch 140 loss: 0.003497033193707466\n",
            "  batch 150 loss: 0.000773095479235053\n",
            "  batch 160 loss: 0.001731768250465393\n",
            "  batch 170 loss: 0.0010306930169463157\n",
            "  batch 180 loss: 0.0009588766843080521\n",
            "  batch 190 loss: 0.00126987649127841\n",
            "  batch 200 loss: 0.0009520450606942177\n",
            "  batch 210 loss: 0.0021383505314588546\n",
            "  batch 220 loss: 0.0022576294839382173\n",
            "  batch 230 loss: 0.001288831327110529\n",
            "  batch 240 loss: 0.0018343284726142883\n",
            "  batch 250 loss: 0.0013737075962126255\n",
            "  batch 260 loss: 0.0008114413358271122\n",
            "  batch 270 loss: 0.00039071538485586644\n",
            "  batch 280 loss: 0.0013513697311282157\n",
            "  batch 290 loss: 0.002150592766702175\n",
            "  batch 300 loss: 0.0003937487956136465\n",
            "  batch 310 loss: 0.00023826421238481998\n",
            "  batch 320 loss: 0.0017816953361034394\n",
            "  batch 330 loss: 0.0003309485502541065\n",
            "  batch 340 loss: 0.0004402184393256903\n",
            "  batch 350 loss: 0.0006497298367321491\n",
            "  batch 360 loss: 0.001562649756669998\n",
            "  batch 370 loss: 0.0003209869377315044\n",
            "  batch 380 loss: 0.0002632310148328543\n",
            "  batch 390 loss: 0.00075766546651721\n",
            "  batch 400 loss: 0.0004186127334833145\n",
            "  batch 410 loss: 0.000979185476899147\n",
            "  batch 420 loss: 0.000787480641156435\n",
            "  batch 430 loss: 0.00027717421762645244\n",
            "LOSS train 0.00027717421762645244 valid 1.9705757558166122\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.0002604877576231956\n",
            "  batch 20 loss: 0.00033466578461229803\n",
            "  batch 30 loss: 0.0005136104300618172\n",
            "  batch 40 loss: 0.0005298549775034189\n",
            "  batch 50 loss: 0.0002704984974116087\n",
            "  batch 60 loss: 0.0014415104873478413\n",
            "  batch 70 loss: 0.00321592316031456\n",
            "  batch 80 loss: 0.0006787199527025223\n",
            "  batch 90 loss: 0.0003359499853104353\n",
            "  batch 100 loss: 0.000588512746617198\n",
            "  batch 110 loss: 0.00036987022031098604\n",
            "  batch 120 loss: 0.0015623930841684342\n",
            "  batch 130 loss: 0.0009889534674584865\n",
            "  batch 140 loss: 0.001571473479270935\n",
            "  batch 150 loss: 0.0003812428563833237\n",
            "  batch 160 loss: 0.0010444040410220623\n",
            "  batch 170 loss: 0.0001251743989996612\n",
            "  batch 180 loss: 0.0017094187438488006\n",
            "  batch 190 loss: 0.0001794172218069434\n",
            "  batch 200 loss: 0.0007838880643248558\n",
            "  batch 210 loss: 0.0009424345567822457\n",
            "  batch 220 loss: 0.0002294230042025447\n",
            "  batch 230 loss: 0.0006497535388916731\n",
            "  batch 240 loss: 0.0013977590948343276\n",
            "  batch 250 loss: 0.0001497634220868349\n",
            "  batch 260 loss: 0.00021249004639685155\n",
            "  batch 270 loss: 0.00016242809360846877\n",
            "  batch 280 loss: 0.00020591774955391884\n",
            "  batch 290 loss: 0.00019979909993708133\n",
            "  batch 300 loss: 0.0006091391202062369\n",
            "  batch 310 loss: 0.00018676439067348837\n",
            "  batch 320 loss: 0.00010320612927898765\n",
            "  batch 330 loss: 0.00026523133274167774\n",
            "  batch 340 loss: 0.00011683229822665453\n",
            "  batch 350 loss: 0.0007493072655051947\n",
            "  batch 360 loss: 0.00022578493226319552\n",
            "  batch 370 loss: 0.00012108301743865013\n",
            "  batch 380 loss: 0.0012382413260638714\n",
            "  batch 390 loss: 0.0001034475164487958\n",
            "  batch 400 loss: 0.0005829897243529558\n",
            "  batch 410 loss: 9.501994354650378e-05\n",
            "  batch 420 loss: 0.0005787702277302742\n",
            "  batch 430 loss: 8.929190807975829e-05\n",
            "LOSS train 8.929190807975829e-05 valid 1.2267235546036874\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.1253146767616272\n",
            "  batch 20 loss: 0.11187181472778321\n",
            "  batch 30 loss: 0.09654269218444825\n",
            "  batch 40 loss: 0.08319059610366822\n",
            "  batch 50 loss: 0.09219986200332642\n",
            "  batch 60 loss: 0.06497805714607238\n",
            "  batch 70 loss: 0.05812216997146606\n",
            "  batch 80 loss: 0.05695708990097046\n",
            "  batch 90 loss: 0.042175698280334475\n",
            "  batch 100 loss: 0.037776333093643186\n",
            "  batch 110 loss: 0.036246204376220705\n",
            "  batch 120 loss: 0.028917765617370604\n",
            "  batch 130 loss: 0.023982292413711546\n",
            "  batch 140 loss: 0.019483503699302674\n",
            "  batch 150 loss: 0.01673130691051483\n",
            "  batch 160 loss: 0.013057664036750793\n",
            "  batch 170 loss: 0.011914656311273576\n",
            "  batch 180 loss: 0.0123239204287529\n",
            "  batch 190 loss: 0.011160550266504287\n",
            "  batch 200 loss: 0.009682761877775193\n",
            "  batch 210 loss: 0.009231098741292954\n",
            "  batch 220 loss: 0.009873126447200776\n",
            "  batch 230 loss: 0.00710170716047287\n",
            "  batch 240 loss: 0.005507301166653633\n",
            "  batch 250 loss: 0.0059270069003105165\n",
            "  batch 260 loss: 0.00447288379073143\n",
            "  batch 270 loss: 0.004209297895431519\n",
            "  batch 280 loss: 0.004019640386104584\n",
            "  batch 290 loss: 0.005533485487103462\n",
            "  batch 300 loss: 0.002398265525698662\n",
            "  batch 310 loss: 0.002977082692086697\n",
            "  batch 320 loss: 0.0025837598368525505\n",
            "  batch 330 loss: 0.004149027541279793\n",
            "  batch 340 loss: 0.0038518022745847704\n",
            "  batch 350 loss: 0.001688634231686592\n",
            "  batch 360 loss: 0.0026323230937123298\n",
            "  batch 370 loss: 0.0028996217995882036\n",
            "  batch 380 loss: 0.003911304473876953\n",
            "  batch 390 loss: 0.0019173910841345787\n",
            "  batch 400 loss: 0.00213661789894104\n",
            "  batch 410 loss: 0.0027132140472531318\n",
            "  batch 420 loss: 0.0037519104778766634\n",
            "  batch 430 loss: 0.0028998902067542075\n",
            "LOSS train 0.0028998902067542075 valid 2.487090629257568\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.0011176045052707195\n",
            "  batch 20 loss: 0.00254126638174057\n",
            "  batch 30 loss: 0.0019651520997285843\n",
            "  batch 40 loss: 0.0009262774139642716\n",
            "  batch 50 loss: 0.001985890232026577\n",
            "  batch 60 loss: 0.0008589056320488453\n",
            "  batch 70 loss: 0.0012422793544828892\n",
            "  batch 80 loss: 0.0018010852858424188\n",
            "  batch 90 loss: 0.0010335521772503852\n",
            "  batch 100 loss: 0.0009878123179078101\n",
            "  batch 110 loss: 0.0015156951732933522\n",
            "  batch 120 loss: 0.0006371394731104374\n",
            "  batch 130 loss: 0.0010468312539160252\n",
            "  batch 140 loss: 0.002467137575149536\n",
            "  batch 150 loss: 0.001825924776494503\n",
            "  batch 160 loss: 0.0014773973263800145\n",
            "  batch 170 loss: 0.0008916738443076611\n",
            "  batch 180 loss: 0.0008913020603358746\n",
            "  batch 190 loss: 0.0019758036360144615\n",
            "  batch 200 loss: 0.0021946027874946593\n",
            "  batch 210 loss: 0.000892789475619793\n",
            "  batch 220 loss: 0.0008994081988930702\n",
            "  batch 230 loss: 0.00098058320581913\n",
            "  batch 240 loss: 0.0014631414785981178\n",
            "  batch 250 loss: 0.0008677387610077858\n",
            "  batch 260 loss: 0.001767784170806408\n",
            "  batch 270 loss: 0.0013552557677030564\n",
            "  batch 280 loss: 0.0008314207196235656\n",
            "  batch 290 loss: 0.0016143467277288438\n",
            "  batch 300 loss: 0.0011060417629778386\n",
            "  batch 310 loss: 0.0006130597554147244\n",
            "  batch 320 loss: 0.0010540198534727096\n",
            "  batch 330 loss: 0.0015250347554683686\n",
            "  batch 340 loss: 0.0006209557875990867\n",
            "  batch 350 loss: 0.0003499541664496064\n",
            "  batch 360 loss: 0.00038022054359316827\n",
            "  batch 370 loss: 0.0014865905046463012\n",
            "  batch 380 loss: 0.0004790430888533592\n",
            "  batch 390 loss: 0.0003360085189342499\n",
            "  batch 400 loss: 0.000419234624132514\n",
            "  batch 410 loss: 0.0008590237237513065\n",
            "  batch 420 loss: 0.00045585311017930507\n",
            "  batch 430 loss: 0.00019330072682350875\n",
            "LOSS train 0.00019330072682350875 valid 2.421918321098945\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.00026595923118293284\n",
            "  batch 20 loss: 0.000286598177626729\n",
            "  batch 30 loss: 0.0013824190013110639\n",
            "  batch 40 loss: 0.0003780855564400554\n",
            "  batch 50 loss: 0.00036363345570862294\n",
            "  batch 60 loss: 0.0005446926690638066\n",
            "  batch 70 loss: 0.0005714333150535822\n",
            "  batch 80 loss: 0.000380369764752686\n",
            "  batch 90 loss: 0.00032734796404838563\n",
            "  batch 100 loss: 0.0006278512068092823\n",
            "  batch 110 loss: 0.00014664569171145557\n",
            "  batch 120 loss: 0.00020771659910678864\n",
            "  batch 130 loss: 0.00022597617935389281\n",
            "  batch 140 loss: 0.00016820052405819297\n",
            "  batch 150 loss: 0.00010011845733970404\n",
            "  batch 160 loss: 0.00043954476714134214\n",
            "  batch 170 loss: 0.00018245421815663577\n",
            "  batch 180 loss: 0.00021754009649157524\n",
            "  batch 190 loss: 0.00033439267426729204\n",
            "  batch 200 loss: 0.0001210416667163372\n",
            "  batch 210 loss: 0.00023699230514466761\n",
            "  batch 220 loss: 0.0009319353848695755\n",
            "  batch 230 loss: 0.00025457225274294615\n",
            "  batch 240 loss: 0.0003096223343163729\n",
            "  batch 250 loss: 0.00031501371413469317\n",
            "  batch 260 loss: 0.0019431609660387039\n",
            "  batch 270 loss: 0.0009006811305880547\n",
            "  batch 280 loss: 0.0005811404436826706\n",
            "  batch 290 loss: 0.002408375032246113\n",
            "  batch 300 loss: 0.0002636675722897053\n",
            "  batch 310 loss: 0.0009157785214483738\n",
            "  batch 320 loss: 0.0002458909060806036\n",
            "  batch 330 loss: 0.0004002755507826805\n",
            "  batch 340 loss: 0.00372207798063755\n",
            "  batch 350 loss: 0.0013134248554706573\n",
            "  batch 360 loss: 0.00014940695837140083\n",
            "  batch 370 loss: 0.00020113917998969554\n",
            "  batch 380 loss: 0.0001951998332515359\n",
            "  batch 390 loss: 0.000697031244635582\n",
            "  batch 400 loss: 0.000560146989300847\n",
            "  batch 410 loss: 0.0002509729005396366\n",
            "  batch 420 loss: 0.0005055779125541449\n",
            "  batch 430 loss: 9.274611948058009e-05\n",
            "LOSS train 9.274611948058009e-05 valid 2.5222815364821427\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.1270069122314453\n",
            "  batch 20 loss: 0.12412426471710206\n",
            "  batch 30 loss: 0.1162634015083313\n",
            "  batch 40 loss: 0.11506389379501343\n",
            "  batch 50 loss: 0.10193665027618408\n",
            "  batch 60 loss: 0.10044295787811279\n",
            "  batch 70 loss: 0.0909623384475708\n",
            "  batch 80 loss: 0.08376216292381286\n",
            "  batch 90 loss: 0.07990008592605591\n",
            "  batch 100 loss: 0.07577183246612548\n",
            "  batch 110 loss: 0.06671692132949829\n",
            "  batch 120 loss: 0.062055337429046634\n",
            "  batch 130 loss: 0.05543818473815918\n",
            "  batch 140 loss: 0.052021920680999756\n",
            "  batch 150 loss: 0.04652843475341797\n",
            "  batch 160 loss: 0.042948460578918456\n",
            "  batch 170 loss: 0.03826087713241577\n",
            "  batch 180 loss: 0.03578762114048004\n",
            "  batch 190 loss: 0.03098820149898529\n",
            "  batch 200 loss: 0.03002800643444061\n",
            "  batch 210 loss: 0.026987236738204957\n",
            "  batch 220 loss: 0.023707668483257293\n",
            "  batch 230 loss: 0.021209299564361572\n",
            "  batch 240 loss: 0.01919333040714264\n",
            "  batch 250 loss: 0.015363305807113647\n",
            "  batch 260 loss: 0.014251823723316192\n",
            "  batch 270 loss: 0.015873289108276366\n",
            "  batch 280 loss: 0.014832179248332977\n",
            "  batch 290 loss: 0.010759091377258301\n",
            "  batch 300 loss: 0.010294322669506074\n",
            "  batch 310 loss: 0.01112181767821312\n",
            "  batch 320 loss: 0.01112874150276184\n",
            "  batch 330 loss: 0.009237027168273926\n",
            "  batch 340 loss: 0.009289637953042985\n",
            "  batch 350 loss: 0.007765872776508332\n",
            "  batch 360 loss: 0.005584879219532013\n",
            "  batch 370 loss: 0.006257518380880356\n",
            "  batch 380 loss: 0.008339574187994003\n",
            "  batch 390 loss: 0.005634064227342606\n",
            "  batch 400 loss: 0.006122284010052681\n",
            "  batch 410 loss: 0.00658976063132286\n",
            "  batch 420 loss: 0.005758629366755486\n",
            "  batch 430 loss: 0.007034505903720856\n",
            "LOSS train 0.007034505903720856 valid 2.014877946511523\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.004384589940309524\n",
            "  batch 20 loss: 0.004902585595846176\n",
            "  batch 30 loss: 0.0033496201038360597\n",
            "  batch 40 loss: 0.004247824847698212\n",
            "  batch 50 loss: 0.0034757088869810104\n",
            "  batch 60 loss: 0.005469351261854172\n",
            "  batch 70 loss: 0.003623620793223381\n",
            "  batch 80 loss: 0.004676071926951408\n",
            "  batch 90 loss: 0.002629659324884415\n",
            "  batch 100 loss: 0.003748650848865509\n",
            "  batch 110 loss: 0.0029376054182648657\n",
            "  batch 120 loss: 0.004018423706293106\n",
            "  batch 130 loss: 0.00207196194678545\n",
            "  batch 140 loss: 0.001694047637283802\n",
            "  batch 150 loss: 0.002355000004172325\n",
            "  batch 160 loss: 0.0018113728612661362\n",
            "  batch 170 loss: 0.002044920064508915\n",
            "  batch 180 loss: 0.0017484314739704132\n",
            "  batch 190 loss: 0.002475506067276001\n",
            "  batch 200 loss: 0.002082355320453644\n",
            "  batch 210 loss: 0.0017961215227842331\n",
            "  batch 220 loss: 0.0021047350019216537\n",
            "  batch 230 loss: 0.0032361023128032683\n",
            "  batch 240 loss: 0.0024271208792924883\n",
            "  batch 250 loss: 0.0020272042602300644\n",
            "  batch 260 loss: 0.0016150079667568208\n",
            "  batch 270 loss: 0.0015073715709149837\n",
            "  batch 280 loss: 0.0016677659004926682\n",
            "  batch 290 loss: 0.001137540489435196\n",
            "  batch 300 loss: 0.0017498323693871498\n",
            "  batch 310 loss: 0.0017839372158050537\n",
            "  batch 320 loss: 0.001695980317890644\n",
            "  batch 330 loss: 0.0011291291564702987\n",
            "  batch 340 loss: 0.0009339012205600739\n",
            "  batch 350 loss: 0.0008349182084202766\n",
            "  batch 360 loss: 0.0011106330901384353\n",
            "  batch 370 loss: 0.001001929584890604\n",
            "  batch 380 loss: 0.000941476970911026\n",
            "  batch 390 loss: 0.0007529158145189285\n",
            "  batch 400 loss: 0.000701101217418909\n",
            "  batch 410 loss: 0.0008803083561360836\n",
            "  batch 420 loss: 0.0009064455516636372\n",
            "  batch 430 loss: 0.000788367073982954\n",
            "LOSS train 0.000788367073982954 valid 2.895413643514395\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.0006724653299897909\n",
            "  batch 20 loss: 0.0007287176791578531\n",
            "  batch 30 loss: 0.0014308330602943898\n",
            "  batch 40 loss: 0.0006543020252138376\n",
            "  batch 50 loss: 0.0007004777900874615\n",
            "  batch 60 loss: 0.0013011040166020394\n",
            "  batch 70 loss: 0.00048022689297795297\n",
            "  batch 80 loss: 0.00045969365164637565\n",
            "  batch 90 loss: 0.0007641109637916088\n",
            "  batch 100 loss: 0.0011167379096150398\n",
            "  batch 110 loss: 0.0012533451430499555\n",
            "  batch 120 loss: 0.002193976193666458\n",
            "  batch 130 loss: 0.0011438308283686638\n",
            "  batch 140 loss: 0.0008561991155147553\n",
            "  batch 150 loss: 0.0005391940008848906\n",
            "  batch 160 loss: 0.0004204380325973034\n",
            "  batch 170 loss: 0.0024044493213295935\n",
            "  batch 180 loss: 0.000511446688324213\n",
            "  batch 190 loss: 0.0006685346830636263\n",
            "  batch 200 loss: 0.0005075464490801096\n",
            "  batch 210 loss: 0.0004030608106404543\n",
            "  batch 220 loss: 0.0005387060344219207\n",
            "  batch 230 loss: 0.0005398664623498916\n",
            "  batch 240 loss: 0.0010089054703712463\n",
            "  batch 250 loss: 0.00034469664096832276\n",
            "  batch 260 loss: 0.001293435413390398\n",
            "  batch 270 loss: 0.0005217575933784246\n",
            "  batch 280 loss: 0.0004832914099097252\n",
            "  batch 290 loss: 0.0023190055042505266\n",
            "  batch 300 loss: 0.001617380604147911\n",
            "  batch 310 loss: 0.0009834861382842063\n",
            "  batch 320 loss: 0.001333465799689293\n",
            "  batch 330 loss: 0.0004464453551918268\n",
            "  batch 340 loss: 0.0005276038311421872\n",
            "  batch 350 loss: 0.0005662109702825546\n",
            "  batch 360 loss: 0.0004535970278084278\n",
            "  batch 370 loss: 0.001555840391665697\n",
            "  batch 380 loss: 0.0004752540960907936\n",
            "  batch 390 loss: 0.0010458460077643395\n",
            "  batch 400 loss: 0.0010601564310491085\n",
            "  batch 410 loss: 0.00037992121651768683\n",
            "  batch 420 loss: 0.000998248439282179\n",
            "  batch 430 loss: 0.0004630901850759983\n",
            "LOSS train 0.0004630901850759983 valid 3.1632287617128014\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.12991538047790527\n",
            "  batch 20 loss: 0.11959309577941894\n",
            "  batch 30 loss: 0.10955435037612915\n",
            "  batch 40 loss: 0.10277023315429687\n",
            "  batch 50 loss: 0.09577940106391906\n",
            "  batch 60 loss: 0.09141274690628051\n",
            "  batch 70 loss: 0.08385894298553467\n",
            "  batch 80 loss: 0.0742564857006073\n",
            "  batch 90 loss: 0.07002716064453125\n",
            "  batch 100 loss: 0.06261849403381348\n",
            "  batch 110 loss: 0.061011725664138795\n",
            "  batch 120 loss: 0.05962082147598267\n",
            "  batch 130 loss: 0.04933561682701111\n",
            "  batch 140 loss: 0.04206778407096863\n",
            "  batch 150 loss: 0.03983474671840668\n",
            "  batch 160 loss: 0.04189417064189911\n",
            "  batch 170 loss: 0.031982052326202395\n",
            "  batch 180 loss: 0.029202738404273988\n",
            "  batch 190 loss: 0.02283981740474701\n",
            "  batch 200 loss: 0.02517804503440857\n",
            "  batch 210 loss: 0.02363346815109253\n",
            "  batch 220 loss: 0.019750261306762697\n",
            "  batch 230 loss: 0.02035679817199707\n",
            "  batch 240 loss: 0.015212640166282654\n",
            "  batch 250 loss: 0.014569959044456482\n",
            "  batch 260 loss: 0.013319379091262818\n",
            "  batch 270 loss: 0.012257585674524308\n",
            "  batch 280 loss: 0.01128690168261528\n",
            "  batch 290 loss: 0.011111976206302642\n",
            "  batch 300 loss: 0.010031601041555404\n",
            "  batch 310 loss: 0.00947490707039833\n",
            "  batch 320 loss: 0.009361179918050766\n",
            "  batch 330 loss: 0.007505980879068374\n",
            "  batch 340 loss: 0.007475055754184723\n",
            "  batch 350 loss: 0.006031514331698418\n",
            "  batch 360 loss: 0.008249041438102723\n",
            "  batch 370 loss: 0.006081961095333099\n",
            "  batch 380 loss: 0.006726362556219101\n",
            "  batch 390 loss: 0.007632404565811157\n",
            "  batch 400 loss: 0.005790121853351593\n",
            "  batch 410 loss: 0.0037826459854841234\n",
            "  batch 420 loss: 0.005071474239230156\n",
            "  batch 430 loss: 0.0052083820104598996\n",
            "LOSS train 0.0052083820104598996 valid 0.9888452569949845\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.004140288382768631\n",
            "  batch 20 loss: 0.0034149758517742156\n",
            "  batch 30 loss: 0.0069088608026504515\n",
            "  batch 40 loss: 0.003524869680404663\n",
            "  batch 50 loss: 0.004470772296190262\n",
            "  batch 60 loss: 0.00472128726541996\n",
            "  batch 70 loss: 0.004612737149000168\n",
            "  batch 80 loss: 0.004022528231143951\n",
            "  batch 90 loss: 0.0035348836332559585\n",
            "  batch 100 loss: 0.0028419695794582368\n",
            "  batch 110 loss: 0.003420374542474747\n",
            "  batch 120 loss: 0.0017530526965856552\n",
            "  batch 130 loss: 0.003663509339094162\n",
            "  batch 140 loss: 0.003354089707136154\n",
            "  batch 150 loss: 0.002543625608086586\n",
            "  batch 160 loss: 0.0027096990495920183\n",
            "  batch 170 loss: 0.0021078899502754212\n",
            "  batch 180 loss: 0.001573401503264904\n",
            "  batch 190 loss: 0.0019320804625749589\n",
            "  batch 200 loss: 0.0026378391310572626\n",
            "  batch 210 loss: 0.0018899617716670036\n",
            "  batch 220 loss: 0.0013798660598695277\n",
            "  batch 230 loss: 0.0017041396349668503\n",
            "  batch 240 loss: 0.0016709007322788239\n",
            "  batch 250 loss: 0.001326604001224041\n",
            "  batch 260 loss: 0.0017668697983026505\n",
            "  batch 270 loss: 0.0023304926231503486\n",
            "  batch 280 loss: 0.001957840844988823\n",
            "  batch 290 loss: 0.0025146160274744033\n",
            "  batch 300 loss: 0.0011926061473786832\n",
            "  batch 310 loss: 0.0018922686576843262\n",
            "  batch 320 loss: 0.0012694915756583213\n",
            "  batch 330 loss: 0.0016221735626459123\n",
            "  batch 340 loss: 0.0017878806218504905\n",
            "  batch 350 loss: 0.002259884960949421\n",
            "  batch 360 loss: 0.000981205888092518\n",
            "  batch 370 loss: 0.0011070644482970237\n",
            "  batch 380 loss: 0.0010766585357487202\n",
            "  batch 390 loss: 0.0006670836359262466\n",
            "  batch 400 loss: 0.0007527694571763277\n",
            "  batch 410 loss: 0.001139157824218273\n",
            "  batch 420 loss: 0.0009361794218420982\n",
            "  batch 430 loss: 0.0010990817099809647\n",
            "LOSS train 0.0010990817099809647 valid 1.123204437894619\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.0009099779650568962\n",
            "  batch 20 loss: 0.000709215272217989\n",
            "  batch 30 loss: 0.0009728869423270226\n",
            "  batch 40 loss: 0.000713732186704874\n",
            "  batch 50 loss: 0.0008308140560984612\n",
            "  batch 60 loss: 0.0009337132796645164\n",
            "  batch 70 loss: 0.0011688890866935254\n",
            "  batch 80 loss: 0.0005203342996537685\n",
            "  batch 90 loss: 0.0008958455175161362\n",
            "  batch 100 loss: 0.0014411824755370616\n",
            "  batch 110 loss: 0.0006620405241847038\n",
            "  batch 120 loss: 0.0005710181314498187\n",
            "  batch 130 loss: 0.0008896470069885253\n",
            "  batch 140 loss: 0.0011427033692598342\n",
            "  batch 150 loss: 0.0009573860093951225\n",
            "  batch 160 loss: 0.0005594858434051275\n",
            "  batch 170 loss: 0.0005480904132127762\n",
            "  batch 180 loss: 0.00046178349293768406\n",
            "  batch 190 loss: 0.0009665749035775662\n",
            "  batch 200 loss: 0.0010165439918637276\n",
            "  batch 210 loss: 0.0005245111882686615\n",
            "  batch 220 loss: 0.0004981503821909428\n",
            "  batch 230 loss: 0.0006024491973221302\n",
            "  batch 240 loss: 0.0006077633239328861\n",
            "  batch 250 loss: 0.0007672058418393135\n",
            "  batch 260 loss: 0.0006318984087556601\n",
            "  batch 270 loss: 0.000832592137157917\n",
            "  batch 280 loss: 0.0005792519077658653\n",
            "  batch 290 loss: 0.0005033203400671482\n",
            "  batch 300 loss: 0.001341261062771082\n",
            "  batch 310 loss: 0.00037868828512728215\n",
            "  batch 320 loss: 0.0008772276341915131\n",
            "  batch 330 loss: 0.0003370389109477401\n",
            "  batch 340 loss: 0.0017510129138827324\n",
            "  batch 350 loss: 0.0024517063051462174\n",
            "  batch 360 loss: 0.0005159248132258654\n",
            "  batch 370 loss: 0.0005156608298420906\n",
            "  batch 380 loss: 0.00033256851602345703\n",
            "  batch 390 loss: 0.0019671790301799773\n",
            "  batch 400 loss: 0.00042822137475013734\n",
            "  batch 410 loss: 0.003435124084353447\n",
            "  batch 420 loss: 0.0018122725188732148\n",
            "  batch 430 loss: 0.0007885643281042576\n",
            "LOSS train 0.0007885643281042576 valid 0.6886396841783944\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.13459389209747313\n",
            "  batch 20 loss: 0.1315768003463745\n",
            "  batch 30 loss: 0.1268819808959961\n",
            "  batch 40 loss: 0.12324202060699463\n",
            "  batch 50 loss: 0.1162622332572937\n",
            "  batch 60 loss: 0.11119544506072998\n",
            "  batch 70 loss: 0.10478966236114502\n",
            "  batch 80 loss: 0.10208430290222167\n",
            "  batch 90 loss: 0.09232428073883056\n",
            "  batch 100 loss: 0.09249053001403809\n",
            "  batch 110 loss: 0.08168237209320069\n",
            "  batch 120 loss: 0.07438710331916809\n",
            "  batch 130 loss: 0.06859580278396607\n",
            "  batch 140 loss: 0.06860019564628601\n",
            "  batch 150 loss: 0.052870643138885495\n",
            "  batch 160 loss: 0.04661098718643188\n",
            "  batch 170 loss: 0.043478524684906004\n",
            "  batch 180 loss: 0.039823830127716064\n",
            "  batch 190 loss: 0.03623424768447876\n",
            "  batch 200 loss: 0.03260712623596192\n",
            "  batch 210 loss: 0.030446287989616395\n",
            "  batch 220 loss: 0.025556308031082154\n",
            "  batch 230 loss: 0.02631073594093323\n",
            "  batch 240 loss: 0.02351522594690323\n",
            "  batch 250 loss: 0.018967069685459137\n",
            "  batch 260 loss: 0.021974049508571625\n",
            "  batch 270 loss: 0.01633569598197937\n",
            "  batch 280 loss: 0.018022677302360533\n",
            "  batch 290 loss: 0.014701226353645324\n",
            "  batch 300 loss: 0.012440688908100128\n",
            "  batch 310 loss: 0.012539967894554138\n",
            "  batch 320 loss: 0.010847830772399902\n",
            "  batch 330 loss: 0.01145886704325676\n",
            "  batch 340 loss: 0.011339230835437775\n",
            "  batch 350 loss: 0.01174234002828598\n",
            "  batch 360 loss: 0.00960690677165985\n",
            "  batch 370 loss: 0.010124507546424865\n",
            "  batch 380 loss: 0.009402737021446228\n",
            "  batch 390 loss: 0.008653488755226136\n",
            "  batch 400 loss: 0.006490403413772583\n",
            "  batch 410 loss: 0.006412135064601898\n",
            "  batch 420 loss: 0.005933554470539093\n",
            "  batch 430 loss: 0.005609690025448799\n",
            "LOSS train 0.005609690025448799 valid 1.2367524709042212\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.006396496295928955\n",
            "  batch 20 loss: 0.004852693155407905\n",
            "  batch 30 loss: 0.004624907299876213\n",
            "  batch 40 loss: 0.010628791153430938\n",
            "  batch 50 loss: 0.005517547577619552\n",
            "  batch 60 loss: 0.004910917207598686\n",
            "  batch 70 loss: 0.00530802495777607\n",
            "  batch 80 loss: 0.00820322185754776\n",
            "  batch 90 loss: 0.005728980898857117\n",
            "  batch 100 loss: 0.004764522612094879\n",
            "  batch 110 loss: 0.004984123632311821\n",
            "  batch 120 loss: 0.003443068265914917\n",
            "  batch 130 loss: 0.005446683615446091\n",
            "  batch 140 loss: 0.0043568454682827\n",
            "  batch 150 loss: 0.002813985012471676\n",
            "  batch 160 loss: 0.00302521213889122\n",
            "  batch 170 loss: 0.0030181735754013063\n",
            "  batch 180 loss: 0.0026922501623630525\n",
            "  batch 190 loss: 0.0026658140122890472\n",
            "  batch 200 loss: 0.0033151425421237947\n",
            "  batch 210 loss: 0.0032992612570524214\n",
            "  batch 220 loss: 0.0032871220260858535\n",
            "  batch 230 loss: 0.0022000100463628767\n",
            "  batch 240 loss: 0.001606358215212822\n",
            "  batch 250 loss: 0.0022833187133073805\n",
            "  batch 260 loss: 0.002305421605706215\n",
            "  batch 270 loss: 0.0029038898646831512\n",
            "  batch 280 loss: 0.004694056510925293\n",
            "  batch 290 loss: 0.003659817203879356\n",
            "  batch 300 loss: 0.0016690433025360108\n",
            "  batch 310 loss: 0.0019968245178461075\n",
            "  batch 320 loss: 0.002092060074210167\n",
            "  batch 330 loss: 0.001975470595061779\n",
            "  batch 340 loss: 0.0016530925408005715\n",
            "  batch 350 loss: 0.0013027429580688477\n",
            "  batch 360 loss: 0.0013195956125855446\n",
            "  batch 370 loss: 0.0017266318202018738\n",
            "  batch 380 loss: 0.002515757828950882\n",
            "  batch 390 loss: 0.0011856650933623313\n",
            "  batch 400 loss: 0.00204942487180233\n",
            "  batch 410 loss: 0.0023147907108068467\n",
            "  batch 420 loss: 0.0014445985667407512\n",
            "  batch 430 loss: 0.0011682625859975814\n",
            "LOSS train 0.0011682625859975814 valid 1.6024003708352372\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.0010091204196214676\n",
            "  batch 20 loss: 0.002248985879123211\n",
            "  batch 30 loss: 0.0019970199093222616\n",
            "  batch 40 loss: 0.0008845623582601547\n",
            "  batch 50 loss: 0.0016441900283098222\n",
            "  batch 60 loss: 0.0009369628503918647\n",
            "  batch 70 loss: 0.0010128865018486977\n",
            "  batch 80 loss: 0.0008873587474226951\n",
            "  batch 90 loss: 0.000816243514418602\n",
            "  batch 100 loss: 0.0029377587139606475\n",
            "  batch 110 loss: 0.0008133349940180779\n",
            "  batch 120 loss: 0.0007865587249398232\n",
            "  batch 130 loss: 0.0006614089943468571\n",
            "  batch 140 loss: 0.0006123867817223072\n",
            "  batch 150 loss: 0.0006164583377540111\n",
            "  batch 160 loss: 0.0009484411217272282\n",
            "  batch 170 loss: 0.0008562734350562095\n",
            "  batch 180 loss: 0.0006089454516768455\n",
            "  batch 190 loss: 0.0020350713282823564\n",
            "  batch 200 loss: 0.002054980769753456\n",
            "  batch 210 loss: 0.0008296419866383076\n",
            "  batch 220 loss: 0.0013594815507531166\n",
            "  batch 230 loss: 0.0005032467655837536\n",
            "  batch 240 loss: 0.0007477182894945145\n",
            "  batch 250 loss: 0.0011394524946808815\n",
            "  batch 260 loss: 0.0012499830685555936\n",
            "  batch 270 loss: 0.0011286757886409759\n",
            "  batch 280 loss: 0.0004512652289122343\n",
            "  batch 290 loss: 0.001358463242650032\n",
            "  batch 300 loss: 0.0005616629496216774\n",
            "  batch 310 loss: 0.0005970161408185958\n",
            "  batch 320 loss: 0.00033983504399657247\n",
            "  batch 330 loss: 0.0005766368471086025\n",
            "  batch 340 loss: 0.0006659197621047497\n",
            "  batch 350 loss: 0.0005545182153582573\n",
            "  batch 360 loss: 0.003940312564373017\n",
            "  batch 370 loss: 0.0009786895476281642\n",
            "  batch 380 loss: 0.0004416906274855137\n",
            "  batch 390 loss: 0.000666196970269084\n",
            "  batch 400 loss: 0.0004096138756722212\n",
            "  batch 410 loss: 0.0004649794660508633\n",
            "  batch 420 loss: 0.0004191844258457422\n",
            "  batch 430 loss: 0.0002730109030380845\n",
            "LOSS train 0.0002730109030380845 valid 1.1495057136101061\n",
            "{'res_blocks': 5, 'res_block_size': 2, 'input_channels': 248, 'downsample': 0.5}\n",
            "Accuracy: 0.7563892061679041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, best_intra_param, best_intra_accuracy = random_search(MEGClassifier, 10, param_range, epochs=3, is_cross=False)\n",
        "print(best_intra_param) \n",
        "print(\"Accuracy:\", best_intra_accuracy)\n",
        "torch.save(best_model.state_dict(), \"best_intra_model\")"
      ],
      "metadata": {
        "id": "86BeSVJT9O-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0abb39a0-b744-4a53-fffd-3dfa43be773b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "  batch 10 loss: 0.13197420835494994\n",
            "  batch 20 loss: 0.12483580112457275\n",
            "  batch 30 loss: 0.12011351585388183\n",
            "  batch 40 loss: 0.11039061546325683\n",
            "  batch 50 loss: 0.1130754828453064\n",
            "  batch 60 loss: 0.09175769686698913\n",
            "  batch 70 loss: 0.08726482391357422\n",
            "  batch 80 loss: 0.08857428431510925\n",
            "  batch 90 loss: 0.07515971660614014\n",
            "  batch 100 loss: 0.08180712461471558\n",
            "  batch 110 loss: 0.07444244623184204\n",
            "  batch 120 loss: 0.06643311977386475\n",
            "  batch 130 loss: 0.06046581268310547\n",
            "  batch 140 loss: 0.05970296859741211\n",
            "  batch 150 loss: 0.049365836381912234\n",
            "  batch 160 loss: 0.05088521838188172\n",
            "  batch 170 loss: 0.04351428747177124\n",
            "  batch 180 loss: 0.04345833659172058\n",
            "  batch 190 loss: 0.05097985863685608\n",
            "  batch 200 loss: 0.040048837661743164\n",
            "  batch 210 loss: 0.040775781869888304\n",
            "  batch 220 loss: 0.03758789896965027\n",
            "  batch 230 loss: 0.03778159022331238\n",
            "  batch 240 loss: 0.03658191561698913\n",
            "  batch 250 loss: 0.026833939552307128\n",
            "  batch 260 loss: 0.026677852869033812\n",
            "  batch 270 loss: 0.02720634639263153\n",
            "  batch 280 loss: 0.02171587198972702\n",
            "  batch 290 loss: 0.02466176301240921\n",
            "  batch 300 loss: 0.02134593576192856\n",
            "  batch 310 loss: 0.01705077737569809\n",
            "  batch 320 loss: 0.015912778675556183\n",
            "  batch 330 loss: 0.015744677186012267\n",
            "  batch 340 loss: 0.019044607877731323\n",
            "  batch 350 loss: 0.013938984274864197\n",
            "  batch 360 loss: 0.0130130335688591\n",
            "  batch 370 loss: 0.015254040062427521\n",
            "  batch 380 loss: 0.014322575926780701\n",
            "  batch 390 loss: 0.00958106592297554\n",
            "  batch 400 loss: 0.01328871250152588\n",
            "  batch 410 loss: 0.012262187898159027\n",
            "  batch 420 loss: 0.010604798048734664\n",
            "  batch 430 loss: 0.009103954583406449\n",
            "LOSS train 0.009103954583406449 valid 0.16256607027555053\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.008973975479602814\n",
            "  batch 20 loss: 0.008004574477672577\n",
            "  batch 30 loss: 0.009013330936431885\n",
            "  batch 40 loss: 0.0076253868639469145\n",
            "  batch 50 loss: 0.006167884171009064\n",
            "  batch 60 loss: 0.008795168250799179\n",
            "  batch 70 loss: 0.007659713923931122\n",
            "  batch 80 loss: 0.008257415890693665\n",
            "  batch 90 loss: 0.0063119567930698395\n",
            "  batch 100 loss: 0.006574912369251252\n",
            "  batch 110 loss: 0.004809633269906044\n",
            "  batch 120 loss: 0.005369095504283905\n",
            "  batch 130 loss: 0.004286740720272064\n",
            "  batch 140 loss: 0.006712795794010162\n",
            "  batch 150 loss: 0.004398132115602494\n",
            "  batch 160 loss: 0.006475996226072311\n",
            "  batch 170 loss: 0.005583672970533371\n",
            "  batch 180 loss: 0.003432150185108185\n",
            "  batch 190 loss: 0.003370046243071556\n",
            "  batch 200 loss: 0.005071219056844711\n",
            "  batch 210 loss: 0.004487589001655579\n",
            "  batch 220 loss: 0.002845487929880619\n",
            "  batch 230 loss: 0.006159428507089615\n",
            "  batch 240 loss: 0.0027345865964889526\n",
            "  batch 250 loss: 0.00441306009888649\n",
            "  batch 260 loss: 0.002333434671163559\n",
            "  batch 270 loss: 0.0029868943616747855\n",
            "  batch 280 loss: 0.004130514711141587\n",
            "  batch 290 loss: 0.0039115406572818754\n",
            "  batch 300 loss: 0.003473559394478798\n",
            "  batch 310 loss: 0.0068892702460289\n",
            "  batch 320 loss: 0.0026384610682725906\n",
            "  batch 330 loss: 0.0021397365257143975\n",
            "  batch 340 loss: 0.0018438417464494705\n",
            "  batch 350 loss: 0.003140308707952499\n",
            "  batch 360 loss: 0.0023447712883353233\n",
            "  batch 370 loss: 0.001964833214879036\n",
            "  batch 380 loss: 0.003287556394934654\n",
            "  batch 390 loss: 0.003642360866069794\n",
            "  batch 400 loss: 0.0036677293479442596\n",
            "  batch 410 loss: 0.0023117536678910256\n",
            "  batch 420 loss: 0.0013908762484788895\n",
            "  batch 430 loss: 0.0027007069438695908\n",
            "LOSS train 0.0027007069438695908 valid 0.03736648618875982\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.002991330809891224\n",
            "  batch 20 loss: 0.0018149934709072112\n",
            "  batch 30 loss: 0.0031060315668582916\n",
            "  batch 40 loss: 0.0020448284223675726\n",
            "  batch 50 loss: 0.0029381163418293\n",
            "  batch 60 loss: 0.0026838334277272226\n",
            "  batch 70 loss: 0.0033412247896194456\n",
            "  batch 80 loss: 0.0014294455759227276\n",
            "  batch 90 loss: 0.0019301392138004303\n",
            "  batch 100 loss: 0.0013051427900791167\n",
            "  batch 110 loss: 0.001131479162722826\n",
            "  batch 120 loss: 0.000881518516689539\n",
            "  batch 130 loss: 0.0014300799928605556\n",
            "  batch 140 loss: 0.0031295306980609894\n",
            "  batch 150 loss: 0.00335816964507103\n",
            "  batch 160 loss: 0.0013991987332701683\n",
            "  batch 170 loss: 0.0017326947301626206\n",
            "  batch 180 loss: 0.0011664710938930512\n",
            "  batch 190 loss: 0.002324557304382324\n",
            "  batch 200 loss: 0.0012921948917210102\n",
            "  batch 210 loss: 0.000607304647564888\n",
            "  batch 220 loss: 0.0015728399157524109\n",
            "  batch 230 loss: 0.0017992811277508735\n",
            "  batch 240 loss: 0.0021577980369329453\n",
            "  batch 250 loss: 0.0024200886487960815\n",
            "  batch 260 loss: 0.0011213243007659913\n",
            "  batch 270 loss: 0.0009744863957166672\n",
            "  batch 280 loss: 0.000853332132101059\n",
            "  batch 290 loss: 0.001078859157860279\n",
            "  batch 300 loss: 0.001579844206571579\n",
            "  batch 310 loss: 0.0016846653074026109\n",
            "  batch 320 loss: 0.0009167333133518696\n",
            "  batch 330 loss: 0.0012554771266877652\n",
            "  batch 340 loss: 0.002466350980103016\n",
            "  batch 350 loss: 0.0010112572461366653\n",
            "  batch 360 loss: 0.0007694448810070753\n",
            "  batch 370 loss: 0.0020901471376419067\n",
            "  batch 380 loss: 0.0018067806959152222\n",
            "  batch 390 loss: 0.000726579362526536\n",
            "  batch 400 loss: 0.0019064204767346381\n",
            "  batch 410 loss: 0.001399376429617405\n",
            "  batch 420 loss: 0.0012243324890732765\n",
            "  batch 430 loss: 0.002031244896352291\n",
            "LOSS train 0.002031244896352291 valid 0.010799827680536757\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.1316129446029663\n",
            "  batch 20 loss: 0.11436986923217773\n",
            "  batch 30 loss: 0.09916396141052246\n",
            "  batch 40 loss: 0.07864421606063843\n",
            "  batch 50 loss: 0.06372491121292115\n",
            "  batch 60 loss: 0.05433485507965088\n",
            "  batch 70 loss: 0.05692746639251709\n",
            "  batch 80 loss: 0.047965535521507265\n",
            "  batch 90 loss: 0.050340449810028075\n",
            "  batch 100 loss: 0.03948127031326294\n",
            "  batch 110 loss: 0.030241522192955016\n",
            "  batch 120 loss: 0.04006598591804504\n",
            "  batch 130 loss: 0.024206092953681944\n",
            "  batch 140 loss: 0.02818266153335571\n",
            "  batch 150 loss: 0.021394523978233337\n",
            "  batch 160 loss: 0.016899126768112182\n",
            "  batch 170 loss: 0.01368950605392456\n",
            "  batch 180 loss: 0.013993585109710693\n",
            "  batch 190 loss: 0.011764266341924668\n",
            "  batch 200 loss: 0.007829317450523376\n",
            "  batch 210 loss: 0.010306037962436676\n",
            "  batch 220 loss: 0.009432418644428254\n",
            "  batch 230 loss: 0.007570080459117889\n",
            "  batch 240 loss: 0.007503046095371247\n",
            "  batch 250 loss: 0.006572635471820831\n",
            "  batch 260 loss: 0.005259308218955994\n",
            "  batch 270 loss: 0.005154684558510781\n",
            "  batch 280 loss: 0.010581161826848984\n",
            "  batch 290 loss: 0.003951617330312729\n",
            "  batch 300 loss: 0.006586244702339173\n",
            "  batch 310 loss: 0.0035164650529623032\n",
            "  batch 320 loss: 0.0032111901789903642\n",
            "  batch 330 loss: 0.003617478907108307\n",
            "  batch 340 loss: 0.0015346286818385124\n",
            "  batch 350 loss: 0.005153613910079002\n",
            "  batch 360 loss: 0.006349019706249237\n",
            "  batch 370 loss: 0.00653659924864769\n",
            "  batch 380 loss: 0.003003297373652458\n",
            "  batch 390 loss: 0.0031847160309553145\n",
            "  batch 400 loss: 0.0040525846183300015\n",
            "  batch 410 loss: 0.004225234687328339\n",
            "  batch 420 loss: 0.0035424105823040008\n",
            "  batch 430 loss: 0.001767030730843544\n",
            "LOSS train 0.001767030730843544 valid 0.08095397035160039\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.000990755297243595\n",
            "  batch 20 loss: 0.004985155910253525\n",
            "  batch 30 loss: 0.002711101248860359\n",
            "  batch 40 loss: 0.002553824707865715\n",
            "  batch 50 loss: 0.003312370926141739\n",
            "  batch 60 loss: 0.0035407375544309616\n",
            "  batch 70 loss: 0.001838843710720539\n",
            "  batch 80 loss: 0.0018408536911010743\n",
            "  batch 90 loss: 0.001038273423910141\n",
            "  batch 100 loss: 0.004434681683778763\n",
            "  batch 110 loss: 0.002063126862049103\n",
            "  batch 120 loss: 0.004366177320480347\n",
            "  batch 130 loss: 0.004366303235292435\n",
            "  batch 140 loss: 0.003280904144048691\n",
            "  batch 150 loss: 0.004141270741820335\n",
            "  batch 160 loss: 0.0020696934312582015\n",
            "  batch 170 loss: 0.0023468634113669395\n",
            "  batch 180 loss: 0.0012847745791077613\n",
            "  batch 190 loss: 0.001245839148759842\n",
            "  batch 200 loss: 0.002701805904507637\n",
            "  batch 210 loss: 0.0032225869596004485\n",
            "  batch 220 loss: 0.0014887122437357903\n",
            "  batch 230 loss: 0.0015930067747831345\n",
            "  batch 240 loss: 0.0013899507001042366\n",
            "  batch 250 loss: 0.0026824664324522018\n",
            "  batch 260 loss: 0.002110271342098713\n",
            "  batch 270 loss: 0.0014274658635258675\n",
            "  batch 280 loss: 0.0012006214819848537\n",
            "  batch 290 loss: 0.002665899693965912\n",
            "  batch 300 loss: 0.0017285890877246856\n",
            "  batch 310 loss: 0.0021660853177309035\n",
            "  batch 320 loss: 0.0015301797538995743\n",
            "  batch 330 loss: 0.004268323630094528\n",
            "  batch 340 loss: 0.0020589496940374374\n",
            "  batch 350 loss: 0.0015486113727092743\n",
            "  batch 360 loss: 0.0041394814848899845\n",
            "  batch 370 loss: 0.0007447839714586734\n",
            "  batch 380 loss: 0.002282232791185379\n",
            "  batch 390 loss: 0.0013221593573689461\n",
            "  batch 400 loss: 0.0015096820890903473\n",
            "  batch 410 loss: 0.0017085708677768707\n",
            "  batch 420 loss: 0.0024044040590524675\n",
            "  batch 430 loss: 0.0035831131041049956\n",
            "LOSS train 0.0035831131041049956 valid 0.08207480334906657\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.0026213541626930238\n",
            "  batch 20 loss: 0.0010790434665977956\n",
            "  batch 30 loss: 0.002450237050652504\n",
            "  batch 40 loss: 0.004510429874062538\n",
            "  batch 50 loss: 0.001187029480934143\n",
            "  batch 60 loss: 0.0013624338433146476\n",
            "  batch 70 loss: 0.0018059955909848213\n",
            "  batch 80 loss: 0.002227071300148964\n",
            "  batch 90 loss: 0.0005002828314900398\n",
            "  batch 100 loss: 0.0033229809254407884\n",
            "  batch 110 loss: 0.0017191728577017785\n",
            "  batch 120 loss: 0.0064591608941555025\n",
            "  batch 130 loss: 0.0003311093430966139\n",
            "  batch 140 loss: 0.0013403741642832756\n",
            "  batch 150 loss: 0.002152465097606182\n",
            "  batch 160 loss: 0.0005278806667774916\n",
            "  batch 170 loss: 0.0008299585431814193\n",
            "  batch 180 loss: 0.002665647864341736\n",
            "  batch 190 loss: 0.0011515814810991287\n",
            "  batch 200 loss: 0.0009270135313272476\n",
            "  batch 210 loss: 0.0018345072865486145\n",
            "  batch 220 loss: 0.0058295346796512605\n",
            "  batch 230 loss: 0.002942970581352711\n",
            "  batch 240 loss: 0.004243750125169754\n",
            "  batch 250 loss: 0.0027541274204850195\n",
            "  batch 260 loss: 0.0008652649819850921\n",
            "  batch 270 loss: 0.0008080702275037766\n",
            "  batch 280 loss: 0.0003987080417573452\n",
            "  batch 290 loss: 0.0010433604009449482\n",
            "  batch 300 loss: 0.0015675906091928483\n",
            "  batch 310 loss: 0.002567325904965401\n",
            "  batch 320 loss: 0.0021071948111057283\n",
            "  batch 330 loss: 0.0012621890753507614\n",
            "  batch 340 loss: 0.000415778299793601\n",
            "  batch 350 loss: 0.002299828268587589\n",
            "  batch 360 loss: 0.003080381453037262\n",
            "  batch 370 loss: 0.001005893386900425\n",
            "  batch 380 loss: 0.00040144631639122964\n",
            "  batch 390 loss: 0.0008971594274044037\n",
            "  batch 400 loss: 0.001282605342566967\n",
            "  batch 410 loss: 0.0036825437098741533\n",
            "  batch 420 loss: 0.0003717479761689901\n",
            "  batch 430 loss: 0.0027441270649433136\n",
            "LOSS train 0.0027441270649433136 valid 0.06409303520598787\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.11065244674682617\n",
            "  batch 20 loss: 0.08076392412185669\n",
            "  batch 30 loss: 0.05985761284828186\n",
            "  batch 40 loss: 0.06551480293273926\n",
            "  batch 50 loss: 0.060986649990081784\n",
            "  batch 60 loss: 0.04866117835044861\n",
            "  batch 70 loss: 0.04681285619735718\n",
            "  batch 80 loss: 0.03156604468822479\n",
            "  batch 90 loss: 0.0248830646276474\n",
            "  batch 100 loss: 0.0216979444026947\n",
            "  batch 110 loss: 0.016482514142990113\n",
            "  batch 120 loss: 0.013462385535240174\n",
            "  batch 130 loss: 0.01170697808265686\n",
            "  batch 140 loss: 0.008392640203237534\n",
            "  batch 150 loss: 0.008846093714237214\n",
            "  batch 160 loss: 0.008099100738763809\n",
            "  batch 170 loss: 0.0065173134207725525\n",
            "  batch 180 loss: 0.007135368883609772\n",
            "  batch 190 loss: 0.004585796594619751\n",
            "  batch 200 loss: 0.0051061086356639866\n",
            "  batch 210 loss: 0.007512697577476501\n",
            "  batch 220 loss: 0.00386519655585289\n",
            "  batch 230 loss: 0.003840725123882294\n",
            "  batch 240 loss: 0.007497966289520264\n",
            "  batch 250 loss: 0.003928374499082565\n",
            "  batch 260 loss: 0.003840877115726471\n",
            "  batch 270 loss: 0.0030133746564388274\n",
            "  batch 280 loss: 0.0032540619373321532\n",
            "  batch 290 loss: 0.00488244965672493\n",
            "  batch 300 loss: 0.0015272987075150013\n",
            "  batch 310 loss: 0.001575491949915886\n",
            "  batch 320 loss: 0.0034320272505283357\n",
            "  batch 330 loss: 0.0017824538052082061\n",
            "  batch 340 loss: 0.0019270824268460274\n",
            "  batch 350 loss: 0.002871689945459366\n",
            "  batch 360 loss: 0.0015016945078969001\n",
            "  batch 370 loss: 0.005624376982450485\n",
            "  batch 380 loss: 0.0023830607533454895\n",
            "  batch 390 loss: 0.0014476903714239597\n",
            "  batch 400 loss: 0.0019935056567192077\n",
            "  batch 410 loss: 0.002990669384598732\n",
            "  batch 420 loss: 0.0010772247798740864\n",
            "  batch 430 loss: 0.002064869739115238\n",
            "LOSS train 0.002064869739115238 valid 2.1895700147597053\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.0007342638913542032\n",
            "  batch 20 loss: 0.001574927568435669\n",
            "  batch 30 loss: 0.003638869524002075\n",
            "  batch 40 loss: 0.003635546937584877\n",
            "  batch 50 loss: 0.003114071674644947\n",
            "  batch 60 loss: 0.0019223621115088464\n",
            "  batch 70 loss: 0.0005333047825843096\n",
            "  batch 80 loss: 0.0016949204728007317\n",
            "  batch 90 loss: 0.000503440573811531\n",
            "  batch 100 loss: 0.0036654982715845106\n",
            "  batch 110 loss: 0.0016209064051508904\n",
            "  batch 120 loss: 0.0003645914141088724\n",
            "  batch 130 loss: 0.0005735548213124276\n",
            "  batch 140 loss: 0.0014463289640843867\n",
            "  batch 150 loss: 0.0010726958513259887\n",
            "  batch 160 loss: 0.0010904794558882712\n",
            "  batch 170 loss: 0.0020259542390704153\n",
            "  batch 180 loss: 0.0020103152841329575\n",
            "  batch 190 loss: 0.0003361830022186041\n",
            "  batch 200 loss: 0.0015043657273054122\n",
            "  batch 210 loss: 0.0004789445083588362\n",
            "  batch 220 loss: 0.001108325459063053\n",
            "  batch 230 loss: 0.0013361937366425992\n",
            "  batch 240 loss: 0.0004254237748682499\n",
            "  batch 250 loss: 0.0012323416769504548\n",
            "  batch 260 loss: 0.00028822298627346755\n",
            "  batch 270 loss: 0.0014183077961206435\n",
            "  batch 280 loss: 0.0003447291674092412\n",
            "  batch 290 loss: 0.0009766090661287308\n",
            "  batch 300 loss: 0.000275363540276885\n",
            "  batch 310 loss: 0.0015806617215275765\n",
            "  batch 320 loss: 0.0004077141173183918\n",
            "  batch 330 loss: 0.0007229866925626994\n",
            "  batch 340 loss: 0.0005929604172706604\n",
            "  batch 350 loss: 0.0001662775408476591\n",
            "  batch 360 loss: 0.0013599918223917485\n",
            "  batch 370 loss: 0.00019423007033765317\n",
            "  batch 380 loss: 0.0001490766182541847\n",
            "  batch 390 loss: 0.0009163230657577515\n",
            "  batch 400 loss: 0.0006644182838499546\n",
            "  batch 410 loss: 0.00022448287345468997\n",
            "  batch 420 loss: 0.0002050043549388647\n",
            "  batch 430 loss: 0.0005639921873807907\n",
            "LOSS train 0.0005639921873807907 valid 0.009969031302121453\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.0004149992950260639\n",
            "  batch 20 loss: 0.00020226400811225177\n",
            "  batch 30 loss: 0.00034436292480677366\n",
            "  batch 40 loss: 0.000728498212993145\n",
            "  batch 50 loss: 0.0005170188378542662\n",
            "  batch 60 loss: 0.00025400444865226747\n",
            "  batch 70 loss: 0.001670847274363041\n",
            "  batch 80 loss: 0.0002793451305478811\n",
            "  batch 90 loss: 0.0014440055005252362\n",
            "  batch 100 loss: 0.0029725514352321623\n",
            "  batch 110 loss: 0.0012010135687887669\n",
            "  batch 120 loss: 0.001796170324087143\n",
            "  batch 130 loss: 0.00024726088158786295\n",
            "  batch 140 loss: 0.0005051047075539828\n",
            "  batch 150 loss: 0.0008683672174811363\n",
            "  batch 160 loss: 0.0007053891196846962\n",
            "  batch 170 loss: 0.002456469647586346\n",
            "  batch 180 loss: 0.00022515635937452315\n",
            "  batch 190 loss: 0.0006983517203480005\n",
            "  batch 200 loss: 0.0004696451127529144\n",
            "  batch 210 loss: 0.0002486132085323334\n",
            "  batch 220 loss: 0.00046896599233150484\n",
            "  batch 230 loss: 0.00031549958512187\n",
            "  batch 240 loss: 0.00012012264924123883\n",
            "  batch 250 loss: 0.00107010155916214\n",
            "  batch 260 loss: 9.267874993383885e-05\n",
            "  batch 270 loss: 0.00010819798335433006\n",
            "  batch 280 loss: 0.0002934528049081564\n",
            "  batch 290 loss: 0.0010868368670344352\n",
            "  batch 300 loss: 0.00053715780377388\n",
            "  batch 310 loss: 0.0009747723117470742\n",
            "  batch 320 loss: 0.00028134770691394805\n",
            "  batch 330 loss: 0.0006129664368927478\n",
            "  batch 340 loss: 0.00011366077233105898\n",
            "  batch 350 loss: 0.00012208251282572747\n",
            "  batch 360 loss: 0.0014059926383197308\n",
            "  batch 370 loss: 0.0002555194310843945\n",
            "  batch 380 loss: 0.0010788511484861375\n",
            "  batch 390 loss: 0.0003550720866769552\n",
            "  batch 400 loss: 4.9379654228687286e-05\n",
            "  batch 410 loss: 6.0879532247781755e-05\n",
            "  batch 420 loss: 0.0008568991906940937\n",
            "  batch 430 loss: 0.00017029233276844025\n",
            "LOSS train 0.00017029233276844025 valid 0.09659151955785732\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.13233613967895508\n",
            "  batch 20 loss: 0.12544249296188353\n",
            "  batch 30 loss: 0.11835458278656005\n",
            "  batch 40 loss: 0.10883262157440185\n",
            "  batch 50 loss: 0.09815913438796997\n",
            "  batch 60 loss: 0.09078566431999206\n",
            "  batch 70 loss: 0.0865042746067047\n",
            "  batch 80 loss: 0.07818728685379028\n",
            "  batch 90 loss: 0.07366814017295838\n",
            "  batch 100 loss: 0.07886587381362915\n",
            "  batch 110 loss: 0.07048304080963134\n",
            "  batch 120 loss: 0.06745364665985107\n",
            "  batch 130 loss: 0.058780992031097413\n",
            "  batch 140 loss: 0.05850159525871277\n",
            "  batch 150 loss: 0.05570906400680542\n",
            "  batch 160 loss: 0.054356008768081665\n",
            "  batch 170 loss: 0.05128251910209656\n",
            "  batch 180 loss: 0.04803553223609924\n",
            "  batch 190 loss: 0.04264618158340454\n",
            "  batch 200 loss: 0.03999441564083099\n",
            "  batch 210 loss: 0.04208508729934692\n",
            "  batch 220 loss: 0.043164363503456114\n",
            "  batch 230 loss: 0.04133270084857941\n",
            "  batch 240 loss: 0.03229226768016815\n",
            "  batch 250 loss: 0.033050379157066344\n",
            "  batch 260 loss: 0.03219282627105713\n",
            "  batch 270 loss: 0.02835448980331421\n",
            "  batch 280 loss: 0.024948994815349578\n",
            "  batch 290 loss: 0.02147383689880371\n",
            "  batch 300 loss: 0.02714027762413025\n",
            "  batch 310 loss: 0.020507001876831056\n",
            "  batch 320 loss: 0.019769304990768434\n",
            "  batch 330 loss: 0.0159379243850708\n",
            "  batch 340 loss: 0.016871845722198485\n",
            "  batch 350 loss: 0.017025236785411835\n",
            "  batch 360 loss: 0.01523130089044571\n",
            "  batch 370 loss: 0.014995315670967102\n",
            "  batch 380 loss: 0.014141449332237243\n",
            "  batch 390 loss: 0.009930619597434997\n",
            "  batch 400 loss: 0.010387207567691802\n",
            "  batch 410 loss: 0.010518990457057953\n",
            "  batch 420 loss: 0.011765886843204499\n",
            "  batch 430 loss: 0.009189751744270325\n",
            "LOSS train 0.009189751744270325 valid 0.1718236795541915\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.010394741594791413\n",
            "  batch 20 loss: 0.0075141675770282745\n",
            "  batch 30 loss: 0.00641406923532486\n",
            "  batch 40 loss: 0.006167794764041901\n",
            "  batch 50 loss: 0.006092678755521774\n",
            "  batch 60 loss: 0.006184908747673035\n",
            "  batch 70 loss: 0.0066249057650566105\n",
            "  batch 80 loss: 0.006689867377281189\n",
            "  batch 90 loss: 0.004675813019275665\n",
            "  batch 100 loss: 0.005112700909376144\n",
            "  batch 110 loss: 0.005634716898202896\n",
            "  batch 120 loss: 0.004158742725849152\n",
            "  batch 130 loss: 0.004981184005737304\n",
            "  batch 140 loss: 0.004388097673654556\n",
            "  batch 150 loss: 0.0040683291852474214\n",
            "  batch 160 loss: 0.00607449933886528\n",
            "  batch 170 loss: 0.004133705049753189\n",
            "  batch 180 loss: 0.002815023995935917\n",
            "  batch 190 loss: 0.003974613919854164\n",
            "  batch 200 loss: 0.003408215567469597\n",
            "  batch 210 loss: 0.003085538372397423\n",
            "  batch 220 loss: 0.003762085735797882\n",
            "  batch 230 loss: 0.003889388591051102\n",
            "  batch 240 loss: 0.002334434539079666\n",
            "  batch 250 loss: 0.0021803049370646478\n",
            "  batch 260 loss: 0.004297607764601708\n",
            "  batch 270 loss: 0.004173198342323303\n",
            "  batch 280 loss: 0.0021177664399147033\n",
            "  batch 290 loss: 0.0018496766686439514\n",
            "  batch 300 loss: 0.0023460619151592253\n",
            "  batch 310 loss: 0.0021657364442944525\n",
            "  batch 320 loss: 0.002251630648970604\n",
            "  batch 330 loss: 0.0025944331660866736\n",
            "  batch 340 loss: 0.0018731411546468735\n",
            "  batch 350 loss: 0.0018158335238695144\n",
            "  batch 360 loss: 0.0038258291780948637\n",
            "  batch 370 loss: 0.0020792165771126745\n",
            "  batch 380 loss: 0.002084065228700638\n",
            "  batch 390 loss: 0.0025816434994339944\n",
            "  batch 400 loss: 0.0031820904463529588\n",
            "  batch 410 loss: 0.0015451897867023944\n",
            "  batch 420 loss: 0.0009927697479724883\n",
            "  batch 430 loss: 0.0012735280208289624\n",
            "LOSS train 0.0012735280208289624 valid 0.016927272937027737\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.001519635133445263\n",
            "  batch 20 loss: 0.001215348020195961\n",
            "  batch 30 loss: 0.0021605342626571655\n",
            "  batch 40 loss: 0.001605786569416523\n",
            "  batch 50 loss: 0.0022817611694335937\n",
            "  batch 60 loss: 0.0009454011917114258\n",
            "  batch 70 loss: 0.0008699181489646435\n",
            "  batch 80 loss: 0.0009954659268260001\n",
            "  batch 90 loss: 0.0011174260638654233\n",
            "  batch 100 loss: 0.0011812318116426468\n",
            "  batch 110 loss: 0.0008058888837695122\n",
            "  batch 120 loss: 0.0017050867900252342\n",
            "  batch 130 loss: 0.0023533761501312255\n",
            "  batch 140 loss: 0.0008324864320456982\n",
            "  batch 150 loss: 0.0008207287639379502\n",
            "  batch 160 loss: 0.0018173161894083022\n",
            "  batch 170 loss: 0.0008005128242075443\n",
            "  batch 180 loss: 0.0012094199657440186\n",
            "  batch 190 loss: 0.0006266494747251272\n",
            "  batch 200 loss: 0.001733168587088585\n",
            "  batch 210 loss: 0.0010784469544887544\n",
            "  batch 220 loss: 0.0008963829837739468\n",
            "  batch 230 loss: 0.0022333428263664246\n",
            "  batch 240 loss: 0.0010030658915638924\n",
            "  batch 250 loss: 0.003049227595329285\n",
            "  batch 260 loss: 0.0004592837765812874\n",
            "  batch 270 loss: 0.0005885274149477482\n",
            "  batch 280 loss: 0.0005374819040298462\n",
            "  batch 290 loss: 0.0007572874892503023\n",
            "  batch 300 loss: 0.0006047888658940792\n",
            "  batch 310 loss: 0.0009402277879416943\n",
            "  batch 320 loss: 0.0006442741490900516\n",
            "  batch 330 loss: 0.0014198333024978637\n",
            "  batch 340 loss: 0.0014722035266458988\n",
            "  batch 350 loss: 0.00044235605746507646\n",
            "  batch 360 loss: 0.0008093600161373615\n",
            "  batch 370 loss: 0.0011979954317212105\n",
            "  batch 380 loss: 0.00044042491354048254\n",
            "  batch 390 loss: 0.0017090249806642533\n",
            "  batch 400 loss: 0.001241290383040905\n",
            "  batch 410 loss: 0.0005801422521471977\n",
            "  batch 420 loss: 0.0005119386594742536\n",
            "  batch 430 loss: 0.000642044935375452\n",
            "LOSS train 0.000642044935375452 valid 0.034128393835536286\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.1250054955482483\n",
            "  batch 20 loss: 0.09389370679855347\n",
            "  batch 30 loss: 0.08763829469680787\n",
            "  batch 40 loss: 0.07770302295684814\n",
            "  batch 50 loss: 0.06187758445739746\n",
            "  batch 60 loss: 0.05655232071876526\n",
            "  batch 70 loss: 0.06276955604553222\n",
            "  batch 80 loss: 0.050207477807998654\n",
            "  batch 90 loss: 0.038347676396369934\n",
            "  batch 100 loss: 0.03547084331512451\n",
            "  batch 110 loss: 0.0330114334821701\n",
            "  batch 120 loss: 0.026207628846168517\n",
            "  batch 130 loss: 0.020494773983955383\n",
            "  batch 140 loss: 0.028597503900527954\n",
            "  batch 150 loss: 0.01327669620513916\n",
            "  batch 160 loss: 0.01249759942293167\n",
            "  batch 170 loss: 0.012208837270736694\n",
            "  batch 180 loss: 0.011658211052417756\n",
            "  batch 190 loss: 0.007060632109642029\n",
            "  batch 200 loss: 0.006783677637577057\n",
            "  batch 210 loss: 0.00661020576953888\n",
            "  batch 220 loss: 0.007613978534936905\n",
            "  batch 230 loss: 0.007043185830116272\n",
            "  batch 240 loss: 0.003918055444955826\n",
            "  batch 250 loss: 0.0042519710958004\n",
            "  batch 260 loss: 0.004262828081846237\n",
            "  batch 270 loss: 0.004659812152385712\n",
            "  batch 280 loss: 0.0026641042903065683\n",
            "  batch 290 loss: 0.00458880253136158\n",
            "  batch 300 loss: 0.003381328284740448\n",
            "  batch 310 loss: 0.003843255341053009\n",
            "  batch 320 loss: 0.0028912223875522613\n",
            "  batch 330 loss: 0.0047304131090641025\n",
            "  batch 340 loss: 0.0019457347691059113\n",
            "  batch 350 loss: 0.006520691514015198\n",
            "  batch 360 loss: 0.0020916204899549483\n",
            "  batch 370 loss: 0.00409890003502369\n",
            "  batch 380 loss: 0.0016815114766359329\n",
            "  batch 390 loss: 0.0018549609929323196\n",
            "  batch 400 loss: 0.0029398705810308456\n",
            "  batch 410 loss: 0.004780914261937141\n",
            "  batch 420 loss: 0.0053021878004074095\n",
            "  batch 430 loss: 0.0013775384053587914\n",
            "LOSS train 0.0013775384053587914 valid 0.02589365812256636\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.0031824205070734024\n",
            "  batch 20 loss: 0.0009897896088659763\n",
            "  batch 30 loss: 0.0007440061308443546\n",
            "  batch 40 loss: 0.001921866647899151\n",
            "  batch 50 loss: 0.003691587969660759\n",
            "  batch 60 loss: 0.0014752890914678574\n",
            "  batch 70 loss: 0.0008369484916329384\n",
            "  batch 80 loss: 0.002566002681851387\n",
            "  batch 90 loss: 0.00045944657176733017\n",
            "  batch 100 loss: 0.0015920303761959077\n",
            "  batch 110 loss: 0.0015075210481882094\n",
            "  batch 120 loss: 0.0021749619394540788\n",
            "  batch 130 loss: 0.0010497353971004487\n",
            "  batch 140 loss: 0.0011120941489934921\n",
            "  batch 150 loss: 0.0013935035094618797\n",
            "  batch 160 loss: 0.0019754722714424135\n",
            "  batch 170 loss: 0.0014398055151104926\n",
            "  batch 180 loss: 0.0023735716938972475\n",
            "  batch 190 loss: 0.00047935182228684423\n",
            "  batch 200 loss: 0.0007715482264757156\n",
            "  batch 210 loss: 0.0007878568023443222\n",
            "  batch 220 loss: 0.0008045502938330174\n",
            "  batch 230 loss: 0.0007525687105953693\n",
            "  batch 240 loss: 0.00038448988925665617\n",
            "  batch 250 loss: 0.002571602165699005\n",
            "  batch 260 loss: 0.0008367063477635383\n",
            "  batch 270 loss: 0.0005313284695148468\n",
            "  batch 280 loss: 0.0007576202973723411\n",
            "  batch 290 loss: 0.0011329822242259978\n",
            "  batch 300 loss: 0.0016496691852808\n",
            "  batch 310 loss: 0.002306707389652729\n",
            "  batch 320 loss: 0.00166025310754776\n",
            "  batch 330 loss: 0.0010601928457617759\n",
            "  batch 340 loss: 0.0003700601402670145\n",
            "  batch 350 loss: 0.0014576775021851063\n",
            "  batch 360 loss: 0.0005146625451743602\n",
            "  batch 370 loss: 0.00066903717815876\n",
            "  batch 380 loss: 0.0011745018884539605\n",
            "  batch 390 loss: 0.0017039325088262559\n",
            "  batch 400 loss: 0.0014800150878727437\n",
            "  batch 410 loss: 0.0023983968421816826\n",
            "  batch 420 loss: 0.0006715183146297932\n",
            "  batch 430 loss: 0.0003899755422025919\n",
            "LOSS train 0.0003899755422025919 valid 0.20496972799955984\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.0002620286773890257\n",
            "  batch 20 loss: 0.00031164786778390405\n",
            "  batch 30 loss: 0.002050451375544071\n",
            "  batch 40 loss: 0.0004007692448794842\n",
            "  batch 50 loss: 0.0014912228100001812\n",
            "  batch 60 loss: 0.00044486229307949543\n",
            "  batch 70 loss: 0.003835470974445343\n",
            "  batch 80 loss: 0.00016715913079679012\n",
            "  batch 90 loss: 0.00044951895251870154\n",
            "  batch 100 loss: 0.0009290855377912521\n",
            "  batch 110 loss: 0.0001347747165709734\n",
            "  batch 120 loss: 0.0016780264675617218\n",
            "  batch 130 loss: 0.0035641781985759737\n",
            "  batch 140 loss: 0.0032459694892168047\n",
            "  batch 150 loss: 0.0004564376547932625\n",
            "  batch 160 loss: 0.00016465105582028628\n",
            "  batch 170 loss: 0.0001235957257449627\n",
            "  batch 180 loss: 8.470698958262801e-05\n",
            "  batch 190 loss: 0.00035454144235700367\n",
            "  batch 200 loss: 0.002015085518360138\n",
            "  batch 210 loss: 0.00019727956969290973\n",
            "  batch 220 loss: 0.00041908523999154566\n",
            "  batch 230 loss: 0.0015371239744126797\n",
            "  batch 240 loss: 0.00194314643740654\n",
            "  batch 250 loss: 0.00024199276231229305\n",
            "  batch 260 loss: 0.0001460472005419433\n",
            "  batch 270 loss: 0.0009752029553055763\n",
            "  batch 280 loss: 0.00010296192485839128\n",
            "  batch 290 loss: 0.00040372018702328205\n",
            "  batch 300 loss: 0.0023301593959331513\n",
            "  batch 310 loss: 0.00021518087014555932\n",
            "  batch 320 loss: 0.0007964412681758404\n",
            "  batch 330 loss: 0.0006230982951819896\n",
            "  batch 340 loss: 0.0006601722911000252\n",
            "  batch 350 loss: 9.376761154271663e-05\n",
            "  batch 360 loss: 0.00022249734029173852\n",
            "  batch 370 loss: 0.00010929990094155073\n",
            "  batch 380 loss: 0.00021546483039855958\n",
            "  batch 390 loss: 6.146612577140331e-05\n",
            "  batch 400 loss: 0.0001507879002019763\n",
            "  batch 410 loss: 0.0020504441112279893\n",
            "  batch 420 loss: 0.0001264088321477175\n",
            "  batch 430 loss: 0.008751124143600464\n",
            "LOSS train 0.008751124143600464 valid 1.522563074816654\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.130819833278656\n",
            "  batch 20 loss: 0.12164334058761597\n",
            "  batch 30 loss: 0.09875485301017761\n",
            "  batch 40 loss: 0.07784801125526428\n",
            "  batch 50 loss: 0.06669396162033081\n",
            "  batch 60 loss: 0.05300703048706055\n",
            "  batch 70 loss: 0.050827968120574954\n",
            "  batch 80 loss: 0.03878272771835327\n",
            "  batch 90 loss: 0.03454141616821289\n",
            "  batch 100 loss: 0.02580316662788391\n",
            "  batch 110 loss: 0.023218441009521484\n",
            "  batch 120 loss: 0.019771480560302736\n",
            "  batch 130 loss: 0.01695869117975235\n",
            "  batch 140 loss: 0.01341678500175476\n",
            "  batch 150 loss: 0.011299869418144226\n",
            "  batch 160 loss: 0.00961354821920395\n",
            "  batch 170 loss: 0.006877250224351883\n",
            "  batch 180 loss: 0.0062698014080524445\n",
            "  batch 190 loss: 0.0065095402300357815\n",
            "  batch 200 loss: 0.0063710793852806095\n",
            "  batch 210 loss: 0.005559602379798889\n",
            "  batch 220 loss: 0.007289819419384003\n",
            "  batch 230 loss: 0.004788104444742203\n",
            "  batch 240 loss: 0.006745921075344085\n",
            "  batch 250 loss: 0.003991707414388657\n",
            "  batch 260 loss: 0.008374743908643723\n",
            "  batch 270 loss: 0.0019222531467676164\n",
            "  batch 280 loss: 0.002724224887788296\n",
            "  batch 290 loss: 0.004158384725451469\n",
            "  batch 300 loss: 0.0028352225199341774\n",
            "  batch 310 loss: 0.003717167302966118\n",
            "  batch 320 loss: 0.0017773179337382317\n",
            "  batch 330 loss: 0.003237876668572426\n",
            "  batch 340 loss: 0.001787286251783371\n",
            "  batch 350 loss: 0.004679279401898384\n",
            "  batch 360 loss: 0.0012890363112092018\n",
            "  batch 370 loss: 0.0014364684000611305\n",
            "  batch 380 loss: 0.0026884520426392554\n",
            "  batch 390 loss: 0.0019273290410637856\n",
            "  batch 400 loss: 0.002626737579703331\n",
            "  batch 410 loss: 0.0013001923449337482\n",
            "  batch 420 loss: 0.002682744339108467\n",
            "  batch 430 loss: 0.0012794191017746926\n",
            "LOSS train 0.0012794191017746926 valid 0.022870271507946942\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.0010688472539186478\n",
            "  batch 20 loss: 0.003128325194120407\n",
            "  batch 30 loss: 0.001170511357486248\n",
            "  batch 40 loss: 0.001643555797636509\n",
            "  batch 50 loss: 0.0019772138446569444\n",
            "  batch 60 loss: 0.002219027653336525\n",
            "  batch 70 loss: 0.0007581969723105431\n",
            "  batch 80 loss: 0.0014233472757041454\n",
            "  batch 90 loss: 0.0009854981675744058\n",
            "  batch 100 loss: 0.0008955703116953373\n",
            "  batch 110 loss: 0.0014501454308629036\n",
            "  batch 120 loss: 0.00048458161763846873\n",
            "  batch 130 loss: 0.0006981668062508106\n",
            "  batch 140 loss: 0.0009439487010240554\n",
            "  batch 150 loss: 0.0008091086521744729\n",
            "  batch 160 loss: 0.000539671303704381\n",
            "  batch 170 loss: 0.000654095783829689\n",
            "  batch 180 loss: 0.001388577464967966\n",
            "  batch 190 loss: 0.0006745620165020227\n",
            "  batch 200 loss: 0.002260316163301468\n",
            "  batch 210 loss: 0.0012759423814713954\n",
            "  batch 220 loss: 0.001297028176486492\n",
            "  batch 230 loss: 0.0006087406072765589\n",
            "  batch 240 loss: 0.0007051271852105856\n",
            "  batch 250 loss: 0.0007497570477426052\n",
            "  batch 260 loss: 0.0003674940438941121\n",
            "  batch 270 loss: 0.0012750236317515373\n",
            "  batch 280 loss: 0.00045981183648109437\n",
            "  batch 290 loss: 0.0005093674175441265\n",
            "  batch 300 loss: 0.0014211924746632576\n",
            "  batch 310 loss: 0.00031419529113918543\n",
            "  batch 320 loss: 0.0002178998664021492\n",
            "  batch 330 loss: 0.0016899731010198594\n",
            "  batch 340 loss: 0.00035501036327332256\n",
            "  batch 350 loss: 0.0011721677146852016\n",
            "  batch 360 loss: 0.0002218949608504772\n",
            "  batch 370 loss: 0.00015665203100070357\n",
            "  batch 380 loss: 0.0015801744535565375\n",
            "  batch 390 loss: 0.0010187644511461258\n",
            "  batch 400 loss: 0.00040564308874309064\n",
            "  batch 410 loss: 0.0002954539377242327\n",
            "  batch 420 loss: 0.00026000700891017916\n",
            "  batch 430 loss: 0.00032211560755968094\n",
            "LOSS train 0.00032211560755968094 valid 0.005141042691056183\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.00015241857618093492\n",
            "  batch 20 loss: 0.0003628185950219631\n",
            "  batch 30 loss: 9.741692920215428e-05\n",
            "  batch 40 loss: 0.0001800473313778639\n",
            "  batch 50 loss: 0.0004506702534854412\n",
            "  batch 60 loss: 0.00029114722274243833\n",
            "  batch 70 loss: 0.00013879069592803717\n",
            "  batch 80 loss: 8.996000397019089e-05\n",
            "  batch 90 loss: 0.0008517405018210411\n",
            "  batch 100 loss: 0.00020169583149254322\n",
            "  batch 110 loss: 8.953943033702671e-05\n",
            "  batch 120 loss: 0.00010420482140034437\n",
            "  batch 130 loss: 0.00010173784103244543\n",
            "  batch 140 loss: 7.457504398189485e-05\n",
            "  batch 150 loss: 0.0003566859755665064\n",
            "  batch 160 loss: 7.232326315715909e-05\n",
            "  batch 170 loss: 0.0005238890182226897\n",
            "  batch 180 loss: 0.0012733005918562413\n",
            "  batch 190 loss: 0.0011525852605700493\n",
            "  batch 200 loss: 0.00018729453440755606\n",
            "  batch 210 loss: 0.0002755804220214486\n",
            "  batch 220 loss: 0.0002267819596454501\n",
            "  batch 230 loss: 7.422633934766054e-05\n",
            "  batch 240 loss: 0.00043683117255568507\n",
            "  batch 250 loss: 0.0008006207644939423\n",
            "  batch 260 loss: 0.00019964147359132766\n",
            "  batch 270 loss: 0.001376011036336422\n",
            "  batch 280 loss: 0.0001025976613163948\n",
            "  batch 290 loss: 5.6529941502958536e-05\n",
            "  batch 300 loss: 0.00016631060279905796\n",
            "  batch 310 loss: 0.000843065045773983\n",
            "  batch 320 loss: 0.0001923591014929116\n",
            "  batch 330 loss: 0.0008502448908984661\n",
            "  batch 340 loss: 0.0001024062279611826\n",
            "  batch 350 loss: 0.00011926635634154081\n",
            "  batch 360 loss: 0.0001073524821549654\n",
            "  batch 370 loss: 0.0001043053693138063\n",
            "  batch 380 loss: 9.864873718470334e-05\n",
            "  batch 390 loss: 0.0020443230867385866\n",
            "  batch 400 loss: 0.00012657927582040429\n",
            "  batch 410 loss: 0.0001967177726328373\n",
            "  batch 420 loss: 0.00020078965462744236\n",
            "  batch 430 loss: 0.00014547391328960656\n",
            "LOSS train 0.00014547391328960656 valid 0.004272981912254181\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.132925283908844\n",
            "  batch 20 loss: 0.12776135206222533\n",
            "  batch 30 loss: 0.12105895280838012\n",
            "  batch 40 loss: 0.11446142196655273\n",
            "  batch 50 loss: 0.10892847776412964\n",
            "  batch 60 loss: 0.10101561546325684\n",
            "  batch 70 loss: 0.09450809359550476\n",
            "  batch 80 loss: 0.08577220439910889\n",
            "  batch 90 loss: 0.09337245225906372\n",
            "  batch 100 loss: 0.08149765133857727\n",
            "  batch 110 loss: 0.07751519680023193\n",
            "  batch 120 loss: 0.07497068047523499\n",
            "  batch 130 loss: 0.06933289766311646\n",
            "  batch 140 loss: 0.06924138069152833\n",
            "  batch 150 loss: 0.05993492603302002\n",
            "  batch 160 loss: 0.05661650896072388\n",
            "  batch 170 loss: 0.05563355684280395\n",
            "  batch 180 loss: 0.05080337524414062\n",
            "  batch 190 loss: 0.05008844137191772\n",
            "  batch 200 loss: 0.044337570667266846\n",
            "  batch 210 loss: 0.0407401442527771\n",
            "  batch 220 loss: 0.03800092339515686\n",
            "  batch 230 loss: 0.036453619599342346\n",
            "  batch 240 loss: 0.035378119349479674\n",
            "  batch 250 loss: 0.032740157842636106\n",
            "  batch 260 loss: 0.032564187049865724\n",
            "  batch 270 loss: 0.026384904980659485\n",
            "  batch 280 loss: 0.02523808479309082\n",
            "  batch 290 loss: 0.023023808002471925\n",
            "  batch 300 loss: 0.023449236154556276\n",
            "  batch 310 loss: 0.019466379284858705\n",
            "  batch 320 loss: 0.01907927393913269\n",
            "  batch 330 loss: 0.017701373994350435\n",
            "  batch 340 loss: 0.014860063791275024\n",
            "  batch 350 loss: 0.015208590030670165\n",
            "  batch 360 loss: 0.01333988606929779\n",
            "  batch 370 loss: 0.013575881719589233\n",
            "  batch 380 loss: 0.013687071204185487\n",
            "  batch 390 loss: 0.011299251019954682\n",
            "  batch 400 loss: 0.013405948877334595\n",
            "  batch 410 loss: 0.011459454894065857\n",
            "  batch 420 loss: 0.00864865630865097\n",
            "  batch 430 loss: 0.008530396968126297\n",
            "LOSS train 0.008530396968126297 valid 2.4307134914296595\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.010535390675067901\n",
            "  batch 20 loss: 0.009693680703639984\n",
            "  batch 30 loss: 0.007832109928131104\n",
            "  batch 40 loss: 0.008199122548103333\n",
            "  batch 50 loss: 0.007225881516933441\n",
            "  batch 60 loss: 0.00683847963809967\n",
            "  batch 70 loss: 0.007143363356590271\n",
            "  batch 80 loss: 0.006044132262468338\n",
            "  batch 90 loss: 0.005355268716812134\n",
            "  batch 100 loss: 0.005531827360391617\n",
            "  batch 110 loss: 0.005879127234220505\n",
            "  batch 120 loss: 0.005523452907800675\n",
            "  batch 130 loss: 0.005235351994633675\n",
            "  batch 140 loss: 0.007719972729682922\n",
            "  batch 150 loss: 0.004782427474856377\n",
            "  batch 160 loss: 0.004801960289478302\n",
            "  batch 170 loss: 0.004685351252555847\n",
            "  batch 180 loss: 0.004720349982380867\n",
            "  batch 190 loss: 0.00574159137904644\n",
            "  batch 200 loss: 0.0036856919527053834\n",
            "  batch 210 loss: 0.003623839095234871\n",
            "  batch 220 loss: 0.00424659438431263\n",
            "  batch 230 loss: 0.003277292102575302\n",
            "  batch 240 loss: 0.0038776777684688566\n",
            "  batch 250 loss: 0.004005567356944084\n",
            "  batch 260 loss: 0.002477332204580307\n",
            "  batch 270 loss: 0.0030706673860549927\n",
            "  batch 280 loss: 0.0021544549614191054\n",
            "  batch 290 loss: 0.0032434985041618347\n",
            "  batch 300 loss: 0.0029042689129710197\n",
            "  batch 310 loss: 0.003053433634340763\n",
            "  batch 320 loss: 0.0027019383385777475\n",
            "  batch 330 loss: 0.002848028764128685\n",
            "  batch 340 loss: 0.002246931754052639\n",
            "  batch 350 loss: 0.002556971088051796\n",
            "  batch 360 loss: 0.0018083292990922928\n",
            "  batch 370 loss: 0.0022900838404893873\n",
            "  batch 380 loss: 0.0024056997150182723\n",
            "  batch 390 loss: 0.002197377383708954\n",
            "  batch 400 loss: 0.001874293014407158\n",
            "  batch 410 loss: 0.005372880399227143\n",
            "  batch 420 loss: 0.003638771548867226\n",
            "  batch 430 loss: 0.002891805022954941\n",
            "LOSS train 0.002891805022954941 valid 0.25355403426695955\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.0016777090728282928\n",
            "  batch 20 loss: 0.0015407465398311615\n",
            "  batch 30 loss: 0.0015628620982170104\n",
            "  batch 40 loss: 0.001321346126496792\n",
            "  batch 50 loss: 0.0026602864265441893\n",
            "  batch 60 loss: 0.0016795363277196883\n",
            "  batch 70 loss: 0.0011008686386048793\n",
            "  batch 80 loss: 0.0015008997172117234\n",
            "  batch 90 loss: 0.001902630552649498\n",
            "  batch 100 loss: 0.0019406676292419434\n",
            "  batch 110 loss: 0.0018283702433109284\n",
            "  batch 120 loss: 0.0014443444088101387\n",
            "  batch 130 loss: 0.0019139867275953294\n",
            "  batch 140 loss: 0.0009400680661201477\n",
            "  batch 150 loss: 0.0010437430813908578\n",
            "  batch 160 loss: 0.0011256253346800804\n",
            "  batch 170 loss: 0.0009792611934244632\n",
            "  batch 180 loss: 0.0012071930803358554\n",
            "  batch 190 loss: 0.0008511010557413102\n",
            "  batch 200 loss: 0.0007639667950570583\n",
            "  batch 210 loss: 0.001087167952209711\n",
            "  batch 220 loss: 0.0008569685742259026\n",
            "  batch 230 loss: 0.0011258386075496674\n",
            "  batch 240 loss: 0.0016061378642916679\n",
            "  batch 250 loss: 0.0007416194304823876\n",
            "  batch 260 loss: 0.001043841429054737\n",
            "  batch 270 loss: 0.0008456285111606121\n",
            "  batch 280 loss: 0.0010130245238542557\n",
            "  batch 290 loss: 0.0015949876978993416\n",
            "  batch 300 loss: 0.0006640848238021136\n",
            "  batch 310 loss: 0.0008181910961866379\n",
            "  batch 320 loss: 0.0008921574801206589\n",
            "  batch 330 loss: 0.0006882516201585532\n",
            "  batch 340 loss: 0.0009070435538887977\n",
            "  batch 350 loss: 0.0015045693144202232\n",
            "  batch 360 loss: 0.001210729405283928\n",
            "  batch 370 loss: 0.0022963903844356536\n",
            "  batch 380 loss: 0.0007197670638561249\n",
            "  batch 390 loss: 0.0006792631931602955\n",
            "  batch 400 loss: 0.0011666106060147286\n",
            "  batch 410 loss: 0.001253513153642416\n",
            "  batch 420 loss: 0.0008314046077430248\n",
            "  batch 430 loss: 0.002393834665417671\n",
            "LOSS train 0.002393834665417671 valid 0.07579419111875309\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.13532308340072632\n",
            "  batch 20 loss: 0.133981192111969\n",
            "  batch 30 loss: 0.12663183212280274\n",
            "  batch 40 loss: 0.11941144466400147\n",
            "  batch 50 loss: 0.11290508508682251\n",
            "  batch 60 loss: 0.10514715909957886\n",
            "  batch 70 loss: 0.09871848821640014\n",
            "  batch 80 loss: 0.09140087962150574\n",
            "  batch 90 loss: 0.08594225645065308\n",
            "  batch 100 loss: 0.07674486637115478\n",
            "  batch 110 loss: 0.07424976825714111\n",
            "  batch 120 loss: 0.07052147388458252\n",
            "  batch 130 loss: 0.062295454740524295\n",
            "  batch 140 loss: 0.06206592321395874\n",
            "  batch 150 loss: 0.06051514744758606\n",
            "  batch 160 loss: 0.052743589878082274\n",
            "  batch 170 loss: 0.0484278678894043\n",
            "  batch 180 loss: 0.04356151819229126\n",
            "  batch 190 loss: 0.04969767928123474\n",
            "  batch 200 loss: 0.039373072981834414\n",
            "  batch 210 loss: 0.043182122707366946\n",
            "  batch 220 loss: 0.033219540119171144\n",
            "  batch 230 loss: 0.040059608221054074\n",
            "  batch 240 loss: 0.025734829902648925\n",
            "  batch 250 loss: 0.026578670740127562\n",
            "  batch 260 loss: 0.02633594274520874\n",
            "  batch 270 loss: 0.02156313955783844\n",
            "  batch 280 loss: 0.017519156634807586\n",
            "  batch 290 loss: 0.022876711189746858\n",
            "  batch 300 loss: 0.01968727558851242\n",
            "  batch 310 loss: 0.016847041249275208\n",
            "  batch 320 loss: 0.01815251410007477\n",
            "  batch 330 loss: 0.014266282320022583\n",
            "  batch 340 loss: 0.013189747929573059\n",
            "  batch 350 loss: 0.01292104423046112\n",
            "  batch 360 loss: 0.010467319190502167\n",
            "  batch 370 loss: 0.011799108982086182\n",
            "  batch 380 loss: 0.01550324261188507\n",
            "  batch 390 loss: 0.009750166535377502\n",
            "  batch 400 loss: 0.009320007264614105\n",
            "  batch 410 loss: 0.008774912357330323\n",
            "  batch 420 loss: 0.006785148382186889\n",
            "  batch 430 loss: 0.007262404263019562\n",
            "LOSS train 0.007262404263019562 valid 0.07138721003112468\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.007353154569864273\n",
            "  batch 20 loss: 0.007178934663534165\n",
            "  batch 30 loss: 0.006976944208145141\n",
            "  batch 40 loss: 0.007185426354408264\n",
            "  batch 50 loss: 0.004791608452796936\n",
            "  batch 60 loss: 0.006674712151288986\n",
            "  batch 70 loss: 0.0037686217576265333\n",
            "  batch 80 loss: 0.005329443514347077\n",
            "  batch 90 loss: 0.004312926158308983\n",
            "  batch 100 loss: 0.0049857888370752335\n",
            "  batch 110 loss: 0.005274122580885887\n",
            "  batch 120 loss: 0.003300487995147705\n",
            "  batch 130 loss: 0.004598186910152435\n",
            "  batch 140 loss: 0.003426053747534752\n",
            "  batch 150 loss: 0.00427793301641941\n",
            "  batch 160 loss: 0.004020814597606659\n",
            "  batch 170 loss: 0.0026433397084474563\n",
            "  batch 180 loss: 0.0035864129662513734\n",
            "  batch 190 loss: 0.003614835813641548\n",
            "  batch 200 loss: 0.002394130825996399\n",
            "  batch 210 loss: 0.0034980103373527527\n",
            "  batch 220 loss: 0.0015740782022476197\n",
            "  batch 230 loss: 0.0015851058065891265\n",
            "  batch 240 loss: 0.00340067595243454\n",
            "  batch 250 loss: 0.0036363072693347933\n",
            "  batch 260 loss: 0.0039054878056049348\n",
            "  batch 270 loss: 0.0032095201313495636\n",
            "  batch 280 loss: 0.003453508764505386\n",
            "  batch 290 loss: 0.0022173400968313215\n",
            "  batch 300 loss: 0.0019642867147922514\n",
            "  batch 310 loss: 0.0028617797419428826\n",
            "  batch 320 loss: 0.001699245348572731\n",
            "  batch 330 loss: 0.0012054170481860637\n",
            "  batch 340 loss: 0.0013353701680898667\n",
            "  batch 350 loss: 0.0018909487873315812\n",
            "  batch 360 loss: 0.001020077522844076\n",
            "  batch 370 loss: 0.0012122796848416328\n",
            "  batch 380 loss: 0.0017392277717590333\n",
            "  batch 390 loss: 0.0019102102145552636\n",
            "  batch 400 loss: 0.0010476751253008843\n",
            "  batch 410 loss: 0.0011615647934377193\n",
            "  batch 420 loss: 0.0017794201150536538\n",
            "  batch 430 loss: 0.0019431965425610541\n",
            "LOSS train 0.0019431965425610541 valid 0.01644116676179692\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.004696363955736161\n",
            "  batch 20 loss: 0.0008472580462694168\n",
            "  batch 30 loss: 0.0013452925719320773\n",
            "  batch 40 loss: 0.002759980410337448\n",
            "  batch 50 loss: 0.002254815772175789\n",
            "  batch 60 loss: 0.0015523688867688178\n",
            "  batch 70 loss: 0.0008654847741127014\n",
            "  batch 80 loss: 0.0008350972086191178\n",
            "  batch 90 loss: 0.0006676394958049058\n",
            "  batch 100 loss: 0.001081348117440939\n",
            "  batch 110 loss: 0.001772790402173996\n",
            "  batch 120 loss: 0.0008219434879720211\n",
            "  batch 130 loss: 0.00162044670432806\n",
            "  batch 140 loss: 0.0012042317539453506\n",
            "  batch 150 loss: 0.0008426900953054428\n",
            "  batch 160 loss: 0.0005994181148707867\n",
            "  batch 170 loss: 0.0005144569557160139\n",
            "  batch 180 loss: 0.0006783820688724517\n",
            "  batch 190 loss: 0.0006607019808143377\n",
            "  batch 200 loss: 0.0010857440531253815\n",
            "  batch 210 loss: 0.0009219368919730186\n",
            "  batch 220 loss: 0.0010981393977999687\n",
            "  batch 230 loss: 0.000687898974865675\n",
            "  batch 240 loss: 0.0008104648441076279\n",
            "  batch 250 loss: 0.0009375249966979027\n",
            "  batch 260 loss: 0.0009600292891263962\n",
            "  batch 270 loss: 0.0019153162837028503\n",
            "  batch 280 loss: 0.00040218452922999857\n",
            "  batch 290 loss: 0.0008857442066073417\n",
            "  batch 300 loss: 0.0003264059312641621\n",
            "  batch 310 loss: 0.0015211992897093296\n",
            "  batch 320 loss: 0.0006331899669021368\n",
            "  batch 330 loss: 0.0022693434730172157\n",
            "  batch 340 loss: 0.0006739499978721142\n",
            "  batch 350 loss: 0.0009944581426680088\n",
            "  batch 360 loss: 0.0003527129534631968\n",
            "  batch 370 loss: 0.0015139173716306686\n",
            "  batch 380 loss: 0.0009146870113909244\n",
            "  batch 390 loss: 0.0003490723203867674\n",
            "  batch 400 loss: 0.0009020494297146797\n",
            "  batch 410 loss: 0.00044335145503282547\n",
            "  batch 420 loss: 0.0006160411518067122\n",
            "  batch 430 loss: 0.00042488034814596175\n",
            "LOSS train 0.00042488034814596175 valid 0.012018181629140269\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.12598532438278198\n",
            "  batch 20 loss: 0.11623978614807129\n",
            "  batch 30 loss: 0.09937452673912048\n",
            "  batch 40 loss: 0.09068409204483033\n",
            "  batch 50 loss: 0.07212156653404236\n",
            "  batch 60 loss: 0.06079011559486389\n",
            "  batch 70 loss: 0.052637737989425656\n",
            "  batch 80 loss: 0.049871259927749635\n",
            "  batch 90 loss: 0.038323530554771425\n",
            "  batch 100 loss: 0.033283394575119016\n",
            "  batch 110 loss: 0.03373448252677917\n",
            "  batch 120 loss: 0.023429083824157714\n",
            "  batch 130 loss: 0.018017840385437012\n",
            "  batch 140 loss: 0.018134787678718567\n",
            "  batch 150 loss: 0.012726476788520813\n",
            "  batch 160 loss: 0.01156666800379753\n",
            "  batch 170 loss: 0.012198035418987275\n",
            "  batch 180 loss: 0.009429553151130676\n",
            "  batch 190 loss: 0.007707389444112778\n",
            "  batch 200 loss: 0.004973540455102921\n",
            "  batch 210 loss: 0.006675264239311219\n",
            "  batch 220 loss: 0.006449105590581894\n",
            "  batch 230 loss: 0.0038390524685382845\n",
            "  batch 240 loss: 0.003937692940235138\n",
            "  batch 250 loss: 0.004059045389294624\n",
            "  batch 260 loss: 0.0030259821563959123\n",
            "  batch 270 loss: 0.003322618082165718\n",
            "  batch 280 loss: 0.002159344032406807\n",
            "  batch 290 loss: 0.004291542246937752\n",
            "  batch 300 loss: 0.0033532556146383284\n",
            "  batch 310 loss: 0.0031778860837221147\n",
            "  batch 320 loss: 0.00522194616496563\n",
            "  batch 330 loss: 0.0017696112394332887\n",
            "  batch 340 loss: 0.002662317082285881\n",
            "  batch 350 loss: 0.001514653582125902\n",
            "  batch 360 loss: 0.0010962143540382385\n",
            "  batch 370 loss: 0.0023539848625659943\n",
            "  batch 380 loss: 0.0014619629830121994\n",
            "  batch 390 loss: 0.0024492375552654265\n",
            "  batch 400 loss: 0.0029109295457601546\n",
            "  batch 410 loss: 0.0022003982216119767\n",
            "  batch 420 loss: 0.0023842141032218932\n",
            "  batch 430 loss: 0.003161759674549103\n",
            "LOSS train 0.003161759674549103 valid 0.018140830002614\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.0016748091205954553\n",
            "  batch 20 loss: 0.0011575953103601933\n",
            "  batch 30 loss: 0.0011269686743617057\n",
            "  batch 40 loss: 0.0018526868894696235\n",
            "  batch 50 loss: 0.0006190755870193243\n",
            "  batch 60 loss: 0.0007809078320860863\n",
            "  batch 70 loss: 0.0021855162456631662\n",
            "  batch 80 loss: 0.0006016820203512907\n",
            "  batch 90 loss: 0.0006206608843058348\n",
            "  batch 100 loss: 0.00043412921950221063\n",
            "  batch 110 loss: 0.00042411270551383494\n",
            "  batch 120 loss: 0.0006446350831538439\n",
            "  batch 130 loss: 0.0013390852138400078\n",
            "  batch 140 loss: 0.0016189223155379296\n",
            "  batch 150 loss: 0.0003709855955094099\n",
            "  batch 160 loss: 0.00042925267480313777\n",
            "  batch 170 loss: 0.0005574331618845463\n",
            "  batch 180 loss: 0.0012159092351794244\n",
            "  batch 190 loss: 0.0017356991767883301\n",
            "  batch 200 loss: 0.0002713700057938695\n",
            "  batch 210 loss: 0.0003774269949644804\n",
            "  batch 220 loss: 0.0002979409182444215\n",
            "  batch 230 loss: 0.0005451063625514507\n",
            "  batch 240 loss: 0.0003659182693809271\n",
            "  batch 250 loss: 0.0009597902186214924\n",
            "  batch 260 loss: 0.00033787884749472143\n",
            "  batch 270 loss: 0.0003659114241600037\n",
            "  batch 280 loss: 0.0004312025383114815\n",
            "  batch 290 loss: 0.0003288206411525607\n",
            "  batch 300 loss: 0.0028240421786904337\n",
            "  batch 310 loss: 0.0002847594209015369\n",
            "  batch 320 loss: 0.0007048781961202621\n",
            "  batch 330 loss: 0.0005279065109789372\n",
            "  batch 340 loss: 0.00022451425902545453\n",
            "  batch 350 loss: 0.0006546901538968086\n",
            "  batch 360 loss: 0.0018540477380156518\n",
            "  batch 370 loss: 0.0007094524335116148\n",
            "  batch 380 loss: 0.0004283923655748367\n",
            "  batch 390 loss: 0.00027760476805269716\n",
            "  batch 400 loss: 0.0003555615432560444\n",
            "  batch 410 loss: 0.00028135166503489016\n",
            "  batch 420 loss: 0.00031403633765876294\n",
            "  batch 430 loss: 0.00020874468609690666\n",
            "LOSS train 0.00020874468609690666 valid 0.006147413621800462\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.00021972083486616613\n",
            "  batch 20 loss: 0.00017251701792702078\n",
            "  batch 30 loss: 0.00028156046755611897\n",
            "  batch 40 loss: 0.0001809548120945692\n",
            "  batch 50 loss: 0.00018536432180553674\n",
            "  batch 60 loss: 0.0005583872087299823\n",
            "  batch 70 loss: 0.0002173876389861107\n",
            "  batch 80 loss: 0.0009110882878303528\n",
            "  batch 90 loss: 0.00012492502573877574\n",
            "  batch 100 loss: 0.00040387799963355065\n",
            "  batch 110 loss: 0.0001815974828787148\n",
            "  batch 120 loss: 0.00034370513167232276\n",
            "  batch 130 loss: 0.0004252476152032614\n",
            "  batch 140 loss: 0.00032016769982874395\n",
            "  batch 150 loss: 7.327554631046951e-05\n",
            "  batch 160 loss: 0.00011002671672031284\n",
            "  batch 170 loss: 0.00011743153445422649\n",
            "  batch 180 loss: 0.0001810369547456503\n",
            "  batch 190 loss: 0.00011228013318032026\n",
            "  batch 200 loss: 0.0014763464219868182\n",
            "  batch 210 loss: 0.0003702896879985929\n",
            "  batch 220 loss: 9.121168404817581e-05\n",
            "  batch 230 loss: 0.0005018619820475578\n",
            "  batch 240 loss: 0.00010196606162935496\n",
            "  batch 250 loss: 0.00012161011109128595\n",
            "  batch 260 loss: 0.0002404005266726017\n",
            "  batch 270 loss: 6.786873564124107e-05\n",
            "  batch 280 loss: 0.00013141371309757232\n",
            "  batch 290 loss: 8.050646865740419e-05\n",
            "  batch 300 loss: 0.00012499191798269748\n",
            "  batch 310 loss: 0.0001286364858970046\n",
            "  batch 320 loss: 7.522914092987776e-05\n",
            "  batch 330 loss: 0.00015084893675521016\n",
            "  batch 340 loss: 0.0002696350682526827\n",
            "  batch 350 loss: 0.00017515915678814053\n",
            "  batch 360 loss: 0.00241538118571043\n",
            "  batch 370 loss: 0.00022256639786064625\n",
            "  batch 380 loss: 0.00010861058253794909\n",
            "  batch 390 loss: 6.752519984729589e-05\n",
            "  batch 400 loss: 0.0001731711789034307\n",
            "  batch 410 loss: 6.164762889966368e-05\n",
            "  batch 420 loss: 5.690563120879233e-05\n",
            "  batch 430 loss: 0.00023282482288777828\n",
            "LOSS train 0.00023282482288777828 valid 0.006127861348648945\n",
            "EPOCH 1:\n",
            "  batch 10 loss: 0.13047510385513306\n",
            "  batch 20 loss: 0.11973423957824707\n",
            "  batch 30 loss: 0.09854576587677003\n",
            "  batch 40 loss: 0.08104316592216491\n",
            "  batch 50 loss: 0.0763207197189331\n",
            "  batch 60 loss: 0.06202156543731689\n",
            "  batch 70 loss: 0.057541316747665404\n",
            "  batch 80 loss: 0.05117136240005493\n",
            "  batch 90 loss: 0.05440995693206787\n",
            "  batch 100 loss: 0.03870534598827362\n",
            "  batch 110 loss: 0.041713875532150266\n",
            "  batch 120 loss: 0.031122756004333497\n",
            "  batch 130 loss: 0.024725912511348723\n",
            "  batch 140 loss: 0.022549064457416536\n",
            "  batch 150 loss: 0.02028857320547104\n",
            "  batch 160 loss: 0.015436789393424988\n",
            "  batch 170 loss: 0.01590408682823181\n",
            "  batch 180 loss: 0.012491419911384583\n",
            "  batch 190 loss: 0.009502331912517547\n",
            "  batch 200 loss: 0.00848640277981758\n",
            "  batch 210 loss: 0.008088208734989166\n",
            "  batch 220 loss: 0.007784685492515564\n",
            "  batch 230 loss: 0.006515568494796753\n",
            "  batch 240 loss: 0.006958913803100586\n",
            "  batch 250 loss: 0.0046463944017887115\n",
            "  batch 260 loss: 0.006260930001735688\n",
            "  batch 270 loss: 0.004576654732227325\n",
            "  batch 280 loss: 0.004274160414934158\n",
            "  batch 290 loss: 0.004290511086583138\n",
            "  batch 300 loss: 0.0030800988897681236\n",
            "  batch 310 loss: 0.0035824764519929884\n",
            "  batch 320 loss: 0.0030013831332325936\n",
            "  batch 330 loss: 0.0025676269084215163\n",
            "  batch 340 loss: 0.0020467713475227356\n",
            "  batch 350 loss: 0.001705610752105713\n",
            "  batch 360 loss: 0.0021940490230917932\n",
            "  batch 370 loss: 0.0026550624519586564\n",
            "  batch 380 loss: 0.0023718981072306635\n",
            "  batch 390 loss: 0.002753134071826935\n",
            "  batch 400 loss: 0.0032148625701665877\n",
            "  batch 410 loss: 0.0012628436088562012\n",
            "  batch 420 loss: 0.001101053599268198\n",
            "  batch 430 loss: 0.0019504077732563018\n",
            "LOSS train 0.0019504077732563018 valid 0.03149260310032828\n",
            "EPOCH 2:\n",
            "  batch 10 loss: 0.0026656003668904305\n",
            "  batch 20 loss: 0.005217427387833595\n",
            "  batch 30 loss: 0.0010453982278704644\n",
            "  batch 40 loss: 0.0015802115201950073\n",
            "  batch 50 loss: 0.0014198461547493934\n",
            "  batch 60 loss: 0.0008594714105129242\n",
            "  batch 70 loss: 0.0006687741726636887\n",
            "  batch 80 loss: 0.0008969703689217567\n",
            "  batch 90 loss: 0.0008405796252191067\n",
            "  batch 100 loss: 0.0009367077611386776\n",
            "  batch 110 loss: 0.0009470805525779724\n",
            "  batch 120 loss: 0.0015096006914973258\n",
            "  batch 130 loss: 0.0014688549563288689\n",
            "  batch 140 loss: 0.0010932793840765953\n",
            "  batch 150 loss: 0.000980185717344284\n",
            "  batch 160 loss: 0.000661466596648097\n",
            "  batch 170 loss: 0.0020526837557554244\n",
            "  batch 180 loss: 0.0009608908556401729\n",
            "  batch 190 loss: 0.0007200698368251323\n",
            "  batch 200 loss: 0.0005310793872922659\n",
            "  batch 210 loss: 0.0018910150974988937\n",
            "  batch 220 loss: 0.000807102769613266\n",
            "  batch 230 loss: 0.0008298046886920929\n",
            "  batch 240 loss: 0.0005087735597044229\n",
            "  batch 250 loss: 0.0006789667997509242\n",
            "  batch 260 loss: 0.0010833850130438805\n",
            "  batch 270 loss: 0.0008010955527424813\n",
            "  batch 280 loss: 0.0008898608386516571\n",
            "  batch 290 loss: 0.00035205930471420287\n",
            "  batch 300 loss: 0.0004485301673412323\n",
            "  batch 310 loss: 0.0004273644648492336\n",
            "  batch 320 loss: 0.0002659794641658664\n",
            "  batch 330 loss: 0.0009428624995052815\n",
            "  batch 340 loss: 0.0015923429280519485\n",
            "  batch 350 loss: 0.000507753249257803\n",
            "  batch 360 loss: 0.0011968088336288929\n",
            "  batch 370 loss: 0.0004758835770189762\n",
            "  batch 380 loss: 0.002411358430981636\n",
            "  batch 390 loss: 0.0005024687387049198\n",
            "  batch 400 loss: 0.0003857343923300505\n",
            "  batch 410 loss: 0.0002976346295326948\n",
            "  batch 420 loss: 0.0003320333082228899\n",
            "  batch 430 loss: 0.000565722119063139\n",
            "LOSS train 0.000565722119063139 valid 0.013574841906930405\n",
            "EPOCH 3:\n",
            "  batch 10 loss: 0.0012836852110922337\n",
            "  batch 20 loss: 0.0020390789955854415\n",
            "  batch 30 loss: 0.00021036898251622916\n",
            "  batch 40 loss: 0.0004374559037387371\n",
            "  batch 50 loss: 0.00019400082528591155\n",
            "  batch 60 loss: 0.0013944305479526519\n",
            "  batch 70 loss: 0.0001756018726155162\n",
            "  batch 80 loss: 0.0003213015152141452\n",
            "  batch 90 loss: 0.0004262739792466164\n",
            "  batch 100 loss: 0.00027222263161093\n",
            "  batch 110 loss: 0.00024535786360502243\n",
            "  batch 120 loss: 0.0001409968826919794\n",
            "  batch 130 loss: 0.0009302535094320774\n",
            "  batch 140 loss: 0.0007197851315140724\n",
            "  batch 150 loss: 0.00022800324950367212\n",
            "  batch 160 loss: 0.00040574450977146623\n",
            "  batch 170 loss: 0.0008551345206797123\n",
            "  batch 180 loss: 0.0011479114182293415\n",
            "  batch 190 loss: 0.00018592106644064189\n",
            "  batch 200 loss: 0.00012859556591138244\n",
            "  batch 210 loss: 0.0006703995633870364\n",
            "  batch 220 loss: 0.0002567862626165152\n",
            "  batch 230 loss: 0.0001643216353841126\n",
            "  batch 240 loss: 0.0005266512278467416\n",
            "  batch 250 loss: 0.00029544683638960123\n",
            "  batch 260 loss: 0.0003523552557453513\n",
            "  batch 270 loss: 0.0005818093195557594\n",
            "  batch 280 loss: 8.434436749666929e-05\n",
            "  batch 290 loss: 0.00028251693584024906\n",
            "  batch 300 loss: 0.00011132709914818406\n",
            "  batch 310 loss: 0.0005034799687564373\n",
            "  batch 320 loss: 0.00017024604603648185\n",
            "  batch 330 loss: 6.769862957298756e-05\n",
            "  batch 340 loss: 0.0010027210228145123\n",
            "  batch 350 loss: 0.00012749979505315422\n",
            "  batch 360 loss: 0.00024981829337775705\n",
            "  batch 370 loss: 7.42646399885416e-05\n",
            "  batch 380 loss: 5.793122109025717e-05\n",
            "  batch 390 loss: 0.00013079107739031315\n",
            "  batch 400 loss: 0.00017648892244324089\n",
            "  batch 410 loss: 0.0003844529390335083\n",
            "  batch 420 loss: 0.00022854248527437449\n",
            "  batch 430 loss: 0.00011202241294085979\n",
            "LOSS train 0.00011202241294085979 valid 0.0040629985939383256\n",
            "{'res_blocks': 3, 'res_block_size': 3, 'input_channels': 248, 'downsample': 0.5}\n",
            "Accuracy: 0.9996430611079383\n"
          ]
        }
      ]
    }
  ]
}